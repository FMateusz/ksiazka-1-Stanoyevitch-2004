\documentclass[../main.tex]{subfiles}
\begin{document}
\setcounter{equation}{0}
	

\chapter{Matrices and Linear Systems}
\label{chap:7}

\section{MATRIX COMPUTATIONS AND MANIPULATIONS WITH MATLAB}
\label{sec:7_1}

\noindent I saw my first matrix in my first linear algebra course as an undergraduate, which
came after the calculus sequence. A matrix is really just a spreadsheet of numbers,
and as computers are having an increasing impact on present-day life and
education, the importance of matrices is becoming paramount. Many interesting
and important problems can be solved using matrices, and the basic concepts for
matrices are quite easy to introduce. Presently, matrices are making their way
down into lower levels of mathematics courses and, in some instances, even
elementary school curricula. Matrix operations and calculations are simple in
principle but in practice they can get quite long. It is often not feasible to perform
such calculations by hand except in special situations. Computers, on the other
hand, are ideally suited to manipulate matrices and MATLAB has been specially
designed to effectively manipulate them. In this section we introduce the basic
matrix operations and show how to perform them in MATLAB. We will also
present some of the numerous tricks, features, and ideas that can be used in
MATLAB to store and edit matrices. In Section 7.2 we present some applications
of basic matrix operations to computer graphics and animation. The very brief
Section 7.3 introduces concepts related to linear systems and Section 7.4 shows
ways to use MATLAB to solve linear systems. Section 7.5 presents an
algorithmic and theoretical development of Gaussian elimination and related
concepts. In Section 7.6, we introduce norms with the goal of developing some
error estimates for numerical solutions of linear systems. Section 7.7 introduces
iterative methods for solving linear systems. When conditions are ripe, iterative
methods can perform much more quickly and effectively than Gaussian
elimination.

We first introduce some definitions and notations from mathematics and then
translate them into MATLAB's language. A matrix A is a rectangular array of
numbers, called entries. A generic form for a matrix is shown in Figure \ref{fig:fig_7_1}. The
\textbf{rows} of a matrix are its horizontal strips of entries (labeled from the top) and the
\textbf{columns} are the vertical strips of entries (labeled from the left). The entry of A
that lies in row i and in column j is written as $a_{ij}$ (if either of the indices $i$ or $j$
is greater than a single digit, then the notation inserts a comma: $a_{i,j}$). The matrix
A is said to be of size $n$ by $m$ (or an $n \times m$ matrix) if it has $n$ rows and $m$ columns.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\linewidth]{fig_7_1}
    \caption{The anatomy of a matrix $A$ having $n$ rows and $m$ columns. The entry that
        lies in the second row and the third column is written as $a_{23}$.}
    \label{fig:fig_7_1}
\end{figure}

In mathematical notation, the matrix A in Figure \ref{fig:fig_7_1} is sometimes written in the
abbreviated form $$A=[a_{ij}]$$
where its size either is understood from the context or is unimportant. With this
notation it is easy to explain how matrix addition/subtraction and scalar
\textbf{multiplication} work. The matrix $A$ can be multiplied by any scalar (real number)
a with the obvious definition: $$\alpha A = [\alpha a_{ij}]$$
Matrices can be added/subtracted only when they have the same size, and the
definition is the obvious one: Corresponding entries get added/subtracted, i.e.,
$$[a_{ij}] \pm [b_{ij}] = [a_{ij} \pm b_{ij}]$$
Matrix multiplication, however, is not done in the obvious way. To explain it we
first recall the definition of \textbf{the dot product} of two vectors of the same size. If
$a = [α_1, α_2, \dots, a)n]$ and $b = [b_1, b_2, \dots, b_n]$ are two vectors of the same length $n$
(for this definition it does not matter if these vectors are row or column vectors),
the dot product of these vectors is defined as follows:
$$a \cdot b = a_1 b_1 + a_2 b_2 + \dots + a_n b_n = \sum_{k=1}^{n} a_kb_k$$

Now, if $A = [a_{ij}]$ is an wxm matrix and $B = [b_{ij}]$ is an $m \times r$ matrix (i.e., the
number of columns of $A $equals the number of rows of $B$) then the matrix product
$C = AB$ is defined and it will be another matrix $C = [c_{ij}]$ of size $n \times r$. To get an
entry $c_{ij}$ we simply take the dot product of the $i$th row vector of $A$ and the
$j$th column vector of $B$. Here is a simple example:

\begin{Example}
    Given the matrices: A=
    \(\begin{bmatrix}
        1  & 0 \\
        -3 & 8
    \end{bmatrix}\),
    B=
    \(\begin{bmatrix}
        4 & 1  \\
        3 & -6
    \end{bmatrix}\),
    M=
    \(\begin{bmatrix}
        4  & -9 \\
        3  & 1  \\
        -2 & 5
    \end{bmatrix}\),
    compute the following: $A-2B$, $AB$, $BA$, $AM$, $MA$.

    \begin{Solution}
        $A-2B = \begin{bmatrix}
                1  & 0 \\
                -3 & 8
            \end{bmatrix} - \begin{bmatrix}
                8 & 2   \\
                6 & -12
            \end{bmatrix} = \begin{bmatrix}
                -7 & -2 \\
                -9 & 20
            \end{bmatrix},$
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.3\linewidth]{fig_7_1_b}
        \end{figure}\\
        (In the product $AB$?, the indicated second row, first column entry was computed by
        taking the dot product of the corresponding second row of $A$ (shown) with the
        first column of $B$ (also indicated).) Similarly,
        $$BA = \begin{bmatrix}
                1  & 8   \\
                21 & -48
            \end{bmatrix} \text{, and } MA=\begin{bmatrix}
                31  & -72 \\
                0   & 8   \\
                -17 & 30
            \end{bmatrix}.$$
        The matrix product $AM$ is not defined since the inner dimensions of the sizes of
        $A$ and $M$ are not the same (i.e., $2 \neq 3$) . In particular, these examples show that
        matrix multiplication is, in general, not commutative; i.e., we cannot say that
        $AB = BA$ even when both matrix products are possible and have the same sizes.

        At first glance, the definition of matrix multiplication might seem too awkward to
        have much use. But later in this chapter (and in subsequent chapters as well) we
        will see numerous applications. We digress momentarily to translate these
        concepts and notations into MATLAB's language. To redo the above example in
        MATLAB, we would simply enter the matrices (rows separated by semicolons, as
        shown in Chapter 1) and perform the indicated linear combination and
        multiplications (since default operations are matrix operations in MATLAB, dots
        are not used before multiplications and other operators).

        \noindent MATLAB SOLUTION TO EXAMPLE \ref{example:7_1}:\\
        \mycode{
        >> A=[l 0; - 3 8 ]; B-[4 1;3 - 6); M=[4 -9;3 l;-2 5];\\
        >> A - 2*B, A*B, B*A, A*M, M*A\\
        $\rightarrow$ans = $\begin{matrix} -7 & -2 \\ -9 & 20\end{matrix}$
        ans =  $\begin{matrix} 4 & 1 \\ 12 & -51 \end{matrix}$
        ans = $\begin{matrix} 1 & 8 \\ 21 & -48 \end{matrix}$\\
        $\rightarrow$??? Error using ==> *\\
        Inner matrix dimensions must agree.\\
        $\rightarrow$ans = $\begin{matrix} 31 & -72 \\ 0 & 8 \\ -17 & 40 \end{matrix}$\\
        }

        By hand, matrix multiplication is feasible only for matrices of very small sizes.
        For example, if we multiply an $n \times n$ (square) matrix by another one of the same
        size, each entry involves a dot product of two vectors with $n$ components and thus
        will require n multiplications and $n-1$ additions (or subtractions). Since there
        are $n^2$ entries to compute, this yields a total of $n^2(n + n - 1) = 2n^3 -n^2$ floating
        point operations (flops). For a $5 \times 5$ matrix multiplication, this works out to be
        225 flops and for a $7 \times 7$, this works already to 637 flops, a very tedious hand
        calculation pushing on the fringes of insanity. As we saw in some experiments in
        Chapter \ref{chap:4}, on a PC with 256 MB of RAM and a 1.6 GHz microprocessor,
        MATLAB can roughly perform approximately 10 million flops in a few seconds.
        This means that (replacing the previous bound $2n^3-n^2$ by the more liberal but
        easier to deal with bound $2n^3$ setting this equal to 10 million and solving for $n$)
        MATLAB can quickly multiply two matrices of size up to about 171x171 (check
        that a matrix multiplication of two such large matrices works out to about 10
        million flops). Actually, not all flop calculations take equal times (this is why the
        word "flop" has a somewhat vague definition). It turns out that because of
        MATLAB's specially designed features that are mainly tailored to deal effectively
        with matrices, MATLAB can actually quickly multiply much larger matrices. On
        a PC with the aforementioned specs, the following experiment took MATLAB just
        a few seconds to multiply a pair of $1000 \times 1000$ matrices.
        \footnote{ As in Chapter \ref{chap:4}, we occasionally use the obsolete flop count
            function (from MATLAB Version 5) when convenient for an illustration.}\\
        \\
        \mycode{
            >> flops(0)\\
            >> A=rand(1000); B=rand(1000); \%constructs two 1000 by 1000 random matrices\\
            >> A*B;
            >> flops $\rightarrow$ 2.0000e + 009\\
        }
        This calculation involved 2 billion flops! Later on we will come across
        applications where such large-scale matrix multiplications come up naturally. Of
        course, if one (or both) matrices being multiplied have a lot of zero entries, then
        the computations needed can be greatly reduced. Such matrices (having a very
        low percentage of nonzero entries) are called \textbf{sparse}, and we will later see some
        special features that MATLAB has to deal with sparse matrices.

        We next move on to introduce several of the handy ways that MATLAB offers us
        to enter and manipulate matrices. For illustration, we assume that the matrices
        $A$, $B$ and $M$ of Example \ref{example:7_1} are still entered in our MATLAB session. The
        exponential operator ( \string^ ) in MATLAB works by default as a matrix power. Thus if
        we enter\\
        \mycode{
            >> A\string^2 \%matrix squaring\\
        }
        we get square of the matrix $A, A^2 = AA$,\\
        \mycode{
            $\rightarrow$ ans = $\begin{matrix}
                    1   & 0  \\
                    -27 & 64
                \end{matrix}$
        }\\
        whereas if we precede the operator by a dot, then, as usual, the operator changes to
        its entry-by-entry analog, and we get the matrix whose entries each equal the
        square of the corresponding entries of $A$.\\
        \mycode{
            >> A.\string^ \%element squaring\\
            $\rightarrow$ans=$\begin{matrix}
                    1 & 0  \\
                    9 & 64
                \end{matrix}$\\
        }
        This works the same way with other operators such as multiplication. Matrix
        operators such as addition/subtraction, which are the same as element-by-element
        addition/subtraction, make the dot a nonissue.

        To refer to a specific entry $a_{ij}$ in a matrix $A$, we use the syntax:
        \begin{center}
            \begin{tabularx}{\linewidth}{ |X|X| }
                \hline
                A(i, j) $\rightarrow$ & MATLAB's way of referring to the ith row jth column entry $a_{ij}$ of a matrix $A$ \\
                \hline
            \end{tabularx}
        \end{center}
        which was introduced in Section 4.3. Thus, for example, if we wanted to change
        the row 1, column 2 entry of our $2 \times 2$ matrix $A$ (from 0) to 2, we could enter:\\
        \mycode{
            >> A(1,2)=2 \%without suppressing output, MATLAB snows us the whole matrix\\
            $\rightarrow$A=$\begin{matrix}
                    1  & 2 \\
                    -3 & 8
                \end{matrix}$
        }

        We say that a (square) matrix $D=[d_{ij}]$ is a \textbf{diagonal matrix} if all entries, except perhaps
        those on the main diagonal, are zero (i.e., $d_{ij} = 0$ whenever $i \neq j$).
        Diagonal matrices (of the same size $n \times n$) are the easiest ones to multiply; indeed,
        for such a multiplication only n flops are needed:
        \begin{equation} \label{equation:7_1}
            \begin{bmatrix}
                d_{1} &       &        &     \\
                      & d_{2} &        & 0   \\
                      &       & \ddots &     \\
                      & 0     &        & d_n
            \end{bmatrix}
            \begin{bmatrix}
                e_{1} &       &        &     \\
                      & e_{2} &        & 0   \\
                      &       & \ddots &     \\
                      & 0     &        & e_n
            \end{bmatrix}
            =
            \begin{bmatrix}
                d_1e_1 &        &        &        \\
                       & d_2e_2 &        & 0      \\
                       &        & \ddots &        \\
                       & 0      &        & d_ne_n
            \end{bmatrix}
        \end{equation}
        The large zeros in the above notation are to indicate that all entries in the
        triangular regions above and below the main diagonal are zeros. There are many
        matrix-related problems and theorems where things boil down to considerations of
        diagonal matrices, or minor variations of them.
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    Prove identity (\ref{equation:7_1}).
\end{ExerciseForTheReader}

In MATLAB, we can enter a diagonal matrix using the command \mycode{diag} as
follows. To create a 5x5 diagonal matrix D with diagonal entries (in order): 1 2
-3 4 5, we could type:\\
\mycode{
    >> diag([1 2 -3 4 5]) $\rightarrow ans=\begin{matrix}
            1 & 0 & 0  & 0 & 0 \\
            0 & 2 & 0  & 0 & 0 \\
            0 & 0 & -3 & 0 & 0 \\
            0 & 0 & 0  & 4 & 0 \\
            0 & 0 & 0  & 0 & 5 \\
        \end{matrix}$\\
}
One very special diagonal matrix is $n \times n$ (square) \textbf{identity matrix $I_n$} or simply
$I$ (if the size is understood or unimportant). It has all the diagonal entries equaling
1 and has the property that whenever it is multiplied (on either side) by a matrix
$A$ (so that the multiplication is possible), the product is $A$, i.e.,
\begin{equation}\label{equation:7_2}
    AI = A = IA
\end{equation}
Thus, the identity matrix $I$ plays the role in matrix theory that the number 1 plays
in arithmetic; i.e., it is the "(multiplicative) identity." Even easier than with the
\mycode{diag} command, we can create identity matrices with the command \mycode{eye}:\\
\mycode{
    I2 = eye(2), I4=eye(4)\\
    $\rightarrow$I2=$\begin{matrix}1 & 0 \\0 & 1\end{matrix}$
    $\rightarrow$I4=$\begin{matrix}1 & 0 & 0 & 0\\0 & 1 & 0 & 0\\0 & 0 & 1 & 0\\0 & 0 & 0 & 1\end{matrix}$\\
}
Let us check identity (\ref{equation:7_2}) for our stored $2 \times 2$ matrix $A=\begin{matrix}1 & 2 \\-3 & 8\end{matrix}$:\\
\mycode{
    >> I2*A, A*I2\\
    $\rightarrow$ans=$\begin{matrix}1 & 2 \\-3 & 8\end{matrix}$
    $\rightarrow$ans=$\begin{matrix}1 & 2 \\-3 & 8\end{matrix}$ \\
}

To be able to divide one matrix $A$ by another one $B$ , we will actually have to
multiply $A$ by the inverse $B^{-1}$ of the matrix $B$, if the latter exists and the
multiplication is possible. It is helpful to think of the analogy with real numbers:
To perform a division, say $5 \div 2$ , we can recast this as a multiplication $5 \cdot 2^{-1}$,
where the inverse of 2 (as with any nonzero real number) is the reciprocal 1/2.
The only real number that does not have an inverse is zero; thus we can always
divide any real number by any other real number as long as the latter is not zero.
Note that the inverse $a^{-1}$ of a real number $a$ has the property that when the two
are multiplied the result will be 1 $(a \cdot a^{-1} = 1 = a^{-1} \cdot a)$. To translate this concept
into matrix theory is simple; since the number 1 translates to the identity matrix,
we see that for a matrix $B^{-1}$ to be the inverse of a matrix $B$, we must have
\begin{equation}\label{equation:7_3}
    BB^{-1} = 1 = B^{-1}B..
\end{equation}
In this case we also say that the matrix B is \textbf{invertible} (or \textbf{nonsingular}).
The only way for the equations of (\ref{equation:7_3}) to be possible is if $B$ is a square matrix.
There are, however, a lot of square matrices that are not invertible. One way to tell
whether a square matrix $B$ is invertible is by looking at its determinant
det($B$) (which was introduced in Section \ref{sec:4_3}), as the following theorem shows:

\begin{Theorem}
    \textit{(Invertibility of Square Matrices)}
    \begin{enumerate}
        \item A square matrix B is invertible exactly when its determinant det(B) is nonzero.
        \item In case of $2 \times 2$ matrix $B = \begin{bmatrix}a & b \\c & d\end{bmatrix}$
              with determinant $det(B) \equiv  ad-bc \neq 0$, the inverse is given by the formula
              $$B^{-1} = \frac{1}{det(B)}\begin{bmatrix}d & -b \\-c & a\end{bmatrix}.$$
    \end{enumerate}
    For a proof of this theorem we refer to any good linear algebra textbook (for
    example [HoKu-71]). There is an algorithm for computing the inverse of a matrix,
    which we will briefly discuss later, and there are more complicated formulas for
    the inverse of a general $n \times n$ matrix, but we will not need to go so far in this
    direction since MATLAB has some nice built-in functions for finding
    determinants and inverses. They are as follows:
    \begin{center}
        \begin{tabular}{ |c|c| }
            \hline
            inv(A) $\rightarrow$ & Numerically computes the inverse of a square matrix $A$ \\
            \hline
            det(A) $\rightarrow$ & Computes the determinant of a square matrix $A$         \\
            \hline
        \end{tabular}
    \end{center}
    The \mycode{inv} command must be used with caution, as the following simple examples show.
    From the theorem, the matrix $M=\begin{bmatrix}2 & 3 \\1 & 2\end{bmatrix}$ is easily inverted,
    and MATLAB confirms the results:\\
    \mycode{
    >> M=[2 3; 1 2];\\
    >> inv(M)\\
    $\rightarrow$ans=$\begin{matrix}2 & -3 \\-1 & 2\end{matrix}$\\
    }
    However, the matrix $M=\begin{bmatrix}3 & -6 \\2 & -4\end{bmatrix}$ has $det(M)=0$,
    so from the theorem we know that the inverse does not exist. If we try to get MATLAB to compute this inverse,
    we get the following:\\
    \mycode{
    >> M=[3 -6; 2 -4];\\
    >> inv(M)\\
    $\rightarrow$ Warning: Matrix is singular to working precision.\\
    ans = $\begin{matrix}inf & inf \\inf & inf\end{matrix}$\\
    }

    The output does not actually tell us that the matrix is not invertible, but it gives us
    a meaningless answer (\mycode{Inf} is MATLAB's way of writing oo) that seems to
    suggest that there is no inverse. This brings us to a subtle and important point
    about floating point arithmetic. Since MATLAB, or any computer system, can
    work only with a finite number of digits, it is not really possible for MATLAB to
    distinguish between zero and a very small positive or negative number.
    Furthermore, when doing computations (e.g., in finding an inverse of a (large)
    matrix,) there are (a lot of) calculations that must be performed and these will
    introduce roundoff errors. Because of this, something that is actually zero may
    appear as a nonzero but small number and vice versa (especially after the "noise"
    of calculations has distorted it). Because of this it is in general impossible to tell if
    a certain matrix is invertible or not if its determinant is very small. Here is some
    practical advice on computing inverses. If you get MATLAB to compute an
    inverse of a square matrix and get a "warning" as above, you should probably
    reject the output. If you then check the determinant of the matrix, chances are
    good that it will be very small. Later in this chapter we will introduce the concept
    of condition numbers for matrices and these will provide a more reliable way to
    detect so-called \mycode{poorly conditioned matrices} that are problematic in linear
    systems.

    Building and storing matrices with MATLAB can be an art. Apart from \mycode{eye} and
    \mycode{diag} that were already introduced, MATLAB has numerous commands for the
    construction of special matrices. Two such commonly used commands are \mycode{ones}
    and \mycode{zeros}.
    \begin{center}
        \begin{tabular}{ |c|c| }
            \hline
            \mycode{zeros(n,m)} $\rightarrow$ & Constructs an $n \times m$ matrix whose entries each equal 0. \\
            \hline
            \mycode{ones(n,m)} $\rightarrow$  & Constructs an $n \times m$ matrix whose entries each equal 1. \\
            \hline
        \end{tabular}
    \end{center}
    Of course, \mycode{zeros(n,m)} is redundant since we can just use 0*\mycode{ones(n, m)} in
    its place. But matrices of zeros come up often enough to justify separate mention
    of this command.
\end{Theorem}

\begin{Example}
    A \textbf{tridiagonal} matrix is one whose nonzero entries can lie either
    on the main diagonal or on the diagonals directly above/below the main diagonal.
    Consider the 60x60 tridiagonal matrix $A$ shown below:
    $$A=\begin{bmatrix}
            1      & 1      & 0      & 0      & 0      & 0      & 0      & \dots  & 0      \\
            -1     & 1      & 2      & 0      & 0      & 0      & 0      & \dots  & 0      \\
            0      & -1     & 1      & 3      & 0      & 0      & 0      & \dots  & 0      \\
            0      & 0      & 1      & 1      & 1      & 0      & 0      & \dots  & 0      \\
            0      & 0      & 0      & -1     & 1      & 2      & \ddots & \dots  & 0      \\
            0      & 0      & 0      & 0      & -1     & 1      & 3      & \dots  & 0      \\
            \vdots & \vdots & \vdots & \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
            0      & 0      & 0      & 0      & \dots  & 0      & -1     & 1      &        \\
            0      & 0      & 0      & 0      & 0      & 0      & 0      & 0      & 0
        \end{bmatrix}$$
    It has 1's straight down the main diagonal, -1's straight down the submain
    diagonal, the sequence (1,2,3) repeated on the supermain diagonal, and zeros for
    all other entries.
    \begin{enumerate}
        \item Store this matrix in MATLAB
        \item Find its determinant and compute and store the inverse matrix as $B$, if it
              exists (do not display it). Multiply the determinant of A with that of $B$.
        \item Print the $6 \times 6$ matrix $C$, which is the submatrix of $A$ whose rows are made
              up of the rows 30, 32, 34, 36, 38, and 40 of $A$ and whose columns are column
              numbers 30, 31, 32, 33, 34, and 60 of $A$ .
    \end{enumerate}

    \begin{Solution}
        Part (a): We start with the 60x60 identity matrix:\\
        \mycode{
            >> A=eye(60);\\
        }
        To put the -1's along the submain diagonal, we can use the following for loop:\\
        \mycode{
        >> for i=1:59, A(i+1,i)=-1; end\\
        }
        (Note: there are 59 entries in the submain diagonal; they are $A(2,1), A(3, 2), \dots,
            A(60,59)$and each one needs to be changed from 0 to -1.)
        The supermain diagonal entries can be changed using a similar for loop structure, but since they
        cycle between the three values 1, 2, 3, we could add in some branching within the
        for loop to accomplish this cycling. Here is one possible scheme:\\
        \mycode{
            >> count=1; \%initialize counter\\
            >> for i=1:59\\
            if count==1, A(i,i+1)=1;\\
            elseif count==2, A(i,i+1)=2;\\
            else A(i,i+1)=3;\\
            end\\
            count==count+1; \%bumps up counter by one\\
            if count==4, count=1; end \%cycles counter back to one after it pases 3\\
            end\\
        }
        We can do a brief check of the upper-left 6x6 submatrix to see if A has shaped
        out the way we want it; we invoke the submatrix features introduced in Section \ref{sec:4_2}.\\
        \mycode{
            >> A(1:6,1;6) $\rightarrow$ ans=$
                \begin{matrix}
                    1  & 1  & 0  & 0  & 0 & 0 \\
                    -1 & 1  & 2  & 0  & 0 & 0 \\
                    0  & -1 & 1  & 3  & 0 & 0 \\
                    0  & 0  & -1 & 1  & 1 & 0 \\
                    0  & 0  & 0  & -1 & 1 & 2
                \end{matrix}$\\
        }
        This looks like what we wanted. Here is another way to construct the supermain
        diagonal of $A$. We first construct a vector v that contains the desired supermain
        diagonal entries:\\
        \mycode{
        >> vseed=[1 2 3]; v=vseed;\\
        >> for i=1:19\\
        v=[v vseed]; \%tacks on "vseed" onto existing v\\
        end\\
        }
        Using this vector $v$, we can reset the supermain diagonal entries of $A$ as we did
        the submain diagonal entries:\\
        \mycode{
            >> for i=1:59\\
            A(i,i+1)=v(i);\\
            end\\
        }
        Shortly we will give another scheme for building such banded matrices, but it
        would behoove the reader to understand why the above loop construction does the
        job.

        \noindent Part (b):\\
        \mycode{
            >> det(A) $\rightarrow$ans = 3.6116e + 017\\
            >> B=inv(A); det(A)*det(B)$\rightarrow$ans=1.0000\\
        }
        This agrees with a special case of a theorem in linear algebra which states that for
        any pair of square matrices A and B of the same size, we have:
        \begin{equation} \label{equation:7_4}
            det(AB)=det(A) \cdot det(B).
        \end{equation}
        Since it is easy to show that $det(I) = l$, from (3) and (4) it follows that
        $det(A)det(A^{-1}) = l$.

        \noindent Part (c): Once again using MATLAB's array features introduced in Section 4.3,
        we can easily construct the desired submatrix of $A$ as follows:\\
        \mycode{
            >> C=A(30:2:40, [30:33 59 60])\\
            $\rightarrow$C=$
                \begin{matrix}
                    1 & 3  & 0 & 0  & 0 & 0 \\
                    0 & -1 & 1 & 2  & 0 & 0 \\
                    0 & 0  & 0 & -1 & 0 & 0 \\
                    0 & 0  & 0 & 0  & 0 & 0 \\
                    0 & 0  & 0 & 0  & 0 & 0
                \end{matrix}
            $
        }

        Tridiagonal matrices, like the one in the above example, are special cases of
        \textbf{banded matrices}, which are square matrices with all zero entries except on a
        certain set of diagonals. Large-sized tridiagonal and banded matrices are good
        examples of sparse matrices. They arise naturally in the very important finite
        difference methods that are used to numerically solve ordinary and partial
        differential equations later in this book. In its full syntax, the \mycode{diag} command
        introduced earlier allows us to put any vector on any diagonal of a matrix.
        \begin{center}
            \begin{tabularx}{\linewidth}{ |X|X| }
                \hline
                \mycode{diag(v,k)} $\rightarrow$ & For an integer k and an appropriately
                sized vector v, this command creates a
                square matrix with all zero entries, except for the kth diagonal on which will
                appear the vector v. k = 0 gives the main diagonal, k = 1 gives the
                supermain diagonal, k = -1 the submain diagonal, etc.                    \\
                \hline
            \end{tabularx}
        \end{center}
        For example, in the construction of the matrix A in the above example, after
        having constructed the $60 \times 60$ identity matrix, we could have put in the -l's in
        the submain diagonal by entering:\\
        \mycode{
            >> A=A+diag(-ones(1,59),-1);\\
        }
        and with the vector $v$ constructed as in our solution, we could put the appropriate
        numbers on the supermain diagonal with the command:\\
        \mycode{
            >>A=A+diag(v(1:59),1);\\
        }
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    \textit{(Random Integer Matrix Generator)} (a) For testing of
    hypotheses about matrices, it is often more convenient to work with
    integer-valued matrices rather than (floating point) decimal-valued matrices.
    Create a function M-file, called \mycode{randint(n,m,k)}, that takes as input three
    positive integers n, m, and k, and will output an $n \times m$ matrix whose entries are
    integers randomly distributed in the set ${-k,-k+l,\dots,-1,0,1,2,\dots,k-1,k}.$
    (b) Use this program to test the formula (\ref{equation:7_4}) given in the above example by creating
    two different $6 \times 6$ random integer matrices $A$ and $B$ and computing
    $det(AB)-det(A) \cdot det(B)$ to see if it equals zero. Use $k = 9$ so that the matrices
    have single-digit entries. Repeat this experiment two times (it will produce
    different random matrices $A$ and $B$) to see if it still checks. In each case, give
    the values of $det(AB)$.\\
    (c) Keeping $k = 9$, try to repeat the same experiment (again three times) using
    instead $16 \times 16$ sized matrices. Explain what has (probably) happened in terms of
    MATLAB's default working precision being restricted to about 15 digits.
\end{ExerciseForTheReader}

The matrix arithmetic operations enjoy the following properties (similar to those
of real numbers): Here $A$ and $B$ are any compatible matrices and a is any
number.

\noindent Commutativity of Addition:
\begin{equation}\label{equation:7_5}
    A+B = B+A.
\end{equation}
\noindent Associativity:
\begin{equation}\label{equation:7_6}
    (A+B)+C=A+(B+C) \text{, } (AB)C=A(BC).
\end{equation}
\noindent Distributive Laws:
\begin{equation}\label{equation:7_7}
    A(B+C)=AB+AC \text{, } (A+B)C=AC+BC.
\end{equation}
\begin{equation}\label{equation:7_8}
    \alpha (A+B)= \alpha A+ \alpha B \text{, } \alpha (AB)=(\alpha A)B=A(\alpha B).
\end{equation}
Each of these matrix identities is true whenever the matrices are of sizes that make
all parts defined. Experiments relating to these identities as well as their proofs
will be left to the exercises. We point out that matrix multiplication is not
commutative; the reader is invited to do an easy counterexample to verify this fact.

We close this section with a few more methods and features for manipulating
matrices in MATLAB, which we introduce through a simple MATLAB demonstration.

Let us begin with the following $3 \times 3$ matrix:\\
\mycode{
>> A = [1 2 3; 4 5 6; 7 8 9] $\rightarrow$ A= $\begin{matrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9
    \end{matrix}$\\
}
We can tack on [10 11 12] as a new bottom row to create a new matrix $B$ as follows:
\mycode{
>> A = [1 2 3; 4 5 6; 7 8 9] $\rightarrow$ A= $\begin{matrix}
        1  & 2  & 3  \\
        4  & 5  & 6  \\
        7  & 8  & 9  \\
        10 & 11 & 12
    \end{matrix}$\\
}
If instead we wanted to append this vector as a new column to get a new matrix C,
we could add it on as above to the transpose A' of Ay and then take transposes
again (the transpose operator was introduced in Section \ref{sec:1_3}).\footnote{
Without transposes this could be done directly with a few more keystrokes as follows:
C=[A, [10; 11; 12]]
}\\
\mycode{
>> C=[A'; 10 11 12]' $\rightarrow$ A= $\begin{matrix}
        1 & 2 & 3 & 10 \\
        4 & 5 & 6 & 11 \\
        7 & 8 & 9 & 12
    \end{matrix}$\\
}
Alternatively, we could start by defining $C = A$ and then introducing the (column)
vector as the fourth column of $C$. The following two commands would give the
same result/output.\\
\mycode{
>> C=A;\\
>> C(:,4)=[10 11 12]'\\
}
To delete any row/column from a matrix, we can assign it to be the empty vector
"[ ]". The following commands will change the matrix $A$ into a new $2 \times 3$ matrix
obtained from the former by deleting the second row.\\
\mycode{
>>A(2,:)=[] $\rightarrow$ A = $\begin{matrix}
        1 & 2 & 3 \\
        7 & 8 & 9
    \end{matrix}$\\
}

\begin{Exercises}
    \begin{enumerate}
        \item In this exercise you will be experimentally verifying the matrix identities (5), (6), (7), and (8)
              using the following square "test matrices:"
              $$A = \begin{bmatrix}
                      1 & 2 \\
                      3 & 4
                  \end{bmatrix}, B = \begin{bmatrix}
                      1  & 2 \\
                      -2 & 2
                  \end{bmatrix}, C = \begin{bmatrix}
                      4 & -2 \\
                      7 & 6
                  \end{bmatrix}.$$
              For these matrices verify the following:
              \begin{enumerate}
                  \item $A+B=B+A$ (matrix addition is commutative),
                  \item $(A+B)+C=A+(B+C)$ (matrix addition is associative),
                  \item $(AB)C=A(BC)$ (matrix multiplication is associative),
                  \item $A(B + C) = AB + AC$ and $(A + B)C = AC + BC$ (matrix multiplication is distributive).
                  \item $\alpha (A + B) = \alpha A + \alpha B$ and $\alpha a(AB) = (\alpha A)B - A(\alpha B)$ (for any real number a ) (test this last
                        one with $\alpha = 3$ ).
              \end{enumerate}
              \textbf{Note:} Such experiments do not constitute proofs. A math experiment can prove only that a
              mathematical claim is false, however, when a lot of experiments test something to be true, this
              can give us more of a reason to believe it and then pursue the proof. In the next three exercises,
              you will be doing some more related experiments. Later exercises in this set will ask for proofs
              of these identities.
        \item Repeat all parts of Exercise 1 using instead the following test matrices:
              $$A=
                  \begin{bmatrix}
                      1 & 2 & 3 \\
                      4 & 5 & 6 \\
                      7 & 8 & 9
                  \end{bmatrix},
                  B=
                  \begin{bmatrix}
                      1  & 2  & 4 \\
                      -2 & 2  & 4 \\
                      -8 & -4 & 8
                  \end{bmatrix},
                  C=
                  \begin{bmatrix}
                      3  & 4  & 7  \\
                      -2 & -8 & 0  \\
                      7  & 3  & 12
                  \end{bmatrix},
              $$
        \item \begin{enumerate}
                  \item  By making use of the r a n d command, create three $20 \times 20$ matrices $A$, $B$, and $C$, each of
                        whose (400) entries are randomly selected numbers in the interval $[0, 1]$. In this problem you
                        are to check that the identities given in all parts of Exercise 1 continue to test true.
                  \item Repeat this by creating $50 \times 50$ sized matrices.
                  \item Repeat again by creating $200 \times 200$ sized matrices.
                  \item Finally do it one last time using $1000 \times 1000$ sized matrices.
              \end{enumerate}
              \textbf{Sugestion:} Of course, here it is not feasible to display all of these matrices and to compare all
              of the entries of the matrices on both sides by eye. (For part (d) this would entail 1 million
              comparisons!) The following \mycode{max} command will be useful here:
              \begin{center}
                  \begin{tabularx}{\linewidth}{ |X|X| }
                      \hline
                      \mycode{max(A)} $\rightarrow$           & if $A$ is a (row or column) vector, this returns the maximum
                      value of the entries of $A$; if $A$ is an $n \times m$ matrix, it returns the
                      m-length vector whose jth entry equals the maximum value in
                      the jth column of $A$.                                                                                       \\
                      \hline
                      \mycode{max(max(A))} $\rightarrow$      & (From the functionality of the single "max" command) this will
                      return the maximum value of all the entries in $A$.                                                          \\
                      \hline
                      \mycode{max(max(abs(A)))} $\rightarrow$ & Since \mycode{abs(A)} is the matrix whose entries are the absolute
                      values of the corresponding entries of $A$, this command will
                      return the maximum absolute value of all the entries in $A$.                                                 \\
                      \hline
                  \end{tabularx}
              \end{center}
              Thus, an easy way to check if two matrices $E$ and $F$ (of the same size) are
              equal is to check that\\ \mycode{max(max(abs(E-F)))} equals zero.\\
              \textbf{Note:} Due to roundoff errors (which should be increasing with the larger-sized matrices), the
              matrices will not, in general, agree to MATLAB's working precision of about 15 decimals.
              Your conclusions should take this into consideration.
        \item \begin{enumerate}
                  \item Use the function \mycode{randint} that was constructed in Exercise for the Reader \ref{exreader:7_2} to create
                        three 3x3 random integer matrices (use $k = 9$) A, B, C on which you are to test once again
                        each of the parts of Exercise 1.
                  \item Repeat for $20 \times 20$ random integer matrices.
                  \item Repeat again for $100 \times 100$ random integer matrices.
                  \item If your results checked exactly in parts (b) and (c), explain why things were able to work
                        with such a large-sized matrix in this experiment, whereas in the experiment of checking
                        identity (4) in Exercise for the Reader \ref{exreader:7_2}, even a moderately sized $16 \times 16$ led to problems.
              \end{enumerate}
              \textbf{Sugestion:} For parts (b) and (c) you need not print the matrices; refer to the suggestion of the
              previous exercise.
        \item \begin{enumerate}
                  \item Build a $20 \times 20$ "checkerboard matrix" whose entries alternate from zeros and ones, with a
                        one in the upper-left corner and such that for each entry of this matrix, the immediate upper,
                        lower, left, and right neighbors (if they exist) are different.
                  \item Write a function M-file, call it \mycode{checker(n)}, that inputs a positive integer $n$ and outputs an
                        $n \times n$ , and run it for $n = 2, 3,4, 5 and 6$.
              \end{enumerate}
        \item Making use of the \mycode{randint} M-file of Exercise for the Reader \ref{exreader:7_2}, perform the following experiments.
              \begin{enumerate}
                  \item Run through $N = 100$ trials of taking the determinant of a $3 \times 3$ matrix with random integer
                        values spread out among -5 through 5. What was the average value of the determinants? What
                        percent of these 100 matrices were not invertible?
                  \item Run through the same experiments with everything the same, except this time let the
                        random integer values be spread out among -10 through 10.
                  \item Repeat part (b) except this time using $N = 500$ trials.
                  \item Repeat part (c) except this time work with $6 \times 6$ matrices.
                  \item Repeat part (c) except this time work with $10 \times 10$ matrices.
              \end{enumerate}
              What general patterns have you noticed? Without doing the experiment, what sort of results
              would you expect if we were to repeat part (c) with $20 \times 20$ matrices?\\
              \textbf{Note:} You need not print all of these matrices or even their determinants in your solution. Just
              include the relevant MATLAB code and output needed to answer the questions along with the
              answers.
        \item This exercise is similar to the previous one, except this time we will be working with matrices
              whose entries are real numbers (rather than integers) spread out uniformly randomly in an
              interval. To generate such an $n \times n$ matrix, for example, if we want the entries to be uniformly
              randomly spread out over $(-3, 3)$, we could use the command \mycode{6*rand(n)-3*ones(n)}.
              \begin{enumerate}
                  \item  Run through N = 100 trials of taking the determinant of a $3 \times 3$ matrix whose entries are
                        selected uniformly randomly as real numbers in $(-3, 3)$. What was the average value of the
                        determinants? What percent of these 100 matrices were not in vertible?
                  \item Run through the same experiments with everything the same, except this time let the real
                        number entries be uniformly randomly spread out among -10 through 10.
                  \item Repeat part (b) except this time using $N = 500$ trials.
                  \item Repeat part (c) except this time work with $6 \times 6$ matrices.
                  \item Repeat part (c) except this time work with $10 \times 10$ matrices.
              \end{enumerate}
        \item let $M=\begin{bmatrix}1 & 1 \\ 0 & 1\end{bmatrix}, N=\begin{bmatrix}1 & 1 \\ 1 & 1\end{bmatrix}$,
              \begin{enumerate}
                  \item Find $M^2, M^3, M^{26}, N^2, N^3, N^{26}$.
                  \item Find general formulas for $M^n$ and $N^n$ where $n$ is any positive integer.
                  \item Can you find a $2 \times 2$ matrix $A$ of real numbers that satisfies $A^2 - 1$ (with $A \neq I$)?
                  \item Find a $3 \times 3$ matrix $A \neq I$ such that $A^3 = 1$. Can you find such a matrix that is not diagonal?
              \end{enumerate}
              Part (c) may be a bit tricky. If you get stuck, use MATLAB to run some experiments on
              (what you think might be) possible square roots.
        \item Let $M$ and $N$ be the matrices in Exercise 8.
              \begin{enumerate}
                  \item  Find (if they exist) the inverses $M^l$ and $N^l$.
                  \item Find square roots of $M$ and $N$, i.e., find (if possible) two matrices $S$ and $T$ so that:
                        $S^2 = M$ (i.e., $S = \sqrt{7}$) and $R^2 = N$.
              \end{enumerate}
        \item \textit{(Discovering Properties and Nonproperties of the Determinant)} For each of the following
              equations, run through 100 tests with $2 \times 2$ matrices whose entries are randomly selected
              integers within [-9, 9] (using the \mycode{randint} M-file of Exercise for the Reader \ref{exreader:7_2}). For those
              that test false, record a single counterexample. For those that test true, repeat the same
              experiment twice more, first using $3 \times 3$ and then using $6 \times 6$ matrices of the same type. In
              each case, write your MATLAB code so that if all 100 matrix tests pass as "true" you will have
              as output (only) something like: "With 100 tests involving $2 \times 2$ matrices with random integer
              entries from - 9 to 9, the identity always worked"; while if it fails for a certain matrix, the
              experiment should stop at this point and output the matrix (or matrices) that give a
              counterexample. What are your conclusions?
              \begin{enumerate}
                  \item $det(A')=det(A)$
                  \item $det(2A)=4det(A)$
                  \item $det(-A)=det(A)$
                  \item $det(A+B)=det(A)+det(B)$
                  \item If matrix $B$ is obtained from $A$ by replacing one row of $A$ by a number $k$ times the
                        corresponding row of $A$, then $det(B) = k det(A)$.
                  \item If one row of $A$ is a constant multiple of another row of $A$ then $det(A) = 0$.
                  \item Two of these identities are not quite correct, in general, but they can be corrected using
                        another of these identities that is correct. Elaborate on this.
              \end{enumerate}
              \textbf{Sugestion:} For part (f) automate the experiment as follows: After a random integer matrix $A$
              is built, randomly select a first row number $r1$ and a different second row number $r2$. Then
              select randomly an integer $k$ in the range
                  [-9, 9]. Replace the row $r2$ with $k$ times row $r1$. This will be a good possible way to create
              your test matrices. Use a similar selection process for part (e).
        \item \begin{enumerate}
                  \item Prove that matrix addition is commutative $A + B = B + A$ whenever $A$ and $B$ are two
                        matrices of the same size. (This is identity (\ref{equation:7_5}) in the text.)
                  \item Prove that matrix addition is associative, $(A + B) + C - A + (B + C)$ whenever $A$, $B$, and
                        $C$ are matrices of the same size. (This is the first part of identity (\ref{equation:7_6}) in the text.)
              \end{enumerate}
        \item \begin{enumerate}
                  \item Prove that the distributive laws for matrices:
                        $A(B + C) = AB + AC$ and $(A + B)C = AC + BC$,
                        whenever the matrices are of appropriate sizes so that a particular
                        identity makes sense. (These are identities (\ref{equation:7_7}) in the text.)
                  \item  Prove that for any real number a, we have that $\alpha (A + B) = \alpha A + \alpha B$, whenever $A$, $B$,
                        and $C$ are matrices of the same size and that $\alpha (AB) = (\alpha A)B- A(\alpha B)$ whenever $A$ and $B$ are
                        matrices with $AB$ defined. (These are identities (\ref{equation:7_8}) in the text.)
              \end{enumerate}
        \item Prove that matrix multiplication is associative, $(AB)C = A(BC)$ whenever $A$, $B$, and $C$ are
              matrices so that both sides are defined. (This is the second part of identity (\ref{equation:7_6}) in the text:)
        \item \textit{(Discovering Facts about Matrices)} As we have seen, many matrix rules closely resemble
              corresponding rules of arithmetic. But one must be careful since there are some exceptions.
              One such notable exception we have encountered is that, unlike regular multiplication, matrix
              multiplication is not commutative; that is, in general we cannot say that $AB = BA$. For each of
              the statements below about matrices, either give a counterexample, if it is false, or give a proof
              if it is true. In each identity, assume that the matrices involved can be any matrices for which
              the expressions are all defined. Also, we use 0 to denote the zero matrix (i.e., all entries are zeros).
              \begin{enumerate}
                  \item $0A=0$
                  \item If $AB = 0$, then either $A = 0$ or $B = 0$.
                  \item If $A^2 = 0$, then $A = 0$.
                  \item $(AB)' = B'A' $(recall $A'$ denotes the transpose of $A$).
                  \item $(AB)^2 = A^2 B^2$.
                  \item If $A$ and $B$ are invertible square matrices, then so is $AB$ and $(AB)^{-1} = B^{-1}A^{-1}$
              \end{enumerate}
              \textbf{Sugestion:} If you are uncertain of any of these, run some experiments first (as shown in some
              of the preceding exercises). If your experiments produce a counterexample, you have disproved
              the assertion. In such a case you merely record the counterexample and move on to the next one.
    \end{enumerate}
\end{Exercises}

\section{INTRODUCTION TO COMPUTER \newline GRAPHICS AND ANIMATION} \label{sec:7_2}

\noindent \textbf{Computer graphics} is the generation and transformation of pictures on the
computer. This is a hot topic that has important applications in science and
business as well as in Hollywood (computer special effects and animated films).
In this section we will show how matrices can be used to perform certain types of
geometric operations on "objects." The objects can be either two- or three-
dimensional but most of our illustrations will be in the two-dimensional plane.
For two-dimensional objects, the rough idea is as follows. We can represent a
basic object in the plane as a MATLAB graphic by using the command
\mycode{plot(x,y)}, where $x$ and $y$ are vectors of the same length. We write $x$ and $y$
as row vectors, stack x on top of y, and we get a 2x« matrix A where n is the
common length of x and y. We can do certain mathematical operations to this
matrix to change it into a new matrix $A1$ whose rows are the corresponding vertex
vectors $x1$ and $yl$. If we look at \textbf{plot(xl,yl)}, we get a transformed version of
the original geometrical object. Many interesting geometric transformations can
be realized by simple matrix multiplications, but to make this all work nicely we
will need to introduce a new artificial third row of the matrix A, that will simply
consist of l's. If we work instead with these so-called homogeneous coordinates,
then all of the common operations of scaling (vertically or horizontally), shifting,
rotating, and reflecting can be realized by matrix multiplications of these
homogeneous coordinates. We can mix and repeat such transformations to get
more complicated geometric transformations; and by putting a series of such plots
together we can even make movies. Another interesting application is the
construction of fractal sets. Fractal sets (or fractals) are beautiful geometric
objects that enjoy a certain "self-similarity property," meaning that no matter how
closely one magnifies and examines the object, the fine details will always look
the same.

Polygons, which we recall are planar regions bounded by a finite set of line
segments, are represented by their vertices. If we store the jc-coordinates of these
vertices and the y-coordinates of these vertices as separate vectors (say the first
two rows of a matrix) preserving the order of adjacency, then MATLAB's \mycode{plot}
command can easily plot the polygon.

\begin{Example}
    We consider the following
    "CAT" polygon shown in Figure \ref{fig:fig_7_2}. Store the
    jc-coordinates of the vertices of the CAT as the
    first row vector of a matrix A, and the
    corresponding y-coordinates as the second row
    of the same matrix in such a way that MATLAB
    will be able to reproduce the cat by plotting the
    second row vector of A versus the first.
    Afterwards, obtain the plot from MATLAB.

    \begin{figure}[h]
        \centering
        \includegraphics[width=0.3\linewidth]{fig_7_2}
        \caption{CAT graphic for Exampe \ref{example:7_3}.}
        \label{fig:fig_7_2}
    \end{figure}

    \begin{Solution}
        We can store these nine vertices
        in a $2 \times 10$ matrix A (the first vertex appears
        twice so the polygon will be closed when we
        plot it). We may start at any vertex we like, but
        we must go around the cat in order (either clockwise or counterclockwise). Here
        is one such matrix that begins at the vertex (0,0) and moves clockwise around the cat.\\
        \mycode{
        >> A=[0 0 .5 1 2 2.5 3 3 1.5 0; ...\\
        0 3 4 3 3 4 3 0 -1 0];\\
        }
        To reproduce the cat, we plot the second row of $A$ (the y's) versus the first row (the x's):\\
        \mycode{
            >> plot(A(1,:), A(2,:))\\
        }

        In order to get the cat to fit nicely in the viewing area (recall, MATLAB always
        sets the view area to just accommodate all the points being plotted), we reset the
        viewing range to $- 2 < x < 5$, $- 3 < y < 6$, and then use the \mycode{equal} setting on the
        axes so the cat will appear undistorted.\\
        \mycode{
            >> axis([-2 5 -3 6])\\
            >> axis('equal)\\
        }
        The reader should check how each of the last two commands changes the cat
        graphic; we reproduce only the final plot in Figure \ref{fig:fig_7_3}(a). Figure \ref{fig:fig_7_3} actually
        contains two cats, the original one (white) as well as a gray cat. The gray cat was
        obtained in the same fashion as the orginal cat, except that the \mycode{plot} command
        was replaced by the \mycode{fill} command, which works specifically with polygons and
        whose syntax is as follows:
        \begin{center}
            \begin{tabularx}{\linewidth}{ |X|X| }
                \hline
                \mycode{fill(x,y,color)} $\rightarrow$ & Here x and y are vectors of the x- and y-coordinates of a polygon
                (preserving adjacency order); \mycode{color} can be either one of the
                predefined plot colors (as in Table \ref{table:1_1}) in single quotes, (e.g., k
                would be a black fill) or an RGB-vector [r g b] (with r, g, and b
                each being numbers in [0,1]) to produce any color; for example, [. 5 . 5 . 5 ] gives medium gray.          \\
                \hline
            \end{tabularx}
        \end{center}
        The elements $r$, $g$, and $b$ in a color vector determine the amounts of red, green,
        and blue to use to create a color; any color can be created in this way. For
        example, [ r g b ] = [l 0 0] would be a pure-red fill; magenta is obtained with the
        rgb vector [1 0 1], and different tones of gray can be achieved by using equal
        amounts of red, green, and blue between [0 0 0] (black) and [ 1 1 1 ] (white).

        \noindent For the gray cat in Figure \ref{fig:fig_7_3}(b), we used the command
        \mycode{fill(A(l,:),A(2,:),[.5.5.5])}. To get a black cat we could either set the rgb vector
        to [0 0 0] or replace it with k, which represents the preprogrammed color character
        for black (see Table \ref{table:1_1}).

        \begin{figure}[h]
            \centering
            \includegraphics[width=0.8\linewidth]{fig_7_3}
            \caption{Two MATLAB versions of the cat polygon: (a) (left) the first white cat was
                obtained using the \mycode{plot} command and (b) (right) the second with the \mycode{fill} command.}
            \label{fig:fig_7_3}
        \end{figure}
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    After experimenting a bit with rgb color
    vectors, get MATLAB to produce an orange cat, a brown cat, and a purple cat.
    Also, try and find the rgb color vector that best matches the MATLAB built-in
    color cyan (from Table \ref{table:1_1}, the symbol for cyan is c).
\end{ExerciseForTheReader}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\linewidth]{fig_7_4}
    \caption{A linear tranformation $L$ in the plane.}
    \label{fig:fig_7_4}
\end{figure}

A \textbf{linear transformation $L$} on the plane $R^2$ corresponds to a $2 \times 2$ matrix $M$(Figure 7.4). It
transforms any point $(x,y) $(represented by the column vector $\begin{bmatrix}x \\ y\end{bmatrix}$) to the
point $M\begin{bmatrix}x \\ y\end{bmatrix}$ obtained by multiplying it on the left by the matrix $M$.
The reason for the terminology is that a linear transformation preserves the two important linear
operations for vectors in the plane: addition and scalar multiplication.
That is, letting $P_1\begin{bmatrix}x_1 \\ y_1\end{bmatrix}$, $P_2\begin{bmatrix}x_2 \\ y_2\end{bmatrix}$
be two points in the plane (represented by column vectors), and writing $L(P)=MP$, the linear
transformation axioms can be expressed as follows:
\begin{equation} \label{equation:7_9}
    L(P_1 + P_2) = L(P_1)+L(P_2),
\end{equation}
\begin{equation} \label{equation:7_10}
    L(\alpha P_1) = \alpha L(P_1).
\end{equation}
Both of these are to be valid for any choice of vectors $P_i(i = 1,2)$ and scalar $\alpha$.
Because $L(P)$ is just $MP$ (the matrix $M$ multiplied by the matrix $P$), these two
identities are consequences of the general properties (\ref{equation:7_7}) and (\ref{equation:7_8}) for matrices. By
the same token, if $M$ is any nxn matrix, then the transformation $L(P) = MP$
defines a linear transformation (satisfying (\ref{equation:7_9}) and (\ref{equation:7_10})) for the space $R^n$ of n-length
vectors. Of course, most of our geomtric applications will deal with the
cases $n = 2$ (the plane) or $n = 3$ (3-dimensional(x,y,z) space).

Such tranformations and their generalizations are a basis for what is used in
contemporary interactive graphics programs and in the construction of computer
videos. If, as in the above example of the CAT, the vertices of a polygon are
stored as columns of a matrix $A$, then, because of the way matrix multiplication
works, we can transform each of the vertices at once by multiplying the matrix M
of a linear transformation by $A$. The result will be a new matrix containing the
new vertices of the transformed graphic, which can be easily plotted.

We now move on to give some important examples of transformations on the plane $R^2$.
\begin{enumerate}[wide, labelindent=0pt]
    \item \underline{Scalings:} or $a > 0$ , $b > 0$ the linear transformation
          $$\begin{bmatrix}x' \\ y'\end{bmatrix} =
              \begin{bmatrix}a&0 \\ 0&b\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} =
              \begin{bmatrix}ax \\ by\end{bmatrix}$$
          will scale the horizontal direction with
          respect to $x = 0$ by a factor of a and the
          vertical direction with respect to $y = 0$ by
          a factor of $b$. If either factor is $< 1$, there
          is contraction (shrinkage) toward 0 in the
          corresponding direction, while factors $> 1$
          give rise to an expansion (stretching) away
          from 0 in the corresponding direction.
          As an example, we use $a = 0.3$ and $b = 1$ to rescale our original CAT (Figure \ref{fig:fig_7_5}).
          We assume we have left in the graphics w indow the first (white) cat of Figure
          \ref{fig:fig_7_3}(a).
          \begin{figure}[h]
              \centering
              \includegraphics[width=0.5\linewidth]{fig_7_5}
              \caption{The scaling of the original cat using factors $a = 0.3$ for horizontal scaling
                  and $b = 1$ (no change) for vertical scaling has produced the narrow-faced cat.}
              \label{fig:fig_7_5}
          \end{figure}
          \\ \mycode{
          >>M=[.3 0; 0 1]; \%store scaling matrix\\
          >>A1=M*A; \%create the vertex matrix of the transformed cat\\
          >>hold on \%leave the original cat in the window so we can compare\\
          >>plot(A1(1,:), A1(2,:), 'r') \%new cat will be in red\\
          }
          \textbf{Caution:} Changes in the axis ranges can also produce scale changes in MATLAB graphics.
    \item \underline{Rotations:} For a rotation angle Θ, the linear tranformation that rotates a point
          $(x,y)$ an angle $θ$ (counterclockwise) around the origin $(0,0)$ is given by the
          following linear tranformation:
          $$\begin{bmatrix}x' \\ y'\end{bmatrix} = \begin{bmatrix}cos0&-sin0 \\ sin0& cos0\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix}$$
          (See Exercise 12 for a justification of this.) As an example, we rotate the original
          cat around the origin using angle $θ = - \pi /4$ (Figure \ref{fig:fig_7_6}). Once again, we assume
          the graphics window initially contains the original cat of Figure \ref{fig:fig_7_3} before we start
          to enter the following MATLAB commands:\\
          \mycode{
          >> M=[cos(-pi/4) -sin(-pi/4); sin(-pi/4) cos(-pi/4)];\\
          >> A1=M*A;, hold on,plot(A1(1,:), A1(2,:), 'r')\\
          }
    \item \underline{Reflections:} The linear tranformation that reflects points over the x-axis is given by
          $$\begin{bmatrix}x' \\ y'\end{bmatrix} =
              \begin{bmatrix}-1&0 \\ 0&1\end{bmatrix}\begin{bmatrix}x \\ y\end{bmatrix} =
              \begin{bmatrix}-x \\ y\end{bmatrix}$$.
          Similary, to reflect points across the >>-axis, the linear transformation will use the
          matrix $M=\begin{bmatrix}1&0 \\ 0&-1\end{bmatrix}$. As an example, we reflect our original CAT over the
          y-axis (Figure \ref{fig:fig_7_7}). We assume we have left in the graphics window the first cat
          of Figure \ref{fig:fig_7_3}.\\
          \mycode{
          >> M=[-1 0; 0 1];\\
          >> A1=M*A; hold on, plot(A1(1,:), A1(2,:), 'r')\\
          }
          \begin{figure}[h]
              \centering
              \includegraphics[width=0.5\linewidth]{fig_7_6}
              \caption{The rotation (red) of the original CAT (blue) using angle
                  $Θ = - \pi /4$. The point of rotation is the origin $(0,0)$.}
              \label{fig:fig_7_6}
          \end{figure}
          \begin{figure}[h]
              \centering
              \includegraphics[width=0.5\linewidth]{fig_7_7}
              \caption{The reflection (left) of the original CAT across the y-axis.}
              \label{fig:fig_7_7}
          \end{figure}
    \item \underline{Shifts:} Shifts are very simple and important transformations that are not linear
          transformations. For a fixed (shift) vector $V_0 = (x_0, y_0) \neq (0,0)$ that we identify,
          when convenient, with the column vector $\begin{bmatrix}x_0 \\ y_0 \end{bmatrix}$, the
          \textbf{shift transformation} $T_{V_{0}}$ associated with the shift vector $V_0$ is defined as follows:
          $$(x', y') = T_{V_{0}}(x,y) = (x,y)+V_0 = (x+x_0, y+y_0).$$

          What the shift transformation does is simply move all x-coordinates by $x_0$ units
          and move all y-coordinates by $y_0$ units. As an example we show the outcome of
          applying the shift transformation $T_{(i,j)}$) to our familiar CAT graphic. Rather than a
          matrix multiplication with the $2 \times 10$ CAT vertex matrix, we will need to add the
          corresponding $2 \times 10$ matrix, each of whose 10 columns is the shift column
          vector $\begin{bmatrix}1 \\ 1 \end{bmatrix}$ that we are using (Figure \ref{fig:fig_7_8})
          Once again, we assume the graphics window initially contains the original (white) CAT of
          Figure \ref{fig:fig_7_3} before we start to enter the following MATLAB commands (and that
          the CAT vertex matrix $A$ is still in the workspace).
          \begin{figure}[h]
              \centering
              \includegraphics[width=0.5\linewidth]{fig_7_8}
              \caption{The shifted CAT (upper right) came from the original CAT using a shift vector $(1,1)$.
                  So the cat was shifted one unit to the right and one unit up.}
              \label{fig:fig_7_8}
          \end{figure}\\
          \mycode{
              >>size(A) \%check size of A $\rightarrow$ans=2 10\\
              >> V=ones(2,10); A1=A+V; hold on, {plot(Al(1,:),Al(2,:), 'r')}\\
          }
\end{enumerate}

\begin{ExerciseForTheReader}
    Explain why the shift transformation is never a linear transformation.
\end{ExerciseForTheReader}

It is unfortunate that the shift transformation cannot be realized as a linear
transformation, so that we cannot realize it as using a $2 \times 2$ matrix multiplication
of our vertex matrix. If we could do this, then all of the important transformations
mentioned thus far could be done in the same way and it would make
combinations (and in particular making movies) an easier task. Fortumately there
is a way around this using so-called homogeneous coordinates. We first point out
a more general type of transformation than a linear transformation that includes all
linear transformations as well as the shifts. We define it only on the two-
dimensional space $R^2$ , but the definition carries over in the obvious way to the
three-dimensional space $R^3$ and higher-dimensional spaces as well. An
\textbf{affine transformation} on $R^2$ equals a linear tranformation and/or a shift (applied
together). Thus, an affine transformation can be written in the form:
\begin{equation} \label{equation:7_11}
    \begin{bmatrix}x' \\ y'\end{bmatrix} =
    M\begin{bmatrix}x \\ y\end{bmatrix}+V_0 =
    \begin{bmatrix}a & b \\ c & d\end{bmatrix}
    \begin{bmatrix}x \\ y\end{bmatrix} +
    \begin{bmatrix}x_0 \\ y_0\end{bmatrix}.
\end{equation}
The \textbf{homogeneous coordinates} of a point/vector $\begin{bmatrix}x \\ y\end{bmatrix}$
in \textbf{$R^2$} is the point/vector $\begin{bmatrix}x \\ y \\ 1\end{bmatrix}$ in \textbf{$R^3$}.
Note that the third coordinate of the identified three-dimensional
point is always 1 in homogeneous coordinates. Geometrically, if we identify a
point $(x,y)$ of \textbf{$R^2$} with the point $(x,y,0)$ in \textbf{$R^3$} (i.e., we identify \textbf{$R^2$} with the
plane $z = 0$ in \textbf{$R^3$}), then homogeneous coordinates simply lift all of these points
up one unit to the plane $z = 1$. It may seem at first glance that homogeneous
coordinates are making things more complicated, but the advantage in computer
graphics is given by the following result.

\begin{Theorem}
    \textit{(Homogeneous Coordinates)} Any affine transformation on R2
    is a linear transformation if we use homogeneous coordinates. In other words, any
    affine transformation T on R2 can be expressed using homogeneous coordinates
    in the form:
    \begin{equation}\label{equation:7_12}
        \begin{bmatrix}
            x' \\ y' \\ 1
        \end{bmatrix}
        =T\left({\begin{bmatrix}
                x \\ y \\ 1
            \end{bmatrix}}\right)
        =H\begin{bmatrix}
            x \\ y \\ 1
        \end{bmatrix}
    \end{equation}
    (matrix multiplication), where H is some $3 \times 3$ matrix.

    \noindent \textit{Proof:} The proof of the theorem is both simple and practical; it will show how to
    form the matrix $H$ in (\ref{equation:7_12}) from the parameters in (\ref{equation:7_11}) that determine the affine
    transformation.
    \begin{enumerate}[wide, labelindent=0pt, label=\textit{Case \arabic*:}]
        \item $T$ is a linear transformation on $R^2$ with matrix $M=\begin{bmatrix}a&b \\ c&d\end{bmatrix}$, i.e.,
              $T=\left({\begin{bmatrix}x\\y\end{bmatrix}}\right) = M\begin{bmatrix}x\\y\end{bmatrix} =
                  \begin{bmatrix}a&b \\ c&d\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix}$
              (no shift) In this case, the transformation can be expressed in homogeneous coordinates as:
              \begin{equation}\label{equation:7_13}
                  \begin{bmatrix}
                      x' \\ y' \\ 1
                  \end{bmatrix}
                  =T\left({\begin{bmatrix}
                          x \\ y \\ 1
                      \end{bmatrix}}\right)
                  = \begin{bmatrix}
                      a & b & 0 \\
                      c & d & 0 \\
                      0 & 0 & 1
                  \end{bmatrix}
                  \begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
                  =H\begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}.
              \end{equation}
              To check this identity, we simply perform the matrix multiplication:
              $$
                  \begin{bmatrix}
                      x' \\ y' \\ 1
                  \end{bmatrix}
                  = \begin{bmatrix}
                      a & b & 0 \\
                      c & d & 0 \\
                      0 & 0 & 1
                  \end{bmatrix}
                  \begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
                  = \begin{bmatrix}
                      ax + by + 0 \\
                      cx + dy + 0 \\
                      0 + 0 + 1
                  \end{bmatrix}
                  \Rightarrow
                  \begin{bmatrix}
                      x' \\ y'
                  \end{bmatrix}
                  = \begin{bmatrix}
                      ax + by \\
                      cx + dy \\
                  \end{bmatrix}
                  = M\begin{bmatrix}
                      x \\ y
                  \end{bmatrix},
              $$
              as desired.
        \item $T$ is a shift transformation on $R^2$ with shift vector $V_0 = \begin{bmatrix}x_0\\y_0\end{bmatrix}$,
              that is, $T\left({\begin{bmatrix}x\\y\end{bmatrix}}\right) = \begin{bmatrix}x\\y\end{bmatrix} + \begin{bmatrix}x_0\\y_0\end{bmatrix}$
              (so the matrix M in (\ref{equation:7_12}) is the indetity matrix). In this case, the transformation can be
              expressed in homogeneous coordinates as:
              \begin{equation}\label{equation:7_14}
                  \begin{bmatrix}
                      x' \\ y' \\ 1
                  \end{bmatrix}
                  =T\left({
                      \begin{bmatrix}
                          x \\ y \\ 1
                      \end{bmatrix}
                  }\right)
                  = \begin{bmatrix}
                      1 & 0 & x_0 \\
                      0 & 1 & y_0 \\
                      0 & 0 & 1
                  \end{bmatrix}
                  \begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
                  =H\begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
              \end{equation}
              We leave it to the reader to check, as was done in Case 1, that this homogeneous
              coordinate linear transformation does indeed represent the shift.
        \item The general case (linear transformation plus shift);
              $$
                  \begin{bmatrix}x' \\ y'\end{bmatrix}
                  =T\left({
                      \begin{bmatrix}
                          x \\ y
                      \end{bmatrix}
                  }\right)
                  \begin{bmatrix}x \\ y\end{bmatrix} =
                  \begin{bmatrix}a & b \\ c & d\end{bmatrix}
                  \begin{bmatrix}x \\ y\end{bmatrix} +
                  \begin{bmatrix}x_0 \\ y_0\end{bmatrix},
              $$
              can now be realized by putting together the matrices in the preceding two special cases:
              \begin{equation}\label{equation:7_15}
                  \begin{bmatrix}
                      x' \\ y' \\ 1
                  \end{bmatrix}
                  =T\left({
                      \begin{bmatrix}
                          x \\ y \\ 1
                      \end{bmatrix}
                  }\right)
                  = \begin{bmatrix}
                      a & b & x_0 \\
                      c & d & y_0 \\
                      0 & 0 & 1
                  \end{bmatrix}
                  \begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
                  =H\begin{bmatrix}
                      x \\ y \\ 1
                  \end{bmatrix}
              \end{equation}
              We leave it to the reader check this (using the distributive law (7)).
    \end{enumerate}
\end{Theorem}

The basic transformations that we have so far mentioned can be combined to
greatly expand the mutations that can be performed on graphics. Furthermore, by
using homogeneous coordinates, the matrix of such a combination of basic
transformations can be obtained by simply multiplying the matrices by the
individual basic transformations that are used, in the correct order, of course. The
next example illustrates this idea.

\begin{Example}
    Working in homogeneous coordinates, find the transformation
    that will rotate the CAT about the tip of its chin by an angle of $-90\degree{}$. Express the
    transformation using the $3 \times 3$ matrix $M$ for homogeneous coordinate
    multiplication, and then get MATLAB to create a plot of the transformed CAT
    along with the original.

    \begin{Solution}
        Since the rotations we have previously introduced will always rotate
        around the origin (0,0), the way to realize this transformation will be by combining
        the following three transformations (in order):
        \begin{enumerate}[wide, labelindent=0pt, label=(\roman*)]
            \item First shift coordinates so that the chin gets moved to ($0,0)$. Since the chin has
                  coordinates ($1.5, -1)$, the shift vector should be the opposite so we will use the
                  shift transformation
                  $$
                      T_{(-1.5,1)}\sim
                      \begin{bmatrix}
                          1 & 0 & -1.5 \\
                          0 & 1 & 1    \\
                          0 & 0 & 1
                      \end{bmatrix}
                      = H_1
                  $$
                  (the tilde notation is meant to indicate that the shift transformation is represented
                  in homogeneous coordinates by the given $3 \times 3$ matrix $H_1$, as specified by (14)).
            \item Next rotate (about $(0,0)$) by $Θ = -90\degree{}$ This rotation transformation $R$ has matrix
                  $$\begin{bmatrix}
                          cos(-90\degree{}) & -sin(-90\degree{}) \\
                          sin(-90\degree{}) & cos(-90\degree{})
                      \end{bmatrix}
                      = \begin{bmatrix}
                          0  & 1 \\
                          -1 & 0
                      \end{bmatrix},
                  $$
                  and so, by (\ref{equation:7_13}), in homogeneous coordinates is represented by
                  $$
                      R \sim \begin{bmatrix}
                          0  & 1 & 0 \\
                          -1 & 0 & 0 \\
                          0  & 0 & 1
                      \end{bmatrix}
                      =H_2.
                  $$
            \item Finally we undo the shift that we started with in (i), using
                  $$
                      T_{(-1.5,1)}\sim
                      \begin{bmatrix}
                          1 & 0 & -1.5 \\
                          0 & 1 & 1    \\
                          0 & 0 & 1
                      \end{bmatrix}
                      = H_3
                  $$
                  If we multiply each of these matrices (in order) on the left of the original
                  homogeneous coordinates, we obtain the transformed homogeneous coordinates:
                  $$
                      \begin{bmatrix}
                          x' \\ y' \\ 1
                      \end{bmatrix}
                      =H_3 H_2 H_1
                      \begin{bmatrix}
                          x \\ y \\ 1
                      \end{bmatrix}
                      =M\begin{bmatrix}
                          x \\ y \\ 1
                      \end{bmatrix},
                  $$
                  that is, the matrix M of the whole transformation is given by the product
                  $H_3 H_2 H_1$. We now turn things over to MATLAB to compute the matrix $M$ and
                  to plot the before and after plots of the CAT.\\
                  \mycode{
                  >> H1=[1 0 -1.5; 0 1 1; 0 0 1]; H2=[0 1 0; -1 0 0; 0 0 1]\\
                  >> H3=[1 0 1.5; 0 1 -1; 0 0 1];\\
                  >> format rat \%will give a nicer display of the matrix M\\
                  >> M=H3*H2*H1\\
                  $\rightarrow M = \begin{matrix}
                          0  & 1 & 5/2 \\
                          -1 & 0 & 1/2 \\
                          0  & 0 & 1
                      \end{matrix}$
                  }\\
                  We will multiply this matrix M by the matrix AH of homogeneous
                  coordinates corresponding to the matrix A. To form AH, we simply need to
                  tack on a row of ones to the bottom of the matrix A. (See Figure \ref{fig:fig_7_9}.)
                  \begin{figure}[h]
                      \centering
                      \includegraphics[width=0.4\linewidth]{fig_7_9}
                      \caption{The red CAT was obtained
                          from the blue cat by rotating -90\degree{} about
                          the chin. The plot was obtained using
                          homogeneous coordinates in Example \ref{example:7_3}.}
                      \label{fig:fig_7_9}
                  \end{figure}
                  \\
                  \mycode{
                      >> AH=A; \%start with A\\
                      >> size(A) \%check the size of A\\
                      $\rightarrow$ans = 2 10\\
                      >> AH(3,:)=ones(l,10); \% form the appropriately sized third row for All\\
                      >> size(AH) $\rightarrow$ans= 3 10\\
                      >> holdon, AH1=M*AH;\\
                      >> plot(AH1(1,:),AH1(2,:),'r')\\
                  }
        \end{enumerate}
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    Working in homogeneous coordinates,
    \begin{enumerate}[label=(\alph*)]
        \item find the transformation that will shift the CAT one unit to the right and then
              horizontally expand it by a factor of 2 (about $x = 0$) to make a "fat CAT". Express
              the transformation using the $3 \times 3$ matrix M for homogeneous coordinate
              multiplication, and then use MATLAB to create a plot of the transformed fat cat
              along with the original.
        \item  Next, find four transformations each shifting the cat by one of the following
              shift vectors $(\pm 1, \pm 1)$ (so that all four shift vectors are used) after having rotated
              the CAT about the central point $(1.5, 1.5)$ by each of the following angles: $30\degree{}$ for
              the upper-left CAT, $-30\degree{}$ for the upper-right CAT, $45\degree{}$ for the lower-left cat,
              and $-45\degree{}$ for the lower-right cat. Then fill in the four cats with four different
              (realistic cat) colors, and include the graphic.
    \end{enumerate}
\end{ExerciseForTheReader}

We now show how we can put graphics transformations together to create a
movie in MATLAB. This can be done in the following two basic steps:

\begin{center}
    \noindent \textbf{STEPS FOR CREATING A MOVIE IN MATLAB:}
    \begin{tabularx}{\linewidth}{ |X| }
        \hline
        \textit{Step 1:} Construct a sequence of MATLAB graphics that will make up the frames
        of the movie. After the yth frame is constructed, use the command \mycode{M(:,j)=getframe;}
        to store the frame as the $j$th column of some (movie) matrix $M$. \\
        \hline
        \textit{Step 2:}  To play the movie, use the command \mycode{movie(M, rep, fps)} , where M
        is the movie matrix constructed in step 1, \mycode{rep} is a positive integer giving the
        number of times the movie is to be (repeatedly) played, and \mycode{}{fps} denotes a
        positive integer giving the speed, in "frames per second," at which the movie is to
        be played.                                                         \\
        \hline
    \end{tabularx}
\end{center}

Our next example gives a very simple example of a movie. The movie star
will of course be the CAT, but this time we will give it eyes (Figure \ref{fig:fig_7_10}). For
this first example, we do not use matrix transformations, but instead we directly
edit (via a loop) the code that generates the graphic. Of course, a textbook
cannot play the movie, so the reader is encouraged to rework the example in
front of the computer and thus replay the movie.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{fig_7_10}
    \caption{The original CAT of
        Example \ref{example:7_3} with eyes added, the star of our first cat movie.}
    \label{fig:fig_7_10}
\end{figure}

\begin{Example}
    Modify the CAT graphic to have a black outline, to have
    two circular eyes (filled in with yellow),
    with two smaller black-filled pupils at the center of the eyes. Then make a movie
    of the cat closing and then reopening its eyes.

    \begin{Solution}
        The strategy will be as follows: To create the new CAT with the
        specified eyes, we use the "hold on" command after having created the basic CAT.
        Then we \mycode{fill} in yellow two circles of radius 0.4 centered at $(1, 2)$ (left eye) and
        at $(2, 2)$ (right eye); after this we fill in black two smaller circles with radii 0.15 at
        the same centers (for the pupils). The circles will actually be polygons obtained
        by parametric equations. To gradually close the eyes, we use a for loop to create
        CATs with the same outline but whose eyes are shrinking only in the vertical direction.

        This could be done with homogeneous coordinate transforms (that would shrink
        in the y direction each eye but maintain the centers—thus it would have to first
        shift the eyes down to $y = 0$, shrink and then shift back), or alternatively we could
        just directly modify the $y$ parametric equations of each eye to put a shrinking
        scaling factor in front of the sine function to turn the eyes both directly into a
        shrinking (and later expanding) sequence of ellipses. We proceed with the second
        approach. Let us first show how to create the CAT with the indicated eyes. We
        begin with the original CAT (this time with black line color rather than blue),
        setting the \mycode{axis} options as previously, and then enter \mycode{hold on}. Assuming this
        has been done, we can create the eyes as follows:\\
        \mycode{
            >> t=0:.02:2*pi; \%creates time vector for parametric equations\\
            >> x=l+.4*cos(t); y=2+.4*sin(t); \%creates circle for left eye.\\
            >> fill(x,y,'y') \%fills in left eye\\
            >> fill(x+l,y, 'y') \%fills in right eye\\
            >> x=l+.15*cos(t); y=2+.15*sin(t); \%creates circle for left pupil\\
            >> fill(x,y,'k') \%fills in left pupil\\
            >> fill (x+l,y,'k') \%fills in right pupil\\
        }

        To make the frames for our movie (and to "get" them), we employ a for loop that
        goes through the above construction of the "CAT with eyes", except that a factor
        will be placed in front of the sine term of the y-coordinates of both eyes and
        pupils. This factor will start at 1, shrink to 0, and then expand back to the value of
        1 again. To create such a factor, we need a function with starting value 1 that
        decreases to zero, then turns around and increases back to 1. One such function
        that we can use is $(l + cosx)/2$ over the interval $[0, 2 \pi]$. Below we give one
        possible implementation of this code:\\
        \mycode{
        >>t=0:.02:2*pi; counter=l; \\
        >>A=[0 0 .5 1 2 2.5 3 3 1.5 0; ... ... 0 3 4 3 3 4 3 0 -1] \\
        >>x=*l+.4*cos(t) ; xp=l+.15*cos(t) ; \\
        >> for s=0:.2:2*pi \\
        factor = (cos(s)+1)/2; \\
        plot(A(l,:), A(2,:), 'k') \\
        axis([-2 5 -3 6]), axis('equal') \\
        y=2+.4*factor*sin(t); yp=2+.15*factor*sin(t); \\
        hold on \\
        fill(x,y,'y'), fill(x+l,y, 'y'), fill(xp,yp,'k'), fill(xp+1,yp,·k') \\
        M(:, counter) = getframe; \\
        hold off, counter=counter+l; \\
        end \\
        }
        The movie is now ready for screening. To view it the reader might try one (or
        both) of the following commands.\\
        \mycode{
            >> movie(M,4,5) \%slow playing movie, four repeats\\
            >> movie(M,20,75) \%'smuch faster play of movie, with 20 repeats\\
        }
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    \begin{enumerate}[label=(\alph*)]
        \item Create a MATLAB function M-file, called \mycode{mkhom(A)}, that takes
              a $2 \times m$ matrix of vertices for a graphic (first row has x-coordinates
              and second row has corresponding y-coordinates) as input and outputs a
              corresponding $3 \times m$ matrix of homogeneous coordinates for the vertices.
        \item Create a MATLAB function M-file, called \mycode{rot(Ah,x0,y0,theta)} that has
              inputs, Ah, a matrix of homogeneous coordinates
              of some graphic, two real numbers, x0, y0 that are the coordinates of
              the center of rotation, and \mycode{theta}, the angle (in radians) of rotation. The output will
              be the homogeneous coordinate vertex matrix gotten from Ah by rotating the
              graph an angle \mycode{theta} about the point (x0, y0).
    \end{enumerate}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\linewidth]{fig_7_11}
        \caption{The more sophisticated cat star of the movie in Exercise for the reader \ref{exreader:7_7}(b).}
        \label{fig:fig_7_11}
    \end{figure}
\end{ExerciseForTheReader}

\begin{ExerciseForTheReader}
    \begin{enumerate}[label=(\alph*)]
        \item Recreate the above movie working in
              homogeneous coordinate transforms on the eyes.
        \item  By the same method, create a similar movie that stars a more sophisticated cat,
              replete with whiskers and a mouth, as shown in Figure \ref{fig:fig_7_11}. In this movie, the cat
              starts off frowning and the pupils will shift first to the left, then to the right, then
              back to center and finally up, down and back to center again, at which point the cat
              will wiggle its whiskers up and down twice and change its frown into a smile.
    \end{enumerate}
\end{ExerciseForTheReader}

\textbf{Fractals} or \textbf{fractal sets} are complicated and interesting sets (in either the plane
or three-dimensional space) that have the \textbf{self-similarity property} that if one
magnifies a certain part of the fractal (any number of times) the details of the
structure will look exactly the same.

The computer generation of fractals is also a hot research area and we will look at
some of the different methods that are extensively used. Fractals were gradually
discovered by mathematicians who were specialists in set theory or function
theory, including (among others) the very famous Georg F. L. P. Cantor
(1845-1918, German), Waclaw Sierpinski (1882-1969, Polish), Gastón Julia
(1893-1978, French) and Giuseppe Peano (1858-1932, Italian) during the late
nineteenth and early twentieth centuries. Initially, fractals came up as being
pathological objects without any type of unifying themes. Many properties of
factals that have shown them to be so useful in an assortment of fields were
discovered and popularized by the Polish/French mathematician Benoit
Mandelbrot(Figure \ref{fig:fig_7_12}).\footnote{
    Mandelbrot was born in Poland in 1924 and his family moved to France when he was 12 years old.
    He was introduced to mathematics by his uncle Szolem Mandelbrojt, who was a mathematics professor
    at the College de France. From his early years, though, Mandelbrot showed a strong preference for
    mathematics that could be applied to other areas rahter than the pure and rather abstruse type of
    mathematics on which his uncle was working. Since World War II was taking place during his school
    years, he often was not able to attend school and as a result much of his education was done at home
    through self-study. He attributes to this informal education the development of his strong geometric
    intuition. After earning his Ph.D. in France he worked for a short time at Cal Tech and the Institute for
    Advanced Study (Princeton) for postdoctoral work. He then returned to France to work at the Centre
    National de la Recherche Scientifique. He stayed at this post for only three years since he was finding
    it difficult to fully explore his creativity in the formal and traditional mathematics societies that
    dominated France in the mid-twentieth century (the "Bourbaki School"). He returned to the United
    States, taking a job as a research fellow with the IBM research laboratories. He found the atmosphere
    extremely stimulating at IBM and was able to study what he wanted. He discovered numerous
    applications and properties of fractals; the expanse of applications is well demonstrated by some of the
    other joint appointments he has held while working at IBM. These include Professor of the Practice of
    Mathematics at Harvard University, Professor of Engineering at Yale, Professor of Economics at
    Harvard, and Professor of Physiology at the Einstein College of Medicine. Many books have been
    written on fractals and their applications. For a very geometric and accessible treatment (with lots of
    beautiful pictures of fractals) we cite [Bar-93], along with [Lau-91]; see also [PSJY-92]. More analytic
    (and mathematically advanced) treatments are nicely done in the books [Fal-85] and [Mat-95].
} The precise definition of a fractal set takes a lot of
preliminaries; we refer to the references, for example, that are cited in the footnote
on this page for details. Instead of this, we will jump into some examples. The
main point to keep in mind is that all of the examples we give (in the text as well
as in the exercises) are actually impossible to print out exactly because of the self-
similarity property; the details would require a printer with infinite resolution.
Despite this problem, we can use loops or recursion with MATLAB to get some
decent renditions of fractals that, as far as the naked eye can tell (your printer's
resolution permitting), will be accurate illustrations.
Fractal sets are usually best described by an iterative
procedure that runs on forever.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{fig_7_12}
    \caption{Benoit Mandelbrot (b. 1924) Polish/French mathematician.}
    \label{fig:fig_7_12}
\end{figure}

\begin{Example}
    \textit{(The Sierpinski Gasket)} To obtain this fractal set, we begin with an equilateral
    triangle that we illustrate in gray in Figure \ref{fig:fig_7_13}(a); we call this set the zeroth generation. By
    considering the midpoints of each of the sides of this triangle, we can form four (smaller) triangles
    that are similar to the original. One is upside-down and the other three have the same orientation as the
    original. We delete this central upside down subtriangle from the zeroth generation to form the
    first generation (Figure \ref{fig:fig_7_13}(b)).
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.9\linewidth]{fig_7_13}
        \caption{Generation of the Sierpinski gasket of Example \ref{example:7_6}: (a) the zeroth
            generation (equilateral triangle), (b) first generation, (c) second generation. The generations
            continue on forever to form the actual set.}
        \label{fig:fig_7_13}
    \end{figure}

    Next, on each of the three (equilateral) triangles that make up this first
    generation, we again perform the same procedure of deleting the upside-down
    central subtriangle to obtain the generation-two set (Figure \ref{fig:fig_7_13}(c)). This process
    is to continue on forever and this is how the Sierpinski gasket set is formed. The
    sixth generation is shown in Figure \ref{fig:fig_7_14}.
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.4\linewidth]{fig_7_14}
        \caption{Sixth generation of the Sierpinski gasket fractal of Example \ref{example:7_6}.}
        \label{fig:fig_7_14}
    \end{figure}

    Notice that higher generations become indistinguishable to the naked eye, and
    that if we were to focus on one of the three triangles of the first generation, the
    Sierpinski gasket looks the same in this triangle as does the complete gasket. The
    same is true if we were to focus on any one of the nine triangles that make up the
    second generation, and so on.
\end{Example}

\begin{ExerciseForTheReader}
    \begin{enumerate}[label=(\alph*)]
        \item Show that the nth generation of the
              Sierpinski triangle is made up of $3^n$ equilateral triangles. Find the area of each of
              these $n$th-generation triangles, assuming that the initial sidelengths are one.
        \item Show that the area of the Sierpinski gasket is zero.
    \end{enumerate}
    NOTE: It can be shown that the Sierpinski gasket has dimension $log4/log3 = 1.2618 \dots$,
    where the dimension of a set is a rigorously defined measure of its
    true size. For example, any countable union of line segments or smooth arcs is of
    dimension one and the inside of any polygon is two-dimensional. Fractals have
    dimensions that are nonintegers. Thus a fractal in the plane will have dimension
    somewhere (strictly) between 1 and 2 and a fractal in three-dimensional space will
    have dimension somewhere strictly between 2 and 3. None of the standard sets in
    two and three dimensions have this property. This noninteger dimensional
    property is often used as a definition for fractals. The underlying theory is quite
    advanced; see [Fal-85] or [Mat-95] for more details on these matters.
\end{ExerciseForTheReader}

In order to better understand the self-similarity property of fractals, we first recall
from high-school geometry that two triangles are similar if they have the same
angles, and consequently their corresponding sides have a fixed ratio. A
similarity transformation (or similitude for short) on R 2 is an affine
transformation made up of one or more of the following special transformations:
scaling (with both x- and y-factors equal), a reflection, a rotation, and/or a shift.
In homogeneous coordinates, it thus follows that a similitude can be expressed in
matrix form as follows:
\begin{equation}\label{equation:7_16}
    \begin{bmatrix}
        x' \\ y' \\ 1
    \end{bmatrix}
    =T\left({
        \begin{bmatrix}
            x \\ y \\ 1
        \end{bmatrix}
    }\right)
    = \begin{bmatrix}
        s\cos0     & -s\sin0    & x_0 \\
        \pm s\sin0 & \pm s\cos0 & y_0 \\
        0          & 0          & 1
    \end{bmatrix}
    \begin{bmatrix}
        x \\ y \\ 1
    \end{bmatrix}
    =H\begin{bmatrix}
        x \\ y \\ 1
    \end{bmatrix},
\end{equation}
where $s$ can be any nonzero real number and the signs in the second row of H
must be the same. A scaling with both x- and y-factors being equal is
customarily called a \textbf{dilation}.

\begin{ExerciseForTheReader}
    \begin{enumerate}[label=(\alph*)]
        \item Using Theorem \ref{theorem:7_2} (and its proof),
              justify the correctness of (\ref{equation:7_16}).
        \item Show that for any two similar triangles in the plane there is a similitude that
              transforms one into the other.
        \item Show that if any particular feature (e.g., reflection) is removed from the
              definition of a similitude, then two similar triangles in the plane can be found,
              such that one cannot be transformed to the other by this weaker type of
              transformation.
    \end{enumerate}
\end{ExerciseForTheReader}

The self-similarity of a fractal means, roughly, that for the whole fractal (or at
least a critical piece of it), a set of similitudes $S_1, S_2, \dots, S_K$ can be found (the
number $K$ of them will depend on the fractal) with the following property: All
$S_j$'s have the same scaling factor $s < 1$ so that F can be expressed as the union of
the transformed images $F_i = S_i(F)$ and these similar (and smaller) images are
essentially disjoint in that different ones can have only vertex points or boundary
edges in common. Many important methods for the computer generation of
fractals will hinge on the discovery of these similitudes $S_1, S_2, \dots, S_K$. Finding
them also has other uses in both the theory and application of fractals. These concepts
will be important in Methods 1 and 2 in our solution of the following example.

\begin{Example}
    Write a MATLAB function M-file that will produce graphics for the Sierpinski gasket fractal.

    \begin{Solution}
        We deliberately left the precise syntax of the M-file open since we
        will actually give three different approaches to this problem and produce three
        different M-files. The first method is a general one that will nicely take advantage
        of the self-similarity of the Sierpinski gasket and will use homogeneous coordinate
        transform methods. It was, in fact, used to produce high-quality graphic of Figure
        \ref{fig:fig_7_14}. Our second method will illustrate a different approach, called the
        \textbf{Monte Carlo method}, that will involve an iteration of a random selection process to
        obtain points on the fractal, and will plot each of the points that get chosen.
        Because of the randomness of selection, enough iterations produce a reasonably
        representative sample of points on the fractal and the resulting plot will give a
        decent depiction of it. Monte Carlo is a city on the French Riviera known for its
        casinos (it is the European version of Las Vegas). The method gets its name from
        the random (chance) selection processes it uses. Our third method works similarly
        to the first but the ideas used to create the M-file are motivated by the special
        structure of the geometry, in this case of the triangles.
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.4\linewidth]{fig_7_15}
            \caption{The three natural similitudes $S_1, S_2, S_3$
                for the Sierpinski gasket with vertices $V_1, V_2, V_3$ shown on the zeroth and
                first generations. Since the zeroth generation is an equilateral triangle,
                so must be the three triangles of the first generation.
            }
            \label{fig:fig_7_15}
        \end{figure}
        \begin{enumerate}[wide, labelindent=0pt, label=\textit{Method \arabic*:}]
            \item Method 1: The Sierpinski gasket has
                  three obvious similitudes, each of which transforms it into one of the
                  three smaller "carbon copies" of it that lie in the three triangles of the
                  first generation (see Figure \ref{fig:fig_7_15}). These similitudes have very simple
                  form, involving only a dilation (with factor 0.5) and shifts.
                  The firsttransformation $S_1$ involves no shift. Referring to the figure, it is clear that $S_2$
                  must shift V to the midpoint of the line segment $V_1V_2$ that is given by
                  (as a vector) $(V_1+V_2)I2.$. The shift vector needed to do this, and hence
                  the shift vector for $S_2$ is $(V_2-V_1)/2$. (Proof: If we shift Vx by this vector we get
                  $V_1 + (V_2 - V_1)/2 = (V_2 + V_1)/2$.) Similarly the shift vector for Ss is ($V_3 -V_1)2$. h
                  follows that the corresponding matrices for these three similitudes are as given below:
                  $$
                      S_1\sim
                      \begin{bmatrix}
                          .5 & 0  & 0 \\
                          0  & .5 & 0 \\
                          0  & 0  & 1
                      \end{bmatrix},
                      S_2\sim
                      \begin{bmatrix}
                          .5 & 0  & (V_2(1) - V_1(1))/2 \\
                          0  & .5 & (V_2(2) - V_1(2))/2 \\
                          0  & 0  & 1
                      \end{bmatrix},
                      S_3\sim
                      \begin{bmatrix}
                          .5 & 0  & (V_3(1) - V_1(1))/2 \\
                          0  & .5 & (V_3(2) - V_1(2))/2 \\
                          0  & 0  & 1
                      \end{bmatrix}.
                  $$
                  Program \ref{program:7_1}, \mycode{sgasketl (V1,V2,V3,ngen)}, has four input \mycode{V1,V2,V3}
                  should be the row vectors representing the vertices $(0,0)$, $(1,\sqrt{3})$,$(2,0)$ of
                  a particular equilateral triangle, and ngen is the generation number of the
                  Sierpinski gasket to be drawn. The program has no output variables, but will
                  produce a graphic of this generation ngen of the Sierpinski gasket. The idea
                  behind the algorithm is the following. The three triangles making up the
                  generation-one gasket can be obtained by applying each of the three special
                  similitudes $S_1,S_2,S_3$ to the single generation-zero Gasket. By the same token,
                  each of the nine triangles that comprise the generation-two gasket can be obtained
                  by applying one of the similitudes of $S_1,S_2,S_3$ to one of the generation-one
                  triangles. In general, the triangles that make up any generation gasket can be
                  obtained as the union of the triangles that result from applying each of the
                  similitudes $S_1,S_2,S_3$ to each of the previous generation triangles. It works with
                  the equilateral triangle having vertices $(0,0),(1,\sqrt{3}),(2,0)$. The program makes
                  excellent use of recursion.
                  \begin{Program}
                      unction M-file for producing a graphic of any generation of the
                      Sierpinski gasket on the special equilateral triangle with vertices $(0,0),(1,\sqrt{3}),(2,0)$
                      (written with comments in a way to make it easily modified to work for other fractals).
                      \begin{lstlisting}[numbers=none]
function sgasketl(V1,V2,V3,ngen)
%input variables: V1,V2,V3 should be the vertices [0,0], [1,sqrt(3)];,
%and [2,0] of a particular equilateral triangle in the plane taken as
$row vectors, ngen is the number of iterations to perform in 
%Sierpinski gasket generation.
%The gasket will be drawn in medium gray color.

%first form matrices for similitudes
    S1=[.5 0 0;0 .5 0;0 0 1];
    S2=[.5 0 1; 0 .5 0;0 0 1 ];
    S3=[.5 0 .5; 0 .5 sqrt(3)/2;0 0 1 ];

if ngen == 0
    %Fill triangle
    fill([V1(1) V2(1) V3(1) V1(1)], [V1(2) V2(2) V3(2) V1(2)], [.5 .5 .5])
    hold on
else
%recursively invoke the same function on three outer subtriangles
%form homogeneous coordinate matrices for three vertices of triangle
    A=[V1; V2 ; V3]; A(3,:)=[1 1 1];
    %next apply the similitudes to this matrix of coordinates
    A1=S1*A; A2=S2*A; A3=S3*A;
    %finally, reappiy sgasketl to the corresponding three triangles with
    %ngen bumped down by 1. Mote, vertex vectors have to be made into
    %row vectors using '(transpose).
    sgasketl(Al([1 2], 1)', A1([1 2], 2)', A1([1 2], 3)', ngen-1)
    sgasketl(A2([1 2], 1)', A2([1 2], 2)', A2([1 2], 3)', ngen-1)
    sgasketl(A3([1 2], 1)', A3([1 2], 2)', A3([1 2], 3)', ngen-1)
end
                    \end{lstlisting}
                  \end{Program}
                  To use this program to produce, for example, the generation-one graphic of Figure \ref{fig:fig_7_13}(b), one need only enter:\\
                  \mycode{ >> sgasketl([0 0], [1 sqrt(3)], [2 0], 1)}\\
                  If we wanted to produce a graphic of the more interesting generation-six
                  Sierpinski gasket of Figure \ref{fig:fig_7_15}, we would have only to change the last input
                  argument from 1 to 6. Note, however, that this function left the graphics window
                  with a h o l d on. So before doing anything else with the graphics window after
                  having used it, we would need to first enter \mycode{hold off}. Alternatively, we could
                  also use the following command:
                  \begin{center}
                      \begin{tabularx}{\linewidth}{ |X|X| }
                          \hline
                          \mycode{clf} $\rightarrow$ & Clears the graphics window. \\
                          \hline
                      \end{tabularx}
                  \end{center}
                  In addition to recursion, the above program makes good use of MATLAB's
                  elaborate matrix manipulation features. It is important that the reader fully
                  understands how each part of the program works. To this end the following
                  exercise should be useful.
                  \begin{ExerciseForTheReader}
                      \begin{enumerate}[label=(\alph*)]
                          \item uppose the above program is invoked
                                with these input variables: V1 = [0 0], V2 = [1 $\sqrt{3}$], K3 = [2 0], ngen = 1. On the
                                first run/iteration, what are the numerical values of each of the following variables:\\
                                $A,A1,A2,A3,A1([1 \text{ } 2],2),A3([1 \text{ } 2],3)$?
                          \item Is it possible to modify the above program so that after the graphic is drawn,
                                the screen will be left with \mycode{hold off}? If yes, show how to do it; if not, explain.
                          \item In the above program, the first three input variables V1,V2,V3 seem a bit
                                redundant since we are forced to input them as the vertices of a certain triangle
                                (which gave rise to the special similitudes S1, S2, and S3). Is it possible to
                                rewrite the program so that it has only one input variable ngen? If yes, show how
                                to do it; if not, explain.
                      \end{enumerate}
                  \end{ExerciseForTheReader}
            \item The Monte Carlo method also will use the special similitudes, but its
                  philosophy is very different from that of the first method. Instead of working on a
                  particular generation of the Sierpinki gasket fractal, it goes all out and tries to
                  produce a decent graphic of the actual fractal. This gets done by plotting a
                  representative set of points on the fractal, a random sample of such. Since so
                  much gets deleted from the original triangle, a good question is What points
                  exactly are left in the Sierpinski gasket? Certainly the vertices of any triangle of
                  any generation will always remain. Such points will be the ones from which the
                  Monte Carlo method samples. Actually there are a lot more points in the fractal
                  than these vertices, although such points are difficult to write down. See one of
                  the books on fractals mentioned earlier for more details.

                  Here is an outline of how the program will work. We start off with a point we
                  call "Float" that is a vertex of the original (generation-zero) triangle, say VI. We
                  then randomly choose one of the similitudes from $S_1,S_2,S_3$, and apply this to
                  "Float" to get a new point "New," that will be the corresponding vertex of the
                  generation-one triangle associated with the similitude that was used (lower left for
                  $S_1$, upper middle for $S_3$ , and lower right for $S_2$). We plot "New," redefine
                  "Float" = "New," and repeat this process, again randomly selecting one of the
                  similitudes to apply to "Float" to get a new point "New" of the fractal that will be
                  plotted. At the Mh iteration, "New" will be a vertex of one of the Mh-generation
                  triangles (recall there are 3" such triangles) that will also lie in one of the three
                  generation-one triangles, depending on which of $S_1,S_2,S_3$ had been randomly
                  chosen. Because of the randomness of choices at each iteration, the points that are
                  plotted usually give a decent rendition of the fractal, as long as a large enough
                  random sample is used (i.e., a large number of iterations).
                  \begin{Program}
                      Function M-file for producing a Monte Carlo approximation graphic of
                      Sierpinski gasket, starting with the vertices V1, V2, and V3 of any equilateral triangle
                      (written with comments in a way that will make it easily modified to work for other fractals).
                      \begin{lstlisting}[numbers=none]
function [ ] = sgasket2(V1,V2,V3,niter)
%input variables: Vl,V2,V3 are vertices of an equilateral triangle in
%the plane taken as row vectors is, niter is the number of iterations
%used to obtain points in the fractal. The output will be a plot of
%all of the points. if niter: is not specified, the. default value
%of 5000 is used.
%if only 3 input arguments ar e given (nargin==3), set niter to
%default
if nargin == 3, niter = 5000; end

%Similitude matrices for Sierpinski gasket.
Sl=[.5 0 0;0 .5 0;0 0 1];
S2=[.5 0 (V2(1)-V1(1))/2; 0 . 5 (V2(2)-Vl(2))/2;0 0 1];
S3=[.5 0 (V3(l)-Vl(l))/2; 0 . 5 (V3(2)-Vl(2))/2;0 0 1];

%Probability vector for Sierpinski gasket has equal probabilities
%(1/3) for choosing one of the three similitudes.
P = [1/3 2/3];

%prepare graphics window for repeated plots of points
clf, axis('equal'); hold on;

%introduce "floating point" (can be any vertex) in homogeneous
%coordinates
Float=[Vl(l);Vl(2);1];
i = 1; %initialize iteration counter

%Begin iteration for creating new floating points and lotting each
$one that arises.

while i <= niter 
    choice = rand;
    if choice < P(1);
        New = S1 * Float;
        plot (New(1), New(2));
    elseif choice < P(2);
        New = S2 * Float;
        plot (New(1), New(2));
    else
        New = S3 = Float;
        plot (New (1), New(2));
    end;
    Float=New;  i = i + 1;
end
hold off
                    \end{lstlisting}
                  \end{Program}
                  Unlike the last program, this one allows us to input the vertices of any equilateral
                  triangle for the generation-zero triangle. The following two commands will
                  invoke the program first with the default 5000 iterations and then with 20,000 (the
                  latter computation took several seconds).\\
                  \mycode{
                      >> sgasket2([0 0], [1 sqrt(3)], [2 0])\\
                      >> sgasket2([0 0], [1 sqrt(3)], [2 0] , 20000)\\
                  }
                  The results are shown in Figure \ref{fig:fig_7_16}. The following exercise should help the
                  reader better undertstand how the above algorithm works.
                  \begin{ExerciseForTheReader}
                      Suppose that we have generated the
                      following random numbers (between zero and one): .5672, .3215, .9543, .4434,
                      .8289, .5661 (written to 4 decimals).
                      \begin{enumerate}
                          \item What would be the corresponding sequence of similitudes chosen in the above
                                program from these random numbers?
                          \item If we used the vertices [0 0], [1 sqrt(3)], [2 0] in the above program, find the
                                sequence of different "Float" points of the fractal that would arise if the above
                                sequence of random numbers were to come up.
                          \item What happens if the vertices entered in the program s g a s k e t 2 are those of a
                                nonequilateral triangle? Will the output ever look anything like a Sierpinski
                                gasket? Explain.
                      \end{enumerate}
                  \end{ExerciseForTheReader}
                  \begin{figure}[h]
                      \centering
                      \includegraphics[width=0.9\linewidth]{fig_7_16}
                      \caption{Monte Carlo renditions of the Sierpinski gasket via the program
                          sgasket2. The left one (a) used 5000 iterations while the right one (b) used 20,000 and
                          took noticeably more time.
                      }
                      \label{fig:fig_7_16}
                  \end{figure}
            \item The last program we write here will actually be the shortest and most
                  versatile of the three. Its drawback is that, unlike the other two, which made use
                  of the specific similitudes associated with thefractal,this program uses the special
                  geometry of the triangle and thus will be more difficult to modify to work for other
                  fractals. The type of geometric/mathematical ideas present in this program,
                  however, are useful in writing other graphics programs. The program
                  \mycode{sgasket3(VI,V2,V3,ngen)} takes as input three vertices \mycode{V1, V2, V3} of a
                  triangle (written as row vectors), and a positive integer ngen. It will produce a
                  graphic of the ngen-generation Sierpinski gasket, as did the first program. It is
                  again based on the fact that each triangle from a positive generation gasket comes
                  in a very natural way from the triangle of the previous generation in which it lies.
                  Instead of using similitudes and homogeneous coordinates, the program simply
                  uses explicit formulas for the vertices of the (N + l)st-generation triangles that lie
                  within a certain $N$th-generation triangle. Indeed, for any triangle from any
                  generation of the Sierpinski gasket with vertices $V_1, V_2, V_3$
                  three subtriangles of this one form the next generation (see Figure \ref{fig:fig_7_15}),
                  each has one vertex from this set, and the other two are the midpoints from this vertex
                  to the other two. For example (again referring to Figure \ref{fig:fig_7_15}) the lower-right triangle will
                  have vertices $V_2, (V_1 + V_2) / 2 =$ the midpoint of $V_2V_1$, and $(V_2 + V_3)/2$ = the midpoint of $V_2V_3$.
                  This simple fact, plus recursion, is the idea behind the following program.
                  \begin{Program}
                      Function M-file for producing a graphic of any generation of the
                      Sierpinski gasket for an equilateral triangle with vertices \mycode{V1, V2} and \mycode{V3}.
                      \begin{lstlisting}[numbers=none]
function sgasket3(VI,V2,V3,ngen)
%input variables V1,V2,V3 are  vertices of a triangle in the plane,
%written as row vectors, ngen is the generation of Sierpinski gasket
%that, will bo drawn in mod i um a ray color.
if ngen == 0
%fill triangle
 fill([V1(1) V2(1) V(3) V1(1)],...
 [V1(2) V2(2) V3(2) V1(2)], [.5 .5 .5])
   hold on
   else
%recursively invoke the sane function on three outer subtriangles
   sgasket3(Vl, (V1+V2)/2, (V1+V3)/2, ngen-1)
   sgasket3(V2, (V2+V1)/2, (V2+V3)/2, ngen-1)
   sgasket3(V3, (V3+V1)/2, (V3+V2)/2, ngen-1)
end
                    \end{lstlisting}
                  \end{Program}
                  \begin{ExerciseForTheReader}
                      \begin{enumerate}
                          \item What happens if the vertices entered in
                                the program \mycode{sgasket3} are those of a nonequilateral triangle? Will the output
                                ever look anything like a Sierpinski gasket? Explain.
                          \item The program \mycode{sgasket3} is more elegant than \mycode{sgasket1} and it is also more
                                versatile in that the latter program applies only to a special equilateral triangle.
                                Furthermore, it also runs quicker since each iteration involves less computing.
                                Justify this claim by obtaining some hard evidence by running both programs (on
                                the standard equilateral triangle of \mycode{sgasket1} ) and comparing \mycode{tic/toe} and
                                flop counts (if available) for each program with the following values for \mycode{ngen}: 1,3,6,8,10.
                      \end{enumerate}
                  \end{ExerciseForTheReader}
        \end{enumerate}
    \end{Solution}

    Since programs like the one in Method 3 of the above example are usually the
    most difficult to generalize, we close this section with yet another exercise for the
    reader that will ask for such a program to draw an interesting and beautiful fractal
    known as the von \textbf{Koch}\footnote{
        The von Koch snowflake was introduced by Swedish mathematician Niels F. H. von Koch
        (1870-1924) in a 1906 paper Une methode geometrique elementaire pour Vetude de certaines
        questions de la theorie des courbes planes. In it he showed that the parametric equations for the curve
        $(x(r), y(t))$ give an example of functions that are everywhere continuous but nowhere differentiable.
        Nowhere differentiable, everywhere continuous functions had been first discovered in 1860 by German
        mathematician Karl Weierstrass (1815-1897), but the constructions known at this time all involved
        very complicated formulas. Von Koch's example thus gives a curve (of infinite arclength) that is
        continuous everywhere (no breaks), but that does not have a tangent line at any of its points. The von
        Koch snowflake has been used in many areas of analysis as a source of examples.
    } \textbf{snowflake}, which is illustrated in Figure \ref{fig:fig_7_17}.
    The iteration scheme for this fractal is shown in Figure \ref{fig:fig_7_18}.
    \begin{ExerciseForTheReader}
        Create a MATLAB function, call it
        \mycode{snow(n)}, that will input a positive integer $n$ and will produce the $n$th generation
        of the so-called von Koch snowflake fractal. Note that we start off (generation 0)
        with an equilateral triangle with sidelength 2. To get from one generation to the
        next, we do the following: For each line segment on the boundary, we put up (in
        the middle of the segment) an equilateral triangle of 1/3 the sidelength. This
        construction is illustrated in Figure \ref{fig:fig_7_18}, which contains the first few generations
        of the von Koch snowflake. Run your program (and include the graphical printout) for the values: n = 1, n = 2, and n = 6.\\
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.4\linewidth]{fig_7_17}
            \caption{The von Koch snowflake fractal. This illustration was produced by the
                MATLAB program snow (n) of Exercise for the Reader \ref{exreader:7_13}, with an input
                value of 6 (generations).
            }
            \label{fig:fig_7_17}
        \end{figure}

        \noindent \textbf{Suggestions:} Each generation can be obtained by plotting its set of vertices (using
        the plot command). You will need to set up a for loop that will be able to produce
        the next generation's vertices from those of a given generation. It is helpful to
        think in terms of vectors.
        \begin{figure}[h]
            \centering
            \includegraphics[width=0.6\linewidth]{fig_7_18}
            \caption{Some initial generations of the von Koch snowflake. Generation zero is an
                equilateral triangle (with sidelength 2). To get from any generation to the next, each line
                segment on the boundary gets replaced by four line segments each having 1/3 of the length
                of the original segment. The first and fourth segments are at the ends of the original
                segment and the middle two segments form two sides of an equilateral triangle that
                protrudes outward.
            }
            \label{fig:fig_7_18}
        \end{figure}
    \end{ExerciseForTheReader}
\end{Example}

\begin{Exercises}
    NOTE: In the problems of this section, the "CAT' refers to the graphic of Example \ref{example:7_2} (Figure \ref{fig:fig_7_3}(a)),
    the "CAT with eyes" refers to the enhanced version graphic of Example \ref{example:7_5} (Figure \ref{fig:fig_7_10}), and the "full
    CAT" refers to the further enhanced CAT of Exercise for the Reader \ref{exreader:7_7}(b) (Figure \ref{fig:fig_7_11}). When asked
    to print a certain transformation of any particular graphic (like the CAT) along with the original, make
    sure to print the original graphic in one plotting style/color along with the transformed graphic in a
    different plotting style/color. Also, in printing any graphic, use the \mycode{axis(equal)} setting to prevent
    any distortions and set the axis range to accommodate all of the graphics nicely inside the bounding box
    \begin{enumerate}
        \item Working in homogeneous coordinates, what is the transformation matrix M that will scale the
              CAT horizontally by a factor of 2 (to make a "fat CAT") and then shift the cat vertically down a
              distance 2 and horizontally 1 unit to the left? Create a before and after graphic of the CAT.
        \item Working in homogeneous coordinates, what is the transformation matrix M that will double the
              size of the horizontal and vertical dimensions of the CAT and then rotate the new CAT by an
              angle of $45\degree{}$ about the tip of its left ear (the double-sized cat's left ear, that is)? Include a
              before-and-after graphic of the CAT.
        \item Working in homogeneous coordinates, what is the transformation matrix M that will shift the
              left eye and pupil of the "CAT with eyes" by 0.5 units and then expand them both by a factor of
              2 (away from the centers)? Apply this transformation just to the left eye. Next, perform the
              analogous transformation to the CAT's right eye and then plot these new eyes along with the
              outline of the CAT, to get a cat with big eyes.
        \item Working in homogeneous coordinates, what is the transformation matrix M that will shrink the
              "CAT with eyes"'s left eye and left pupil by a factor of 0.5 in the horizontal direction (toward
              the center of the eye) and then rotate them by an angle of $25\degree{}$ ? Apply this transformation just
              to the left eye, reflect to get the right eye, and then plot these two along with the outline of the
              CAT, to get a cat with thinner, slanted eyes.
        \item \begin{enumerate}
                  \item Create a MATLAB function M-file, called \mycode{refix(Ah,x0)} that has inputs, Ah, a matrix
                        of homogeneous coordinates of some graphic, and a real number x0. The output will be the
                        homogeneous coordinate vertex matrix obtained from Ah by reflecting the graphic over the
                        line $x = x0$ . Apply this to the CAT graphic using $x0 = 2$, and give a before-and-after plot.
                  \item Create a similar function M-file \mycode{refly(Ah,y0)} for vertical reflections (about the
                        horizontal line $y - y0$ ) and apply to the CAT using $y0 = 4$ to create a before and after plot.
              \end{enumerate}
        \item \begin{enumerate}
                  \item Create a MATLAB function M-file, called \mycode{shift(Ah,x0,y0)} that has as inputs Ah, a
                        matrix of homogeneous coordinates of some graphic, and a pair of real numbers \mycode{x0, y0}. The
                        output will be the homogeneous coordinate vertex matrix obtained from Ah by shifting the
                        graphic using the shift vector $(x0, y0)$. Apply this to the CAT graphic using \mycode{x0 - 2} and
                        \mycode{y0 = -1} and give a before-and-after plot.
                  \item Create a MATLAB function M-file, called \mycode{scale(Ah,a,b,x0,y0)} that has inputs Ah,
                        matrix of homogeneous coordinates of some graphic, positive numbers: a and b that represent
                        the horizontal and vertical scaling factors, and a pair of real numbers x0, y0 that represent the
                        coordinates about which the scaling is to be done. The output will be the homogeneous
                        coordinate vertex matrix obtained from Ah by scaling the graphic as indicated. Apply this to
                        the CAT graphic using $a = .25$, $b = 5$ once each with the following sets for ($x0,y0): (0,0), (3,0),
                            (0,3), (2.5,4)$ and create a single plot containing the original CAT along with all four of these
                        smaller, thin cats (use five different colors/plot styles).
              \end{enumerate}
        \item Working in homogeneous coordinates, what is the transformation matrix M that will reflect an
              image about the line $y = x$ ? Create a before-and-after graphic of the CAT.\\
              \textbf{Suggestion:} Rotate first, reflect, and then rotate back again.
        \item Working in homogeneous coordinates, what is the transformation matrix $M$ that will shift the
              left eye and left pupil of the "CAT with eyes" to the left by .0.5 units and then expand them by
              a factor of 2 (away from the centers)? Apply this transformation just to the left eye, reflect to
              get the right eye, and then plot these two along with the outline of the "CAT with eyes," to get a
              cat with big eyes.
        \item The \textbf{shearing} on \textbf{R$^2$} that shears by 6 in the x-direction and d in the y-direction is the linear
              transformation whose matrix is $\begin{bmatrix} 1 & b \\ c & 1\end{bmatrix}$. Apply the shearing to the CAT using several different
              values of $b$ when $c - 0$, then set $b = 0$ and use several different values of $c$, and finally apply
              some shearings using several sets of nonzero values for $b$ and $c$.1
        \item \begin{enumerate}
                  \item Show that the $2 \times 2$ matrix $\begin{bmatrix} cos0 & -sin0 \\ sin0 & cos0\end{bmatrix}$,
                        which represents the linear transformation for rotations by angle Θ, is invertible,
                        with inverse being the corresponding matrix for rotations by angle -Θ.
                  \item Does the same relationship hold true for the corresponding $3 \times 3$ homogeneous coordinate
                        transform matrices? Justify your answer.
              \end{enumerate}
        \item \begin{enumerate}
                  \item Show that the $3 \times 3$ matrix $\begin{bmatrix} 1 & 0 & x_0 \\ 0 & 1 & y_0 \\ 0 & 0 & 1\end{bmatrix}$,
                        which represents the shift with shift vector $\begin{bmatrix} x_0  \\ y_0 \end{bmatrix}$
                        is invertible, with its inverse being the corresponding matrix for the shift using the opposite shift vector.
              \end{enumerate}
        \item Show that the $2 \times 2$ matrix $\begin{bmatrix} cos0 & -sin0 \\ sin0 & cos0\end{bmatrix}$ indeed represents the linear
              transformation for rotations by angle Θ around the origin (0,0).\\
              \textbf{Suggestion:} Let $(x,y)$ have polar coordinates $(r, \alpha)$; then $(x',y')$ has polar coordinates
              $(r, \alpha + Θ)$. Convert the latter polar coordinates to rectangular coordinates.
        \item \textit{(Graphic Art: Rotating Shrinking Squares)}
              \begin{enumerate}
                  \item By starting off with a square, and repeatedly
                        shrinking it and rotating it, get MATLAB to create a graphic similar to the one shown in Figure \ref{fig:fig_7_19}(a).
                  \item Next modify your construction to create a graph similar to the one in Figure \ref{fig:fig_7_19}(b) but that
                        uses alternating colors.
              \end{enumerate}
              \textbf{Note:} This object is not a fractal.
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.6\linewidth]{fig_7_19}
                  \caption{A rotating and shrinking square of Exercise 13: (a) (left) with no fills; (b)
                      (right) with alternate black-and-white fills.}
                  \label{fig:fig_7_19}
              \end{figure}
        \item \textit{(Graphic Art: Cat with Eyes Mosaic)} The cat mosaic of Figure \ref{fig:fig_7_20} has been created by taking
              the original CAT, and creating new pairs of cats (left and right) for each step up. This
              construction was done with a for loop using 10 iterations (so there are 10 pairs of cats above the
              original), and could easily have been changed to any number of iterations. Each level upward of
              cats got scaled to 79\% of the preceding level. Also, for symmetry, the left and right cats were
              shifted upward and to the left and right by the same amounts, but these amounts got smaller
              (since the cat size did) as we moved upward.
              \begin{enumerate}
                  \item Use MATLAB to create a picture that is similar to that of Figure \ref{fig:fig_7_20}, but replace the "CAT
                        with eyes" with the ordinary CAT.
                  \item Use MATLAB to create a picture that is similar to that of Figure \ref{fig:fig_7_20}.
                  \item Use MATLAB to create a picture that is similar to that of Figure \ref{fig:fig_7_20}, but replace the "CAT
                        with eyes" with the "full CAT" of Figure \ref{fig:fig_7_11}.
              \end{enumerate}
              \textbf{Suggestion:} You should definitely use a for loop. Experiment a bit with different schemes for
              horizontal and vertical shifting to get your picture to look like this one.
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.6\linewidth]{fig_7_20}
                  \caption{ CAT with eyes mosaic for Exercise 14(b). The original cat (center) has been
                      repeatedly shifted to the left and right, and up, as well as scaled by a factor of 79\% each time we
                      go up.}
                  \label{fig:fig_7_20}
              \end{figure}
        \item \textit{(Movie: "Sudden Impact")}
              \begin{enumerate}
                  \item Create a movie that stars the CAT and proceeds as follows:
                        The cat starts off at the left end of the screen. It then "runs" horizontally towards the right end
                        of the screen. Just as its right side reaches the right side of the screen, it begins to shrink
                        horizontally (but not vertically) until it degenerates into a vertical line segment on the right side
                        of the screen.
                  \item Make a movie similar to the one in part (a) except that this one stars the "CAT with eyes"
                        and before it begins to run to the right, its pupils move to the right of the eyes and stay there.
                  \item Make a film similar to the one in part (b) except that this one should star the "full CAT"
                        (Figure \ref{fig:fig_7_11}) and upon impact with the right wall, the cat's smile changes to a frown.
              \end{enumerate}
        \item \textit{(Movie: "The Chase")}
              \begin{enumerate}
                  \item Create a movie that stars the "CAT with eyes" and co-stars another
                        smaller version of the same cat (scaled by factors of 0.5 in both the x- and y-directions). The
                        movie starts off with the big cat in the upper left of the screen and the small cat to its right side
                        (very close). Their pupils move directly toward one another to the end of the eyes, and at this
                        point both cats begin moving at constant speed toward the right. When the smaller cat reaches
                        the right side of the screen, it starts moving down while the big cat. also starts moving down.
                        Finally, cats stay put in the lower-right corner as their pupils move back to center.
                  \item Make the same movie except starring the "full CAT" and costarring a smaller counterpart.
              \end{enumerate}
        \item \textit{(Movie: "Close Encounter")}
              \begin{enumerate}
                  \item Create a movie that stars the "full CAT" (Figure \ref{fig:fig_7_11}) and
                        with the following plot: The cat starts off smiling and then its eyes begin to shift all the way to
                        the lower left. It spots a solid black rock moving horizontally directly toward its mouth level, at
                        constant speed. As the cat spots this rock, its smile changes to a frown. It jumps upward as its
                        pupils move back to center and just misses the rock as it brushes just past the cat's chin. The
                        cat then begins to smile and falls back down to its original position.
                  \item Make a film similar to the one in part (a) except that it has the additional feature that the
                        rock is rotating clockwise as it is moving horizontally.
                  \item Make a film similar to the one in part (b) except that it has the additional feature that the
                        cat's pupils, after having spotted the rock on the left, slowly roll (along the bottom of the eyes)
                        to the lower-right postion, exactly following the rock. Then, after the rock leaves the viewing
                        window, have the cat's pupils move back to center postion.
              \end{enumerate}
        \item \textit{(Fractal Geometry: The Cantor Square)}
              The Cantor square is a fractal that starts with the unit
              square in the plane: $C_0 = {(x,y) : 0 \leq x \leq 1 \text{ and } 0 \leq y \leq 1}$ (generation zero). To move to the
              next generation, we delete from this square all points such that at least one of the coordinates is
              inside the middle 1/3 of the original spread. Thus, to get $C$, from $C_0$, we delete all the points
              $(x,y)$ having either $1/3 < x < 2/3 \text{ or } 1/3<y<2/3$. So $C$, will consist of four smaller
              squares each having sidelength equal to $1/3$ (that of $C_0$ ) and sharing one corner vertex with
              $C_0$. Future generations are obtained in the same way. For example, to get from $C_1$, (first
              generation) to $C_2$ (second generation) we delete, from each of the four squares of $C_1$, all points
              $(x,y)$ that have one of the coordinates lying in the middle $1/3$ of the original range (for a
              certain square of $C_1$). What will be left is four squares for each of the squares of $C_1$, leaving a
              total of 16 squares each having sidelength equal to $1/3$ that of the squares of $C_1$, and thus equal
              to $1/9$. In general, letting this process continue forever, it can be shown by induction that the
              $n$th-generation Cantor square consists of $4^n$ squares each having sidelength $1/3^n$. The Cantor
              square is the set of points that remains after this process has been continued indefinitely.
              \begin{enumerate}
                  \item Identify the four similitudes $S_1, S_2, S_3, S_4$ associated with the Cantor square (an illustration
                        as in Figure \ref{fig:fig_7_16} would be fine) and then, working in homogeneous coordinates, find the
                        matrices of each. Next, following the approach of Method 1 of the solution of Example \ref{example:7_7},
                        write a function M-file \mycode{cantorsql(VI,V2,V3,V4,ngen)}, that takes as input the
                        vertices \mycode{V1 = [0 0], V2 = [1 0], V3 = [1 1]} , and \mycode{V4 = [0 1]} of the unit
                        square and a nonnegative integer ngen and will produce a graphic of the generation \mycode{ngen}
                        Cantor square.
                  \item Write a function M-file \mycode{cantorsq2(V1,V2,V3,V4,niter)} that takes as input the
                        vertices \mycode{V1,V2,V3,V4} of any square and a positive integer \mycode{niter} and will produce a Monte
                        Carlo generated graphic for the Cantor square as in Method 2 of the solution of Example \ref{example:7_7}.
                        Run your program for the square having sidelength 1 and lower-left vertex (-1,2) using \mycode{niter}
                        = 2000 and \mycode{niter} = 12,000.
                  \item Write a function M-file \mycode{cantorsq3(V1,V2,V3,V4,ngen)} that takes as input the
                        vertices \mycode{V1,V2,V3,V4} of any square and a positive integer ngen and will produce a graphic
                        for the ngen generation Cantor square as did \mycode{cantorsql} (but now the square can be any
                        square). Run your program for the square mentioned in part (b) first with \mycode{ngen} = 1 then with
                        \mycode{ngen} = 3. Can this program be written so that it produces a reasonable generalization of the
                        Cantor square when the vertices are those of any rectangle?
              \end{enumerate}
        \item \textit{(Fractal Geometry: The Sierpinski Carpet)}
              The Sierpinski carpet is the fractal that starts with
              the unit square ${(x,y) : 0 \leq x \leq 1 \text{ and } 0 \leq y \leq 1}$ with the central square of $1/3$ the sidelength
              removed (generation zero). To get to the next generation, we punch eight smaller squares out of
              each of the remaining eight squares of sidelength 1/3 (generation one), as shown in Figure 7.21.
              Write a function M-file, \mycode{scarpet2(niter)}, based on the Monte Carlo method that will take
              only a single input variable \mycode{niter} and will produce a Monte Carlo approximation of the
              Sierpinski carpet. You will, of course, need to find the eight similitudes associated with this
              fractal and get their matrices in homogeneous coordinates. Run your program with inputs
              \mycode{niter} = 1000, 2000, 5000, and 10,000.
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.6\linewidth]{fig_7_21}
                  \caption{Illustration of generations zero (left), one (middle), and two (right) of the
                      Sierpinski gasket fractal of Exercises 19, 20, and 21. The fractal consists of the points that
                      remain (shaded) after this process has continued on indefinitely.}
                  \label{fig:fig_7_21}
              \end{figure}
        \item \textit{(Fractal Geometry: The Sierpinski Carpet)}
              Read first Exercise 19 (and see Figure \ref{fig:fig_7_21}), and if
              you have not done so yet, identify the eight similitudes $S_1,S_2,\dots,S_8$ associated with the
              Sierpinski carpet along with the homogeneous coordinate matrices of each. Next, following the
              approach of Method 1 of the solution of Example \ref{example:7_7}, write a function M-file
              \mycode{scarpetl(VI,V2,V3,V4,ngen)} that takes as input the vertices \mycode{VI = [0 0], V2 =
                      [1 0], V3 = [1 1], and V4 = [0 1]} of the unit square and a nonnegative integer
              ngen and will produce a graphic of the generation ngen Cantor square.
              \textbf{Suggestions:} Fill in each outer square in gray, then to get the white central square "punched
              out," use the h o l d on and then \mycode{fill} in the smaller square in the color white (rgb vector [1 1
                      1]). When MATLAB fills a polygon, by default it draws the edges in black. To suppress the
              edges from being drawn, use the following extra option in the fill commands: \mycode{fill(xvec, yvec, rgbvec, 'EdgeColor', 'none')}.
              Of course, another nice way to edit a graphic plot from MATLAB is to import the file into a drawing software (such as Adobe
              Illustrator or Corel Draw) and modify the graphic using the software.
        \item \textit{(Fractal Geometry: The Sierpinski Carpet)}
              \begin{enumerate}
                  \item Write a function M-file called
                        \mycode{scarpet3(VI, V2, V3, V4, ngen)} that works just like the program \mycode{scarpet1} of the
                        previous exercise, except that the vertices can be those of any square. Also, base the code not
                        on similitudes, but rather on mathematical formulas for next-generation parameters in terms of
                        present-generation parameters. The approach should be somewhat analogous to that of Method
                        3 of the solution to Example \ref{example:7_7}.
                  \item  Is it possible to modify the \mycode{scarpet1} program so that it is able to take as input the vertices
                        of any equilateral triangle? If yes, indicate how. If no, explain why not.
              \end{enumerate}
        \item \textit{(Fractal Geometry: The Fern Leaf)}
              There are more general ways
              to construct fractals than those that came up in the text. One
              generalization of the self similarity approach given in the text
              allows for transformations that are not invertible (similitudes
              always are). In this exercise you are to create a function M-file,
              called \mycode{fracfern(n)}, which will input a positive integer n and
              will produce a graphic for the fern fractal pictured in Figure \ref{fig:fig_7_22},
              using the Monte Carlo method. For this fractal the four
              transformations to use are (given by their homogeneous
              coordinate matrices)
              $$
                  S_1 = \begin{bmatrix}
                      0 & 0   & 0 \\
                      0 & .16 & 0 \\
                      0 & 0   & 1 \\
                  \end{bmatrix},
                  S_2 = \begin{bmatrix}
                      .85  & .04 & 0   \\
                      -.04 & .85 & 1.6 \\
                      0    & 0   & 0   \\
                  \end{bmatrix},
                  S_3 = \begin{bmatrix}
                      .2  & -2.6 & 0   \\
                      .23 & 2.2  & 1.6 \\
                      0   & 0    & 1   \\
                  \end{bmatrix},
                  S_4 = \begin{bmatrix}
                      -.15 & .28 & 0   \\
                      .26  & .24 & .44 \\
                      0    & 0   & 1   \\
                  \end{bmatrix}.
              $$
              and the associated probability vector is [.01 .86 .93] (i.e., in
              the Monte Carlo process, 1\% of the time we choose 51, 85\% of the time we choose 52, 7\% of
              the time we choose 53, and the remaining 7\% of the time we choose 54).
              Suggestion: Simply modify the program \mycode{scarpet2} accordingly.
              \begin{figure}[h!]
                  \centering
                  \includegraphics[width=0.1\linewidth]{fig_7_22}
                  \caption{The fern leaf fractal.}
                  \label{fig:fig_7_22}
              \end{figure}
        \item \textit{(Fractal Geometry: The Gosper Island)}
              \begin{enumerate}
                  \item Write a function M-file \mycode{gosper(n)} that will input
                        a positive integer \mycode{n} and will produce a graphic of the nth generation of the Gosper island
                        fractal, which is defined as follows: Generation zero is a regular hexagon (with, say, unit side
                        lengths). To get from this to generation one, we replace each of the six sides on the boundary
                        of generation zero with three new segments as shown in Figure \ref{fig:fig_7_23}. The first few generations
                        of the Gosper island are shown in Figure \ref{fig:fig_7_24}.
                        \begin{figure}[h]
                            \centering
                            \includegraphics[width=0.6\linewidth]{fig_7_23}
                            \caption{Iteration scheme for the definition of the Gosper island fractal of Exercise 23.
                                The dotted segment represents a segment of a certain generation of the Gosper island, and the
                                three solid segments represent the correspondingpart of the next generation.}
                            \label{fig:fig_7_23}
                        \end{figure}
                        \begin{figure}[h]
                            \centering
                            \includegraphics[width=0.8\linewidth]{fig_7_24a}
                            \includegraphics[width=0.3\linewidth]{fig_7_24b}
                            \caption{Four different generations of
                                the Gosper island fractal of Exercise 23. In
                                order of appearance, they are (a) the zeroth
                                generation (regular hexagon), (b) the first, (c)
                                the second, and (d) the fifth generation.}
                            \label{fig:fig_7_24}
                        \end{figure}
                  \item \textit{(Tessellations of the Plane)} It is well known that the only regular polygons that can
                        tessellate (or tile) the plane are the equilateral triangle, the square, and the regular hexagon
                        (honeybees have figured this out). It is an interesting fact that any generation of the Gosper
                        island can also be used to tessellate the plane, as shown in Figure \ref{fig:fig_7_25}. Get MATLAB to
                        reproduce each of tessellations that are shown in Figure \ref{fig:fig_7_25}.
                        \begin{figure}[h]
                            \centering
                            \includegraphics[width=0.5\linewidth]{fig_7_25}
                            \caption{Tessellations with generations of Gosper islands. The top one (with regular
                                hexagons) is the familiar honeycomb structure.}
                            \label{fig:fig_7_25}
                        \end{figure}
              \end{enumerate}
    \end{enumerate}
\end{Exercises}

\section{NOTATIONS AND CONCEPTS OF LINEAR SYSTEMS} \label{sec:7_3}
\noindent The general linear system in n variables $x_1,x_2,\dots,x_n$ and $n$ equations can be written as
\begin{equation} \label{equation:7_17}
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n = b_1 \\
        a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n = b_2 \\
        \dots                                           \\
        a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nn}x_n = b_n
    \end{cases}
\end{equation}
Here, the $a_{ij}$, and $b_j$. represent given data, and the variables $x_1, x_2, \dots, x_n$
unknowns whose solution is sought. In light of how matrix multiplication is
defined, these $n$ equations can be expressed as a single matrix equation:
\begin{equation}\label{equation:7_18}
    Ax=b,
\end{equation}
where
\begin{equation}\label{equation:7_19}
    A=
    \begin{bmatrix}
        a_{11} & a_{13} & a_{13} & \dots  & a_{1n} \\
        a_{21} & a_{23} & a_{23} & \dots  & a_{2n} \\
        \vdots &        &        & \ddots & \vdots \\
        a_{n1} & a_{n3} & a_{n3} & \dots  & a_{nn}
    \end{bmatrix},
    x=
    \begin{bmatrix}
        x_1    \\
        x_2    \\
        \vdots \\
        x_n
    \end{bmatrix},
    \text{ and }
    b=
    \begin{bmatrix}
        b_1    \\
        b_2    \\
        \vdots \\
        b_n
    \end{bmatrix}.
\end{equation}

It is also possible to consider more general linear systems that can contain more
or fewer variables than equations, but such systems represent ill-posed problems in
the sense that they typically do not have unique solutions. Most linear systems
that come up in applications will be well-posed, meaning that there will exist a
unique solution, and thus we will be focusing most of our attention on solving
well-posed linear systems. The above linear system is well posed if and only if the
coefficient matrix A is invertible, in which case the solution is easily obtained, by
left multiplying the matrix equation by $A^{-1}: \; Ax=b \; \Rightarrow \; A^{-1}Ax = A^{-1}b \; \Rightarrow$
\begin{equation} \label{equation:7_20}
    x = A^{-1}b.
\end{equation}
Despite its algebraic simplicity, however, this method of solution, namely
computing and then left multiplying by $A^{-1}$, is, in general, an inefficient way of
solving the system. The best general methods for solving linear systems are based
on the Gaussian elimination algorithm. Such an algorithm is actually tacitly used
by the MATLAB command for matrix division (left divide):
\begin{center}
    \begin{tabularx}{\linewidth}{ |X|X| }
        \hline
        \mycode{x=A\textbackslash b} $\rightarrow$ &
        Solves the matrix equation Ax = b by an elaborate Gaussian elimination procedure. \\
        \hline
    \end{tabularx}
\end{center}

If the coefficient matrix A is invertible but "close" to being singular and/or if the
size of A is large, the system (\ref{equation:7_17}) can be difficult to solve numerically. We will
make this notion more precise later in this chapter. In case where there are two or
three variables, the concepts can be illustrated effectively using geometry.

\noindent \textit{CASE:} $n=2$ For convenience of notation,
we drop the subscripts and rewrite the system (\ref{equation:7_17}) as:
\begin{equation} \label{equation:7_21}
    \begin{cases}
        ax+by=e \\
        cx+dy=f
    \end{cases}.
\end{equation}
The system (\ref{equation:7_21}) represents a pair of lines in the plane and the solutions, if any, are
the points of intersection. Three possible situations are illustrated in Figure \ref{fig:fig_7_26}.
We recall (Theorem \ref{theorem:7_1}) that the coefficient matrix A is nonsingular exactly
when det(A) is nonzero. The singular case thus has the lines being parallel
(Figure \ref{fig:fig_7_26}(c)). Nearly parallel lines (Figure \ref{fig:fig_7_26}(b)) are problematic since they
are difficult to distinguish numerically from parallel lines. The determinant alone
is not a reliable indicator of near singularity of a system (see Exercise for the
Reader \ref{exreader:7_14}), but the condition number introduced later in this chapter will be.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{fig_7_26}
    \caption{Three possibilities for the system (21)
        $\begin{cases}
                ax+by=e \\
                cx+dy=f
            \end{cases}$
        (a) well-conditioned, (b) ill-conditioned (nearly parallel lines), and (c) singular (parallel lines).}
    \label{fig:fig_7_26}
\end{figure}

\begin{ExerciseForTheReader}
    Show that for any pair of nearly parallel
    lines in the plane, it is possible to represent this system by a matrix equation
    $Ax = b$, which uses a coefficient matrix $A$ with $det(A) = 1$.
\end{ExerciseForTheReader}

\noindent \textit{CASE:} $n=3$ A linear equation $ax + by + cz = d$ represents a plane in three-
dimensional space \textbf{$R^3$}. Typically, two planes in \textbf{$R^3$} will intersect in a line and a
typical intersection of a line with a third plane (and hence the typical intersection
of three planes) will be a point. This is the case if the system is nonsingular.
There are several ways for such a three-dimensional system to be singular or
nearly so. Apart from two of the planes being parallel (making a solution
impossible), another way to have a singular system is for one of the planes to be
parallel to the line of intersection of the other two. Some of these possibilities are
illustrated in Figure \ref{fig:fig_7_27}.

For higher-order systems, the geometry is similar although not so easy to
visualize since the world we live in has only three (visual) dimensions. For
example, in four dimensions, a linear equation $ax + by + cz + dw = e$ will, in
general, be a three-dimensional hyperplane in the four-dimensional space \textbf{$R^4$}.
The intersection of two such hyperplanes will typically be a two-dimensional
plane in \textbf{$R^4$}. If we intersect with one more such hyperplane we will (in
nonsingular cases) be left with a line in \textbf{$R^3$}, and finally if we intersect this line
with a fourth such hyperplane we will be left with a point, the unique solution, as
long as the system is nonsingular.

The variety of singular systems gets extremely complicated for large values of n,
as is partially previewed in Figure \ref{fig:fig_7_27} for the case $n = 3$. This makes the
determination of near singularity of a linear system a complicated issue, which is
why we will need analytical (rather than geometric) wavs to detect this.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{fig_7_27}
    \caption{Four geometric possibilities for a three-dimensional system (18) $Ax=b$.
        (a) (upper left) Represents a typical nonsingular system, three planes intersect at one
        common point; (b) (upper right) parallel planes, no solution, a singular system; (c) (lower
        left) three planes sharing a line, infinitely many solutions, a singular system; (d) (lower
        right) three different parallel lines arise from intersections of pairs of the three planes, no
        solution, singular system.\protect\footnotemark}
    \label{fig:fig_7_27}
\end{figure}
\footnotetext{Note: These graphics were created on MATLAB.}

\section{SOLVING GENERAL LINEAR SYSTEMS WITH MATLAB} \label{sec:7_4}
\noindent The best all-around algorithms for solving linear systems (\ref{equation:7_17}) and (\ref{equation:7_18}) are based
on Gaussian elimination with partial pivoting. MATLAB's default linear system
solver is based on this algorithm. In the next section we describe this algorithm;
here we will show how to use MATLAB to solve such systems. In the following
example we demonstrate three different ways to solve a (nonsingular) linear
system in MATLAB by solving the following interpolation problem, and compare flop counts.

\begin{Example}
    \textit{(Polynomial Interpolation)} Find the equation of the polynomial
    $p(x) = ax^3 + bx^3 +cx +d$ of of degree at most 3 that passes through the data points
    (-5, 4), (-3, 34), (-1, 16), and (1, 2).
    \begin{Solution}
        In general, a set of n data points (with different x-coordinates) can
        always be interpolated with a polynomial of degree at most $n - 1$. Writing out the
        interpolation equations produces the following four-dimensional linear system:
        \begin{align*}
            p(-5)=4 \; \Rightarrow \; a(-5)^3 + b(-5)^2 + c(-5) + d = 4   \\
            p(-3)=34 \; \Rightarrow \; a(-3)^3 + b(-3)^2 + c(-3) + d = 34 \\
            p(-1)=34 \; \Rightarrow \; a(-1)^3 + b(-1)^2 + c(-1) + d = 16 \\
            p(1)=2 \; \Rightarrow \; a \cdot 1^3 + b \cdot 1^2 + c \cdot 1 + d = -2
        \end{align*}
        In matrix (\ref{equation:7_18}) form this system becomes:
        $$
            \begin{bmatrix}
                -125 & 25 & -5 & 1 \\
                -27  & 9  & -3 & 1 \\
                -1   & 1  & -1 & 1 \\
                1    & 1  & 1  & 1
            \end{bmatrix}
            \begin{bmatrix}
                a \\
                b \\
                c \\
                d
            \end{bmatrix} =
            \begin{bmatrix}
                4  \\
                34 \\
                16 \\
                -2
            \end{bmatrix}.
        $$
        We now solve this matrix equation $Ax - b$ in three different ways with MATLAB,
        and do a flop count\footnote{
            Flop counts will help us to compare efficiencies of algorithms. Later versions of MATLAB (Version
            6 and later) no longer support the flop count feature. We will occasionally tap into the older Version 5
            of MATLAB (using \mycode{flops}) to compare flop counts, for purely illustrative purposes.
        } for each.

        \begin{enumerate}[label=\textit{Method \arabic*}]
            \item \label{method:7_4_1}Left divide by $A$. This is the Gaussian elimination method
                  mentioned in the previous section. It is the recommended method.\\
                  \mycode{
                  >> fomat long, A=[-125 25 -5 1;-27 9 -3 1;-1 1 -1 1; 1 1 1 1];\\
                  b=[4 34 16 -2]';\\
                  >> flops(0);\\
                  >> x=A$\backslash$ b\\
                  $\rightarrow$ 1.00000000000000\\
                  3.00000000000000\\
                  -10.00000000000000\\
                  4.00000000000000\\
                  >> flops $\rightarrow$ans=180\\
                  }
                  Thus we have found our interpolating polynomial to be
                  $p(x) = x^3 +3x^2 -10x + 4$. An easy check verifies that the function
                  indeed interpolates the given data. Its graph along with the data points
                  are shown in Figure \ref{fig:fig_7_28}.
                  \begin{figure}[h]
                      \centering
                      \includegraphics[width=0.5\linewidth]{fig_7_28}
                      \caption{Graph of the interpolating
                          cubic polynomial $p(x) = x^3 +3x^2 -10x + 4$
                          for the four data points that were given in
                          Example \ref{example:7_8}.\protect\footnotemark}
                      \label{fig:fig_7_28}
                  \end{figure}
                  \footnotetext{The MATLAB plot in the figure was created by first plotting the function, as usual, applying \mycode{hold}
                      on, and then plotting each of the points as red dots using the following syntax (e.g., for the data point
                      (-1, 16)), \mycode{plot(-1,16,'ro')}. The "EDIT' menu on the graphics window can then be used to
                      enlarge the size of the red dots after they are selected.}
            \item \label{method:7_4_2}We compute the inverse of A and multiply this inverse on the left of b.\\
                  \mycode{
                      >> flops(0); x=inv(A)*b\\
                      $\rightarrow$x = 1.00000000000000\\
                      3.00000000000000\\
                      -10.00000000000000\\
                      4.00000000000000\\
                      >> flops $\rightarrow$ans=262\\
                  }
                  We arrived at the same (correct) answer, but with more work. The amount of
                  extra work needed to compute the inverse rather than just solve the system gets
                  worse with larger-sized matrices; moreover, this method is also more prone to
                  errors. We will give more evidence and will substantiate these claims later in this section.
            \item \label{method:7_4_3}This method is more general than the first two since it will work also to
                  solve singular systems that need not have square coefficient matrices. MATLAB
                  has the following useful command:
                  \begin{center}
                      \begin{tabularx}{\linewidth}{ |X|X| }
                          \hline
                          \mycode{rref(Ab)} $\rightarrow$
                           &
                          Puts an augmented $n\times(m + l)$ matrix $[A \text{ \textbrokenbar}\text{ } b]$ for the system $Ax = b$ into reduced row echelon form. \\
                          \hline
                      \end{tabularx}
                  \end{center}
                  The reduced row echelon form of the augmented matrix is a form from which the
                  general solution of the linear system can be easily obtained. In general, a linear
                  system can have (i) no solution, (ii) exactly one solution (nonsingular case), or (iii)
                  infinitely many solutions. We will say more about how to interpret the reduced
                  row echelon form in singular cases later in this section, but in case of a
                  nonsingular (square) coefficient matrix A , the reduced row echelon form of the
                  augmented matrix $[A \text{ \textbrokenbar}\text{ } b]$ will be $[I \text{ \textbrokenbar}\text{ } x]$, where x is the solution vector.

                  Assuming A and b are still in the workspace, we construct from them the
                  augmented matrix and then use \mycode{rref}:\\
                  \mycode{
                      >> Ab=A; Ab(:,5)=b;\\
                      >> flops(0); ref(Ab)\\
                      $\rightarrow$ ans =
                      $\begin{matrix}
                              1 & 0 & 0 & 0 & 1   \\
                              0 & 1 & 0 & 0 & 3   \\
                              0 & 0 & 1 & 0 & -10 \\
                              0 & 0 & 0 & 1 & 4
                          \end{matrix}$\\
                      >> flops $\rightarrow$ ans = 634\\
                  }
                  Again we obtained the same answer, but with a lot more work (more than triple
                  the flops that were needed in \ref{method:7_4_1}). Although \mycode{rref} is also based in
                  Gaussian elimination, putting things into reduced row echelon form (as is usually
                  taught in linear algebra courses) is a computational overkill of what is needed to
                  solve a nonsingular system. This will be made apparent in the next section.
        \end{enumerate}
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    \begin{enumerate}
        \item Find the coefficients a9b,c, and*/of
              the polynomial $p(x) = ax^3 +bx^2 +cx + d$ of degree at most 3 that passes through
              these data points: (-2, 4), (1, 3), (2, 5), and (5, -22).
        \item Find the equation of the polynomia $p(x) = ax^8 + bx^7 +cx^6 + dx^5 + ex^4 + fx^3 + gx^2 + hx + k$
              of degree at most 8 that passes through the following data
              points: (-3,-14.5), (-2,-12), (-1,15.5), (0,2), (1,-22.5), (2,-112),
              (3,-224.5), (4,318), (5,3729.5). Solve the system using each of the three
              methods shown in Example 7.8 and compare the solutions, computation times
              (using \mycode{tic/toe}), and flop counts (if available).
    \end{enumerate}
\end{ExerciseForTheReader}

In the preceding example, things worked great and the answer MATLAB gave us
was the exact one. But the matrix was small and not badly conditioned. In
general, solving a large linear system involves a large sequence of arithmetic
operations and the floating point errors can add up, sometimes (for poorly
conditioned matrices) intolerably fast. The next example will demonstrate such a
pathology, and should convince the reader of our recommendation to opt for
\ref{method:7_4_1} (left divide).

\begin{Example}
    \textit{The Hilbert Matrix}
    $$
        H_n=
        \begin{bmatrix}
            1      & 1/2     & 1/3     & \dots  & 1/n      \\
            1/2    & 1/3     & 1/4     & \dots  & 1/(n+1)  \\
            \vdots &         &         & \ddots & \vdots   \\
            1/n    & 1/(n+1) & 1/(n+2) & \dots  & 1/(2n-1)
        \end{bmatrix},
    $$
    A classical example of a matrix that comes up in applications and is very poorly
    conditioned is the \textbf{Hilbert matrix $H_n$ of order $n$}, which is defined above. This matrix can
    easily be entered into a MATLAB session using a for loop, but MATLAB even
    has a separate command \mycode{hilb(n)} to create it. In this example we will solve,
    using each of the three methods from the last example, the equation
    $Hx=b$, where b is the vector $Hx$, and where $x = (1 1 1 \dots 1)'$ with $n = 12$.
\end{Example}

\noindent We now proceed with each of the three methods of the solution of the last example
to produce three corresponding "solutions,"
\mycode{x\_methl}, \mycode{x\_meth2}, and \mycode{x\_meth3}, to the linear system $Hx = b$.
Since we know the exact solution to be
$x = (1 1 1 \dots 1)'$, we will be able to compare both accuracy and speeds of the three methods.\\
\mycode{
    >> x=ones(12,1); H=hilb(12); b=H*x;\\
    >> flops(0); x\_meth1=H$\backslash$b10; max(abs(x-x\_meth1)) $\rightarrow$ 0.2385\\
    >> \%because each component of the exact solution is 1, this can be\\
    >> \%thought of as the maximum relative error in any component.\\
    >> flops $\rightarrow$ 995\\
    >> flops(0); \\
    >> x\_meth2=inv(H)*b; max(abs(x-x\_meth2)) $\rightarrow$ 0.6976\\
    >> flops $\rightarrow$ 1997\\
    >> flops(0);, R=rref([H b]); flops $\rightarrow$ans = 6484\\
}
If we view row reduced matrix R produced in \ref{method:7_4_3}
(or just the tenth row by entering R(12,:)), we
see that the last row is entirely made up of zeros.
Thus, \ref{method:7_4_3} leads us to the false conclusion that
the linear system is singular.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\linewidth]{fig_7_29}
    \caption{David Hilbert\protect\footnotemark (0862-1943), German mathematician.}
    \label{fig:fig_7_29}
\end{figure}
\footnotetext{
    The Hubert matrix is but a small morsel of the vast set of mathematical contributions produced in the
    illustrious career of David Hubert (Figure \ref{fig:fig_7_29}). Hubert is among the very top echelon of the greatest
    German mathematicians, and this group contains a lot of competition. Hubert's ability to transcend the
    various subfields of mathematics, to delve deeply into difficult problems, and to discover fascinating
    interrelations was unparalleled. Three years after earning his Ph.D., he submitted a monumental paper
    containing a whole new and elegant treatment of the so-called Basis Theorem, which had been proved
    by Paul Albert Gordan (1837-1912) in a much more specialized setting using inelegant computational
    methods. Hubert submitted his manuscript for publication to the premier German mathematical journal
    Mathematische Annalen, and the paper was sent to Gordan to (anonymously) referee by the editor Felix
    Christian Klein (1849-1925), also a famous German mathematician and personal friend of Gordan's.
    It seemed that Gordan, the world-renowned expert in the field of Hubert's paper, was unable to follow
    Hubert's reasoning and rejected the paper for publication on the basis of its incoherence. In response,
    Hubert wrote to Klein, \textit{"... I am not prepared to alter or delete anything, and regarding this paper, I
        say with all modesty, that this is my last word so long as no definitive and irrefutable objection against
        my reasoning is raised."} Hubert's paper finally appeared in this journal in its original form, and he
    continued to produce groundbreaking papers and influential books in an assortment of fields spanning
    all areas of mathematics. He even has a very important branch of analysis named in his honor (Hubert
    space theory). In the 1900 International Conference of Mathematics in Paris, Hubert posed 23
    unsolved problems to the mathematical world. These "Hubert problems" have influenced much
    mathematical research activity throughout the twentieth century. Several of these problems have been
    solved, and each such solution marked a major mathematical event. The remaining open problems
    should continue to influence mathematical thoughts well into the present millennium. In 1895, Hubert
    was appointed to a "chair" at the University of Gottingen and although he was tempted with offers from
    other great universities, he remained at this position for his entire career. Hubert retired in his
    birthplace city of Konigsberg, which (since he had left it) had become part of Russia after WWII (with
    the name of the city changed to "Kaliningrad"). Hubert was made an honorary citizen of this city and
    in his acceptance speech he gave this now famous quote: \textit{"Wir mussen wissen, wir werden wissen (We
        must know, we shall know)."}
}

The results of \ref{method:7_4_3} are catastrophic, since the
reduced row echelon form produced would imply that
there are infinitely many solutions! (We will explain
this conclusion shortly.) \ref{method:7_4_1} and \ref{method:7_4_2} both had
(unacceptably) large relative errors of about 24\% and
70\%, respectively. The increasing flop counts in the
three methods also shows us that left divide (\ref{method:7_4_1})
gives us more for less work. Of course, the Hilbert matrix
is quite a pathological one, but such matrices do come up often enough in
applications that we must always remember to consider floating point/roundoff
error when we solve linear systems. We point out that many problems that arise in
applied mathematics involve very well-conditioned linear systems that can be
effectively solved (using MATLAB) for dimensions of order 1000! This is the
case for many linear systems that arise in the numerical solution of partial
differential equations. In Section \ref{sec:7_6}, we will examine more closely the concept
of the condition number of a matrix and its affect on error bounds for numerical
solutions of corresponding linear systems. Just because a matrix is poorly
conditioned does not mean that all linear systems involving it will be difficult to
solve numerically. The next exercise for the reader gives such an example with
the Hubert matrices.

\begin{ExerciseForTheReader}
    In this exercise, we will be considering
    larger analogs of the system studied in Example \ref{example:7_9}. For a positive integer $n$, we
    let $c(n)$ be the least common multiple of the integers $1, 2, 3, \dots, n$, and we define
    $b_n = h_n(c(n)e_1')$, where $H_n$ is the $n$th-order Hubert matrix and $e_1$ is the vector
    $(1,0,0, ..., 0)$ (having $n$ components). We chose $c(n)$ to be as small as possible
    so that the vector bn will have all integer components. Note, in Example \ref{example:7_9}, we
    used $c(10) = 2520$.
    \begin{enumerate}
        \item For n = 20, solve this system using \ref{method:7_4_1}, time it with \mycode{tic/toe}, and (if
              available) do a flop count and find the percentage of the largest error in any of the
              20 components of x\_methl to that of the largest component of the exact solution
              $x (= c(20))$; repeat using \ref{method:7_4_2}.
        \item Repeat part (a) for $n = 30$.
        \item In parts (a) and (b), you should have found that x\_methl equaled the exact
              solution (so the corresponding relative error percentages were zero), but the
              relative error percentages for \mycode{x\_meth2} grew from 0.00496\% in case $n = 10$
              (Example \ref{example:7_9}), to about 500\% in part (a) and to about 5000\% in part (b). Thus the
              "noise" from the errors has transcended, by far, the values of the exact solution.
              Continue solving this system using \ref{method:7_4_1}, for $n - 40$, $n = 50$, and so on, until
              you start getting errors from the exact solution or the computations start to take too
              much time, whichever comes first.
    \end{enumerate}
    \textbf{Suggestion:}  MATLAB has a built-in function \mycode{lem(a, b)} that will find the least
    common multiple of two positive integers. You can use this, along with a for loop
    to get MATLAB to easily compute $c(n)$, for any value of $n$. In each part, you
    may wish to use \mycode{max(abs(x-x\_methl))} to detect any errors.
    \textbf{Note:} Exercise 31 of Section \ref{sec:7_6} will analyze why things have gone so well with
    these linear systems.
\end{ExerciseForTheReader}

We close this section by briefly explaining how to interpret the reduced row
echelon form to obtain the general solution of a linear system with a singular
coefficient matrix that need not be square. Suppose we have a linear system with
n equations and m unknowns $x_1, x_2, \dots, x_m$:
\begin{equation} \label{equation:7_22}
    \begin{cases}
        a_{11}x_1 + a_{12}x_2 + \dots + a_{1m}x_m = b_1 \\
        a_{21}x_1 + a_{22}x_2 + \dots + a_{2m}x_m = b_2 \\
        \dots                                           \\
        a_{n1}x_1 + a_{n2}x_2 + \dots + a_{nm}x_m = b_n \\
    \end{cases}
\end{equation}
We can write this equation in matrix form $Ax = b$, as before; but in general the
coefficient matrix $A$ need not be square. We form the augmented matrix of the
system by tacking on the vector $b$ as an extra column on the right of $A$:
$$
    [A \text{ \textbrokenbar}\text{ } B ] =
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \dots  & a_{1m} & \text{\textbrokenbar} & b_1    \\
        a_{21} & a_{22} & a_{23} & \dots  & a_{2m} & \text{\textbrokenbar} & b_2    \\
        \vdots &        &        & \ddots & \vdots & \text{\textbrokenbar} & \vdots \\
        a_{n1} & a_{n2} & a_{n3} & \dots  & a_{nm} & \text{\textbrokenbar} & b_n
    \end{bmatrix}.
$$
The augmented matrix is said to be in reduced row echelon form if the following
four conditions are met. Each condition pertains only to the left of the partition
line (i.e., the $a_{ij}$'s):
\begin{enumerate}
    \item Rows of all zero entries, if any, must be grouped together at the bottom.
    \item If a row is not all zeros, the leftmost nonzero entry must equal 1 (such an
          entry will be called a \textbf{leading one} for the row).
    \item All entries above and below (in the same column as) a leading one must
          be zero.
    \item If there are more than one leading ones, they must move to the right as we
          move down to lower rows.
\end{enumerate}

Given an augmented matrix A, the command \mycode{rref(Ab)} will output an
augmented matrix of the same size (but MATLAB will not show the partition line)
that is in reduced row echelon form and that represents an equivalent linear
system to Ab, meaning that both systems will have the same solution. It is easy
to get the solution of any linear system, singular or not, if it is in reduced row
echelon form. Since most of our work will be with nonsingular systems (for
which \mycode{rref} should not be used), we will not say more about how to construct the
reduced row echelon form. The algorithm is based on Gaussian elimination,
which will be explained in the next section. For more details on the reduced row
echelon form, we refer to any textbook on basic linear algebra; see, e.g., [Kol-99],
or [Ant-00]. We will only show, in the next example, how to obtain the general
solution from the reduced row echelon form.

\begin{Example}
    \begin{enumerate}[label=(\alph*)]
        \item Which of the following augmented matrices are in reduced
              row echelon form?
              $$
                  M_1 =
                  \begin{bmatrix}
                      1 & 2 & 0 & \text{\textbrokenbar} & -2 \\
                      0 & 0 & 1 & \text{\textbrokenbar} & 3  \\
                      0 & 0 & 0 & \text{\textbrokenbar} & 0
                  \end{bmatrix},
                  M_2 =
                  \begin{bmatrix}
                      1 & 2 & 0 & 1 & \text{\textbrokenbar} & 1  \\
                      0 & 1 & 0 & 2 & \text{\textbrokenbar} & -8 \\
                      0 & 0 & 1 & 3 & \text{\textbrokenbar} & 4
                  \end{bmatrix},
                  M_3 =
                  \begin{bmatrix}
                      1 & 0 & 0 & 1 & \text{\textbrokenbar} & 1  \\
                      0 & 1 & 0 & 2 & \text{\textbrokenbar} & -8 \\
                      0 & 0 & 0 & 0 & \text{\textbrokenbar} & 4
                  \end{bmatrix}.
              $$
        \item For those that are in reduced row echelon form, find the general solution of the
              corresponding linear system that the matrix represents.
    \end{enumerate}
    \begin{Solution}
        Part (a): $M_1$ and $M_3$ are in reduced row echelon form; $M_2$ is not.
        The reader who is inexperienced in this area of linear algebra should carefully
        verify these claims for each matrix by running through all four of the conditions (i) through (iv).\\
        Part (b): If we put in the variables and equal signs in the three-equation linear
        system represented by $M_1$, and then solve for the variables that have leading ones
        in their places (here $x_1$ and $x_3$ ), we obtain:
        $$
            \begin{cases}
                x_1 + 2x_2 = -2 \Rightarrow x_1 = -2 -2x_2 \\
                x_3 = 3                                    \\
                0 = 0
            \end{cases},
        $$
        Thus there are infinitely many solutions: Letting $x_2 = t$ , where $t$ is any real
        number, the general solution can be expressed as
        $$
            \begin{cases}
                x_1 = -2 - 2t \\
                x_2 = t       \\
                x_3 = 3
            \end{cases}\text{, t = any real number.}
        $$
        If we do the same with the augmented matrix A/3 , we get 0 = 4 for the last
        equation of the system. Since this is impossible, the system has no solution.

        Here is a brief summary of how to solve a linear system with MATLAB. For a
        square matrix $A$, you should always try $x = A\backslash b$ (left divide). For singular
        matrices (in particular, nonsquare matrices) use \mycode{rref} on the augmented matrix.
        There will be either no solution or infinitely many solutions. No solution will
        always be seen in the reduced row echelon form matrix by a row of zeros before
        the partition line and a nonzero entry after it (as in the augmented matrix $A/3$ in
        the above example). In all other (singular) cases there will be infinitely many
        solutions; columns without leading ones will correspond to variables that are to be
        assigned arbitrary real numbers (s, t, u, ...); columns with leading ones correspond
        to variables that should be solved for in terms of variables of the first type.
    \end{Solution}
\end{Example}

\begin{ExerciseForTheReader}
    Parts (a) and (b): Repeat the instructions
    of both parts (a) and (b) of the preceding example for the following augmented matrices:
    $$
        M_1 =
        \begin{bmatrix}
            1 & 0 & \text{\textbrokenbar} & 3 \\
            0 & 1 & \text{\textbrokenbar} & 2 \\
            0 & 0 & \text{\textbrokenbar} & 0
        \end{bmatrix},
        M_2 =
        \begin{bmatrix}
            1 & -2 & 0 & 3  & \text{\textbrokenbar} & -2 \\
            0 & 0  & 1 & -5 & \text{\textbrokenbar} & 1
        \end{bmatrix},
        M_3 =
        \begin{bmatrix}
            1 & 0 & 0 & \text{\textbrokenbar} & 1 \\
            0 & 0 & 2 & \text{\textbrokenbar} & 3 \\
            0 & 1 & 0 & \text{\textbrokenbar} & 4
        \end{bmatrix}.
    $$
    Part (c): Using the MATLAB command \mycode{rref}, find the general solutions of the
    following linear systems:
    $$
        (i)
        \begin{cases}
            x_1 + 3x_2 + 2x_3 = 3 \\
            2x_1 + 6x_2 + 2x_3 - 8x_4 = 4
        \end{cases}\\
        (ii)
        \begin{cases}
            x_1 - 2x_2 + x_3 + x_4 + 2x_5 = 2     \\
            -2x_1 + 4x_2 + 2x_3 + 2x_4 - 2x_5 = 0 \\
            3x_1 - 6x_2 + x_3 + x_4 = x_5 = 4     \\
            -x_1 + 2x_2 + 3x_3 +x_4 +x_5 = 3
        \end{cases}
    $$
\end{ExerciseForTheReader}

\begin{Exercises}
    \begin{enumerate}
        \item Use MATLAB to solve the linear system $Ax = b$, with the following choices for $A$ and $b$.
              Afterward, check to see that your solution x satisfies $Ax = b$.
              \begin{enumerate}
                  \item $
                            A=\begin{bmatrix}
                                9  & -5 & 2 \\
                                0  & 7  & 5 \\
                                -1 & -9 & 6 \\
                            \end{bmatrix},
                            b=\begin{bmatrix}
                                67 \\ 13 \\ 27
                            \end{bmatrix}
                        $
                  \item $
                            A=\begin{bmatrix}
                                -1 & 1  & 3  & 5  \\
                                3  & -4 & -1 & 5  \\
                                5  & -1 & 4  & -5 \\
                                -2 & 3  & -5 & -4 \\
                            \end{bmatrix},
                            b=\begin{bmatrix}
                                9 \\ 29 \\ -22 \\ -5
                            \end{bmatrix}
                        $
                  \item $
                            A=\begin{bmatrix}
                                -3 & -3 \\
                                1  & 3
                            \end{bmatrix},
                            b=\begin{bmatrix}
                                2.928 \\ 3.944
                            \end{bmatrix}
                        $
                  \item $
                            A=\begin{bmatrix}
                                -12 & -20 & 10 \\
                                -2  & 18  & -1 \\
                                -3  & 14  & 1
                            \end{bmatrix},
                            b=\begin{bmatrix}
                                -112.9 \\ 71.21 \\ 45.83
                            \end{bmatrix}
                        $
              \end{enumerate}
        \item \textit{(Polynomial Interpolation)}
              \begin{enumerate}
                  \item Find the equation of the polynomial of degree at most 2
                        (parabola) that passes through the data points
                        $(-1,21),(1,-3),(5,69)$; then plot the function
                        along with the data points.
                  \item Find the equation of the polynomial of degree at most 3 that passes through the data points:\\
                        $(-4,-58.8),(2,9.6),(8,596.4),(1,4.2)$; then plot the function along with the data points.
                  \item Find the equation of the polynomial of degree at most 6 that passes through the data points:\\
                        $(-2,42),(-1,-29),(-0.5,-16.875),(0,-6),(1,3),(1.5,18.375),(2,-110)$; \\then plot the
                        function along with the data points.
                  \item Find the equation of the polynomial of degree at most 5 that passes through the data points:\\
                        $(1,2),(2,4),(3,8),(4,16),(5,32),(6,64)$; then plot the function along with the data points.
              \end{enumerate}
        \item Find the general solution of each of the following linear systems:
              \begin{enumerate}
                  \item $
                            \begin{cases}
                                3x_1 + 3x_2 + 2x_3 + 5x_4 = 12 \\
                                2x_1 + 5x_2 + 2x_3 - 8x_4 = 4
                            \end{cases}
                        $
                  \item $
                            \begin{cases}
                                x_1 + x_2 + 3x_3 - x_4 - 5x_5 = 2   \\
                                x_1 - 3x_2 + 2x_3 - 2x_4 + x_5 = 2  \\
                                2x_1 - 7x_2 + 3x_3 - x_4 - 4x_5 = 4 \\
                                -3x_1 - 2x_2 - 4x_3 + 4x_4 - x_5 = 6
                            \end{cases}
                        $
                  \item $
                            \begin{cases}
                                2x_1 + 2x_2 - 3x_3 + x_4 = 8 \\
                                4x_1 - 3x_3 - 2x_4 = 0       \\
                                -3x_1 + 4x_2 + x_3 + x_4 = -1
                            \end{cases}
                        $
                  \item $
                            \begin{cases}
                                3x_1 + 6x_2 + x_3 - 2x_4 + 12x_5 = 22     \\
                                -8x_1 - 16x_2 + 16x_4 - 32x_5 + x_6 = -60 \\
                                x_1 + 2x_2 - 2x_4 + 4x_5 = 8
                            \end{cases}
                        $
              \end{enumerate}
        \item \textit{(Polynomial Interpolation)}
              In polynomial interpolation problems as in Example 7.8, the
              coefficient matrix that arises is the so-called Vandermonde matrix corresponding to the vector
              $v=[x_0 \; x_1 \; x_2 \; \dots \; x_n]$ that is the $(n+1)\times (n+1)$ matrix defined By
              $$
                  \begin{bmatrix}
                      x_{0}^{n} & x_{0}^{n-1} & \dots & x_0    & 1      \\
                      x_{1}^{n} & x_{1}^{n-1} & \dots & x_1    & 1      \\
                      \vdots    &             &       & \ddots & \vdots \\
                      x_n^n     & x_n^{n-1}   & \dots & x_n    & 1
                  \end{bmatrix}.
              $$
              MATLAB (of course) has a function \mycode{vender(v)}, that will create the Vandermonde matrix
              corresponding to the inputted vector v. Redo parts (a) through (d) of Exercise 2, this time using
              \mycode{vender(v)}.\\
              (e) Write your own version \mycode{myvander(v)}, that does what MATLAB's \mycode{vender(v)} does.
              Check the efficiency of your M-file with that of MATLAB's \mycode{vender(v)} by typing (after you
              have created and debugged your program) \mycode{>> type vander} to display the code of
              MATLAB's \mycode{vender(v)}.
        \item \textit{(Polynomial Interpolation)}
              \begin{enumerate}
                  \item Write a MATLAB function M-file called pv
                        \mycode{polyinterp(x,y)} that will input two vectors $x$ and $y$ of the same length (call this length
                        $"n+1"$ for now) that correspond to the $x$- and $y$-coordinates of $n +1$ data points on which we
                        wish to interpolate with a polynomial of degree at most n. The output will be a vector
                        $pv=[a_n \; a_{n-1} \; a_2 \; a_1 \; a_0]$ that contains the coefficients of the interpolating polynomial
                        $p(x)=a_n x^n + a_{n-1} x^{n-1} + \dots + a_1 x + a_0$.
                  \item Use this program to redo part (b) of Exercise 2.
                  \item Use this program to redo part (c) of Exercise 2.
                  \item Use this program with input vectors
                        $x = [0 \; \pi/2 \; \pi \; 3\pi/2 \; 2\pi \; 5\pi/2 \; 3\pi \; 7\pi/2 \; 4\pi]$,
                        and $y=[1 \; 0 \; -1 \; 0 \; 1 \; 0 \; -1 \; 0 \; 1]$. Plot the resulting polynomial along with the data points.
                        Include also the plot (in a different style/color) of a trig function that interpolates this data.
              \end{enumerate}
        \item \textit{(Polynomial Interpolation: Asking a Bit More)}
              Often in applications, rather than just needing a polynomial (or some other nice interpolating curve)
              that passes through a set of data points, we also need the interpolating curve to
              satisfy some additional smoothness requirements. For example, consider the design of a railroad
              transfer segment shown in Figure \ref{fig:fig_7_30}. The curved portion of "interpolating"
              railroad track needs to do more than just connect the two parallel tracks; it must do so "smoothly"
              lest the trains using it would derail. Thus, if we seek a function $y = p(x)$ that models this interpolating
              track, we see from the figure (and the reference to the xy-coordinate system drawn in) that we would like the center curve of this
              interpolating track to satisfy the following conditions on the interval $0 \leq x \leq 300$ feet:
              $$p(0) = 0, \; p(300)=100\text{ feet, } p'(0)=p'(300)=0$$
              (The last two conditions geometrically will require the graph of $y = p(x)$ to have a horizontal
              tangent line at the endpoints $x = 0$ and $x = 300$, and thus connect smoothly with the existing
              tracks.) If we would like to use a polynomial for this interpolation, since we have four
              requirements, we should be working with a polynomial of degree at most 3 (that has four
              parameters): $p(x) - ax^3 + bx^2 +cx + d$.
              \begin{enumerate}
                  \item Set up a linear system for this interpolating polynomial and get MATLAB to solve it.
                  \item Next, get MATLAB to graph the rail network (just the rails) including the two sets of parallel
                        tracks as well as the interpolating rails. Leave a 6-foot vertical distance between each set of adjacent rails.
              \end{enumerate}
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.5\linewidth]{fig_7_30}
                  \caption{Exercise 6 asks to find a polynomial function modeling the junction between the two parallel sects of rails.}
                  \label{fig:fig_7_30}
              \end{figure}
        \item \textit{(Polynomial interpolation: Asking a Bit More)} Three parallel railroad tracks need to be
              connected by a pair of curved junction segments, as shown in Figure \ref{fig:fig_7_31}.
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.5\linewidth]{fig_7_31}
                  \caption{In Exercise 7, this set of three parallel rails is required to be joined by two sets of smooth junction rails.}
                  \label{fig:fig_7_31}
              \end{figure}
              \begin{enumerate}
                  \item If we wish to use a single polynomial function to model (the center curves of) both pairs of
                        junction rails shown in Figure \ref{fig:fig_7_31}, what degree polynomials should we use in our model? Set
                        up a linear system to determine the coefficients of this polynomial, then get MATLAB to solve it
                        and determine the polynomial.
                  \item Next, get MATLAB to graph the rail network (just the rails) including the three sets of
                        parallel tracks as well as the interpolating rails gotten from the polynomial function you found in
                        part (a). Leave a 6-foot vertical distance between each set of adjacent rails.
                  \item Do a separate polynomial interpolation for each of the two junction rails and thus find two\\
                        different polynomials that model each of the two junctions. Set up the two linear systems, solve
                        them using MATLAB, and then write down the two polynomials.
                  \item Next, get MATLAB to graph the rail network (just the rails) including the three sets of
                        parallel tracks as well as the interpolating rails gotten from the polynomial functions you found
                        in part (c). Leave a 6-foot vertical distance between each set of adjacent rails. How does this
                        picture compare with the one in part (b)?
              \end{enumerate}
              \textbf{Note:} In general it is more efficient to do the piecewise polynomial interpolation that was done
              in part (d) rather than the single polynomial interpolation in part (b). The advantages become
              more apparent when there are a lot of data points. This approach is an example of what is called
              spline interpolation.
        \item \textit{(City Planning: Traffic Logistics)}
              The Honolulu street map of Figure \ref{fig:fig_7_32} shows the rush-hour numbers of vehicles per hour
              that enter or leave the network of four one-way streets. The variables $x_1,x_2,x_3,x_4$
              represent the traffic flows on the segments shown. For smooth traffic flow, we would
              like to have equilibrium at each of the four intersections; i.e., the number of incoming
              cars (per hour) should equal the number of outgoing cars. For example, at the
              intersection of Beretania and Piikoi (lower right), we should have $x_1 + 800 = x_2 + 2000$
              or (after rearranging) $x_1 - x_2 = 1200$.
              \begin{enumerate}
                  \item Obtain a linear system for the smooth traffic flow in the above network by looking at the
                        flows at each of the four intersections.
                  \item How many solutions are there? If there are solutions, which, if any, give rise to feasible
                        traffic flow numbers?
                  \item Is it possible for one of the four segments in the network to be closed off for construction
                        (a perennial occurrence in Honolulu) so that the network will still be able to support smooth traffic
                        flow? Explain.
              \end{enumerate}
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.5\linewidth]{fig_7_32}
                  \caption{Rush-hour traffic on some Honolulu streets.}
                  \label{fig:fig_7_32}
              \end{figure}
        \item \textit{(City Planning: Traffic Logistics)} Figure \ref{fig:fig_7_33} shows
              a busy network of one-way roads in the center of a city. The rush-hour inflows and
              outflows of vehicles per hour for the network are given as well as a listing of the nine
              variables that represent hourly flows of vehicles in the network.
              \begin{enumerate}
                  \item Following the directions of the preceding exercise, use the equilibria at each intersection
                        to obtain a system of linear equations in the nine variables that will govern smooth traffic flow in this network.
                  \item Use MATLAB's \mycode{rref} to solve this system. There are going to be infinitely many
                        solutions, but of course not all are going to be feasible answers to the original problem. For
                        example, we cannot have a negative traffic flow on any given street segment, and also the $x_j$'s
                        should be integers. Thus the solutions consist of vectors with eight components where each
                        component is a nonnegative integer.
                  \item Considering all of the feasible solutions that were contemplated in part (b), what is the
                        maximum that $x_6$ can be (in a solution)? What is the minimum?
                  \item Repeat part (c) for $x_8$.
                  \item If the city wants to have a parade and close up one of the segments (corresponding to some
                        $X_j$ in the figure) of the town center, is it possible to do this without disrupting the main traffic
                        flow?
                  \item If you answered yes to part (e), go further and answer this question. The mayor would like to
                        set up a "Kick the Fat" 5K run through some of the central streets in the city. How many
                        segments (corresponding to some $x_j$ in the figure) could the city cordon off without disrupting
                        the main flow of traffic? Answer the same question if we require, in addition, that the streets
                        which are cordoned off are connected together.
              \end{enumerate}
              \begin{figure}[h]
                  \centering
                  \includegraphics[width=0.5\linewidth]{fig_7_33}
                  \caption{Rush-hour traffic in a busy city center.}
                  \label{fig:fig_7_33}
              \end{figure}
        \item \textit{(Economics: Input-Output Analysis)} In any large economy, major industries that are producing
              essential goods and services need products from other industries in order to meet their own
              production demands. Such demands need to be accounted for by the other industries in addition
              to the main consumer demands. The model we will give here is due to Russian ecomomist
              Wassily Leontief (1906-1999).
              \footnote{
                  In the 1930s and early 1940s, Leontief did an extensive analysis of the input and output of 500 sectors
                  of the US economy. The calculations were tremendous, and Leontief made use of the first large-scale
                  computer (in 1943) as a necessary tool. He won the Nobel Prize in Economics in 1973 for this
                  research. Educated in Leningrad, Russia (now again St. Petersburg, as it was before 1917), and in
                  Berlin, Leontif subsequently moved to the United States to become a professor of economics at
                  Harvard. His family was quite full of intellectuals: His father was also a professor of economics, his
                  wife (Estelle Marks) was a poet, and his daughter is a professor of art history at the University of
                  California at Berkeley.
              }
              To present the main ideas, we deal with only three dependent
              industries: (i) electricity, (ii) steel, and (iii) water. In a certain economy, let us assume that
              the outside demands for each of these three industries are (annually) $d_1$ = \textdollar140 million for
              electricity, $d_2$ = \textdollar46 million for steel, and $d_3$ = \textdollar92 million for water. For each dollar that the
              electricity industry produces each year, assume that it will cost \textdollar0.02 in electricity, \textdollar0.08 in
              steel, and \textdollar0.16 in water. Also for each dollar of steel that the steel industry produces each year,
              assume that it will cost \textdollar0.05 in electricity, \textdollar0.08 in steel, and \textdollar0.10 in water. Assume the
              corresponding data for producing \textdollar1 of water to be \textdollar0.12, \textdollar0.07, and \textdollar0.03. From this data, we
              can form the so-called \textbf{technology matrix}:
              $$
                  M =
                  \begin{blockarray}{cccc}
                      & E & S & W \\
                      \begin{block}{c[ccc]}
                          \text{Electricity demand (per dollar)} & .02 & .05 & .12 \\
                          \text{Steel demand (per dollar)} & .08 & .08 & .07 \\
                          \text{Water demand (per dollar)} & .16 & .10 & .03 \\
                      \end{block}
                  \end{blockarray}.
              $$
              \begin{enumerate}
                  \item Let $x_1$ = The amount (in dollars) of electricity produced by the electricity industry,\\
                        $x_2$ = The amount of steel produced by the steel industry,\\
                        $x_3$ = The amount of water produced by the water industry, and let\\
                        $X = \begin{bmatrix}
                                x_1 \\ x_2 \\ x_3
                            \end{bmatrix}$.
                        The matrix $X$ is called the \mycode{output matrix}. Show/explain why the matrix $MX$
                        (called the internal demand matrix) gives the total internal costs of electricity, steel, and water
                        that it will collectively cost the three industries to produce the outputs given in $X$.
                  \item For the economy to function, the output of these three major industries must meet both the
                        external demand and the internal demand. The external demand is given by the following
                        \textit{external demand matrix}:
                        $$
                            D =
                            \begin{bmatrix}
                                d_1 \\ d_2 \\ d_3
                            \end{bmatrix}
                            =
                            \begin{bmatrix}
                                140,000,000 \\ 46,000,000 \\ 96,000,000
                            \end{bmatrix}
                        $$
                        (The data was given above.) Thus since the total output $X$ of the industries must meet both the
                        internal $MX$ and external $D$ demands, the matrix $X$ must solve the matrix equation:
                        $$X = MX+D \Rightarrow (I - M)X = D.$$
                        It can always be shown that the matrix $I - M$ is nonsingular and thus there is always going to
                        be a unique solution of this problem. Find the solution of this particular input/output problem.
                  \item  In a particular year, there is a construction boom and the demands go up to these values:
                        $$d_1 = \$160 mil., d_2 = \$87mil., d_3 = \$104mil$$
                        How will the outputs need to change for that year?
                  \item In a recessionary year, the external demands drop to the following numbers:
                        $$d_1 = \$90 mil., d_2 = \$18 mil., d_3 = \$65 mil$$
                        Find the corresponding outputs.
              \end{enumerate}
        \item \textit{(Economics: Input-Output Analysis)} Suppose, in the economic model of the previous exercise,
              two additional industries are added: (iv) oil and (v) plastics. In addition to the assumptions of
              the previous exercise, assume further that it has been determined that for each dollar of
              electricity produced it will cost \textdollar0.18 in oil and \textdollar0.03 in plastics, for each dollar of steel
              produced it will cost \textdollar0.07 in oil and \textdollar0.01 in plastics, for each dollar of water produced it will
              cost \textdollar0.02 in plastics (but no oil), for each dollar of oil produced it will cost \textdollar0.06 in electricity,
              \textdollar0.02 in steel, \textdollar0.05 in water, and \textdollar0.01 in plastics (but no oil), and finally for each dollar in
              plastics produced, it will cost \textdollar0.02 in electricity, \textdollar0.01 in steel, \textdollar0.02 in water, \textdollar0.22 in oil,
              and \textdollar0.12 in plastics.
              \begin{enumerate}
                  \item Write down the technology matrix.
                  \item  Assuming the original external demands for the first three industries given in the preceding
                        exercise and external demands of $d_4$ = \textdollar188 mil. for oil and $d_5$ = \textdollar35 mil. for plastics, solve
                        the Leontief model for the resulting output matrix.
                  \item Resolve the model using the data in part (c) of Exercise 7, along with $d_4$ = \textdollar209 mil.,
                        $d_5$ = \textdollar60 mil.
                  \item Resolve the model using the data in part (c) of Exercise 7, along with $d_4$ =\textdollar149 mil.,
                        $d_5$ =\textdollar16 mil.
              \end{enumerate}
              NOTE: \textit{(Combinatorics: Power Sums)} It is often necessary to find the sum of fixed powers of the first
              several positive integers. Formulas for the sums are well known but it is difficult to remember them
              all. Here are the first four such power sums:
              \begin{equation} \label{equation:7_23}
                  \sum_{k=1}^{n} k = 1 + 2 + 3 + \dots + n = \frac{1}{2}n(n+1)
              \end{equation}
              \begin{equation} \label{equation:7_24}
                  \sum_{k=1}^{n} k^2 = 1 + 4 + 9 + \dots + n^2 = \frac{1}{6}n(n+1)(2n+2)
              \end{equation}
              \begin{equation} \label{equation:7_25}
                  \sum_{k=1}^{n} k^3 = 1 + 8 + 27 + \dots + n^3 = \frac{1}{4}n^2(n+1)^2
              \end{equation}
              \begin{equation} \label{equation:7_26}
                  \sum_{k=1}^{n} k^4 = 1 + 16 + 81 + \dots + n^4 = \frac{1}{30}n(n+1)(2n+2)(3n^2+3n-1)
              \end{equation}
              Each of these formulas and more general ones can be proved by mathematical induction, but deriving
              them is more involved. It is a general fact that for any positive integer $p$, the power sum
              $\sum_{k=1}^{n} k^p$ can always be expressed as a polynomial $f(n)$ that has degree $p + 1$ and has rational coefficients
              (fractions). See Section 3.54 (p. 199ft) of [Ros-00] for details. The next two exercises will show a way
              to use linear systems not only to verify, but to derive such formulas.
        \item \textit{(Combinatorics: Power Sums)}
              \begin{enumerate}
                  \item  Use the fact (from the general fact mentioned in the preceding note) that $\sum_{k=1}^{n} k$ can
                        be be expressed as $f(n)$ where $f(n)$ is a polynomial of degree 2: $f(n)=an^2 + bn + c$, to set up
                        a linear system for a, b ,c using $f(1)=1, f(2)=3, f(3)=6$, and use  MATLAB (in "format rat") to solve for
                        the coefficients and then verify identity (\ref{equation:7_23}).
                  \item In a similar fashion, verify identity (\ref{equation:7_24}).
                  \item In a similar fashion, verify identity (\ref{equation:7_25}).
                  \item In a similar fashion, verify identity (\ref{equation:7_26}).
              \end{enumerate}
              \textbf{Note:} If you have the Student Version of MATLAB (or have access to the Symbolic Toolbox),
              the command \mycode{factor} can be used to develop your formulas in factored form-see Appendix
              A; otherwise leave them in standard polynomial form.
        \item \textit{(Combinatorics: Power Sums)}
              \begin{enumerate}
                  \item By mimicking the approach of the previous exercise, use
                        MATLAB to get a formula for the power sum $\sum_{k=1}^{n} k^5$. Check your formula for the values
                        $n=5$, and $n=100$ (using MATLAB, of cours).
                  \item Repeat for the sum $\sum_{k=1}^{n} k^6$
              \end{enumerate}
              \textbf{Note:} If you have the Student Version of MATLAB (or have access to the Symbolic Toolbox),
              the command \mycode{factor} can be used to develop your formulas in factored form—see Appendix
              A; otherwise leave them in standard polynomial form.
        \item \textit{(Combinatorics: Alternating Power Sum)} For positive integers $p$ and $n$ the \textit{alternating power sum}:
              $$\sum_{k=1}^{n} (-1)^{n-k}k^p = n^p - (n-1)^p + (n-2)^p - \dots + (-1)^{n-2}2^p + (-1)^{n-1},$$
              like the power sum, can be expressed as f(n) where f(n) is polynomials of degree $n+1$
              having rational coefficients. (For details, see [Ros-00], Section 3.17, page 152ff.) As in
              Exercise 5, set up linear systems for the coefficients of these polynomial, use MATLAB to solve
              them, and develop formulas for the alternating power sums for the following values of $p$. (a)
              $p = 1$, (b) $p = 2$, (c) $p = 3$, (d) $p = 4$. For each formula you derive, get MATLAB to check
              it (against the actual power sum) for the following values of n: 10,250, and 500.
        \item \textit{(Linear Algebra: Cramer's Rule)} There is an attractive and explicit formula for solving a
              nonsingular system Ax-b which expresses the solution of each component of the vector x
              entirely in terms of determinants. This formula, known as Cramer's rule,
              \footnote{
                  Gabriel Cramer (1704-1752) was a Swiss mathematician who is credited for introducing his
                  namesake rule in his famous book, Introduction a I'analyse des lignes courbes aigébraique. Cramer
                  entered his career as a mathematician with impressive speed, earning his PhD at age 18 and being
                  awarded a joint chaired professorship of mathematics at the Académie de Clavin in Geneva. With his
                  shared appointment, he would take turns with his colleague Caldrini teaching for 2 to 3 years.
                  Through this arrangement he was able to do much traveling and made contacts with famous
                  mathematicians throughout Europe; all of the famous mathematicians that met him were most
                  impressed. For example, the prominent Swiss mathematician Johann Bernoulli (1667-1748), insisted
                  that Cramer and only Cramer be allowed to edit the former's collected works. Throughout his career,
                  Cramer was always very productive. He put great energy into his teachings, his researches, and his
                  correspondences with other mathematicians whom he had met. Despite the lack of practicality of
                  Cramer's rule, Cramer actually did a lot of work in applying mathematics to practical areas such as
                  national defense and structural design. Cramer was always in good health until an accidental fall off a
                  carriage. His doctor recommended for him to go to a resort city in the south of France for
                  recuperation, but he passed away on that journey.
              }
              is often given in linear algebra courses (because it has nice theoretical applications), but it is a very expensive
              one to use. Cramer's rule states that the solution $x = [x_1 \; x_2 \; \dots \; x_n]'$ of the (nonsingular)
              system $Ax = b$, is given by the following formulas:
              $$x_1 = \frac{det(A_1)}{det(A)}, x_2 = \frac{det(A_2)}{det(A)}, \dots, x_n=\frac{det(A_n)}{det(A)},$$
              where the nxn matrix $A_i$ is formed by replacing the ith column of the coefficient matrix $A$,
              by the column vector $b$.
              \begin{enumerate}
                  \item Use Cramer's rule to solve the linear system of Examples \ref{example:7_7} and \ref{example:7_8}, and compare
                        performance time and (if you have Version 5) flop counts and accuracies with Methods 1,2, and
                        3 that were used in the text.
                  \item Write a function M-file called \mycode{x = cramer(A,b)} that will input a square nonsingular
                        matrix Ay a column vector $b$ of the same dimension, and will output the column vector solution
                        of the linear system $Ax = b$ obtained using Cramer's rule. Apply this program to resolve the
                        two systems of part (a).
              \end{enumerate}
              \textbf{Note:}  Of course, you should set up your calculations and programs so that you only ask
              MATLAB to compute $det(A)$ once, each time you use Cramer's rule.
    \end{enumerate}
\end{Exercises}

\section{GAUSSIAN ELIMINATION, PIVOTING, \\AND LU FACTORIZATION} \label{sec:7_5}
\noindent Here is a brief outline of this section. Our goal is a general method for solving the
linear system (\ref{equation:7_17}) with $n$ variables and $n$ equations, and we will be working with
the corresponding augmented matrix [A \textbrokenbar b] from the resulting matrix equation
(\ref{equation:7_18}). We will first observe that if a linear system has a triangular matrix $A$ (i.e.,
all entries below the main diagonal are zero, or all entries above it are zero), then
the linear system is very easy to solve. We next introduce the three elementary
row operations that, when performed on an augmented matrix, will lead to another
augmented matrix which represents an equivalent linear system that has a
triangular coefficient matrix and thus can be easily solved. We then explain the
main algorithm, Gaussian elimination with partial pivoting, that will transform any
augmented matrix (representing a nonsingular system) into an upper triangular
system that is easily solved. This is less work than the usual "Gaussian
elimination" taught in linear algebra classes, since the latter brings the
augmented matrix all the way into reduced row echelon form. The partial
pivoting aspect is mathematically redundant, but is numerically very
important since it will help to cut back on floating point arithmetic errors.
Gaussian elimination produces a useful factorization of the coefficient matrix,
called the LU factorization, that will considerably cut down the amount of work needed to solve other
systems having the same coefficient matrix. We will postpone the error analysis of this procedure to
the next section. The main algorithms of this section were invented by the German mathematician Carl F.
Gauss.\footnote{
    Carl F. Gauss is acknowledged by many mathematical scholars as the greatest mathematician who
    ever lived. His potential was discovered early. His first mathematical discovery was the power sum
    identity (23), and he made this discovery while in the second grade! His teacher, looking to keep
    young Carl Friedrich occupied for a while asked him to perform the addition of the first 100 integers:
    $S = 1 + 2 + ... + 100$. Two minutes later, Gauss gave the teacher the answer. He did it by rewriting the
    sum in the reverse order $S = 100 + 99 + ... + 1$, adding vertically to the original to get $2S= 101 + 101 +
        ... + 101=100 \cdot 101$, so $S = 50 \cdot 101 = 5050$ . This idea, of course, yields a general proof of the identity
    (\ref{equation:7_23}). He was noticed by the Duke of Brunswick, who supported Gauss's education and intellectual
    activity for many years. Gauss's work touched on numerous fields of mathematics, physics, and other
    sciences. His contributions are too numerous to attempt to do them justice in this short footnote. It is
    said that very routinely when another mathematician would visit him in his office to present him with a
    recent mathematical discovery, after hearing about the theorems, Gauss would reach into his file
    cabinet and pull out some of his own works, which would invariably transcend that of his guest. For
    many years, until 2001 when the currency in Germany, as well as that of other European countries,
    changed to the Euro, Germany had honored Gauss by putting his image on the very common 10
    Deutsche Mark banknote (value about \textdollar5); see Figure \ref{fig:fig_7_34}.
}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{fig_7_34}
    \caption{Carl Friedrich Gauss (1777-1855), German mathematician.}
    \label{fig:fig_7_34}
\end{figure}

A square matrix $A = [a_{ij}]$ is called upper triangular if all entries below the main
diagonal equal zero (i.e., $a_{ij} = 0 $whenever $i > j$) . Thus an upper triangular
matrix has the following form:
$$
    \begin{bmatrix}
        a_{11} & a_{12} & a_{13} & \dots  & a_{1n} \\
               & a_{22} & a_{23} & \dots  & a_{2n} \\
               &        & a_{33} & \dots  & \vdots \\
               & 0      &        & \ddots & \vdots \\
               &        &        &        & a_{nm} \\
    \end{bmatrix}
$$
Similarly, a square matrix is lower triangular if all entries above the main
diagonal are zeros. A matrix is triangular if it is of either of these two forms.
Many matrix calculations are easy for triangular matrices. The next proposition
shows that determinants for triangular matrices are extremely simple to compute.

\noindent \textbf{PROPOSITION 7.3:} \label{propossition:7_3}
If $A =[a_{ij}]$ is a triangular matrix, then the determinant of
A is the product of the diagonal entries, i.e., $det(A) = a_{11}a_{12}\dots a_{nm}$.

\noindent The proof can be easily done by mathematical induction and cofactor expansion
on the first column or row; we leave it as Exercise 14(a).

From the proposition, it follows that for a triangular matrix to be nonsingular it is
equivalent that each of the diagonal entries must be nonzero. For a system $Ax = b$
with an upper triangular coefficient matrix, we can easily solve the system by
starting with the last equation, solving for $x_n$, then using this in the second-to-last
equation and solving for. $x_{n-1}$, and continuing to work our way up. Let us make
this more explicit. An upper triangular system has the form:
\begin{equation} \label{equation:7_27}
    \begin{cases}
        \begin{matrix}
            a_{11}x_1+ & a_{12}x_2 + & \dots &  & + a_{1n}x_n = b_1                          \\
                       & a_{22}x_2 + & \dots &  & + a_{1n}x_n = b_1                          \\
                       & \ddots      &       &  & \vdots                                     \\
                       &             &       &  & a_{n-1,n-1}x_{n-1} +a_{n-1,n}x_n = b_{n-1} \\
                       &             &       &  & a_{nm}x_n = b_n                            \\
        \end{matrix}
    \end{cases}
\end{equation}
Assuming A is nonsingular, we have that each diagonal entry aH is nonzero.
Thus, we can start off by solving the last equation of (\ref{equation:7_27}):
$$x_n = b_n / a_{nm}.$$
Knowing now the value of $x_n$, we can then substitute this into the second-to-last
equation and solve for $x_{n-1}$:
$$x_{n-1} = (b_{n-1} - a_{n-1,n}x_n)/a_{n-1,n-1}$$
Now that we know both $x_n$ and $x_{n-1}$, we can substitute these into the third-to-last
equation and then, similarly, solve for the only remaining unknown in this
equation: $$x_{n-2} = (b_{n-2} - a_{n-2,n-1}x_{n-1}-a_{n-2,n}x_n)/a_{n-2,n-2}$$
If we continue this process, in general, after having solved for $x_{j+1}, x_{j+2}, \dots x_n$, we
can get $x$. by the formula:
\begin{equation} \label{equation:7_28}
    x_j = \left({b_j - \sum_{k=j+1}^{n} a_{jk}x_k}\right) / a_{jj}
\end{equation}
This algorithm is called \textbf{back substitution}, and is a fast and easy method of
solving any upper triangular (nonsingular) linear system. Would it not be nice if
all linear systems were so easy to solve? Transforming arbitrary (nonsingular)
linear systems into upper triangular form will be the goal of the Gaussian
elimination algorithm. For now we record for future reference a simple M-file for
the back substitution algorithm.

\begin{Example}
    \begin{enumerate}
        \item Create an M-file \mycode{x=backsubst(U,b)} that inputs a
              nonsingular upper triangular matrix U, and a column vector b of the same
              dimension and the output will be a column vector x which is the numerical
              solution of the linear system $Ux = b$ obtained from the back substitution algorithm.
        \item Use this algorithm to solve the system $Ux = b$ with
              $$
                  U=
                  \begin{bmatrix}
                      1 & 2 & 3 & 4 \\
                      0 & 2 & 3 & 4 \\
                      0 & 0 & 3 & 4 \\
                      0 & 0 & 0 & 4 \\
                  \end{bmatrix},
                  b=
                  \begin{bmatrix}
                      4 \\ 3 \\ 2 \\ 1
                  \end{bmatrix}
              $$
    \end{enumerate}

    \begin{Solution}
        Part (a): The M-file is easily written using equation (\ref{equation:7_28}).
        \begin{Program}
            Function M-file solving an upper triangular linear system $Ux = b$.
            \begin{lstlisting}[numbers=none]
function x=backsubst(U,b)
%Solves the upper triangular system Ux = b by back substitution
%inputs: U - upper triangular matrix, b - column vector of same dimensions
%Output: x - column vector (solution)
[n m]=size(U);
x(n)=b(n)/U(n,n);
for j=n-1:-1:1
    x(j)=(b(j)-U(j,j+l:n)*x(j+l:n)')/U(j,j) ;
end
x=x';
            \end{lstlisting}
        \end{Program}
        Notice that MATLAB's matrix multiplication allowed us to replace the sum in
        (\ref{equation:7_28}) with the matrix product shown. Indeed \mycode{U(j,j+1:n)} is a row vector with
        the same number of entries as the column vector \mycode{x(j+1:n)}, so their matrix
        product will be a real number ($l \times l$ matrix) equaling the sum in (\ref{equation:7_28}). The
        transpose on the x-vector was necessary here to make it a column vector.

        \noindent Part (b):\\
        \mycode{
            >> U=[1 2 3 4; 0 2 3 4; 0 0 0 3 4; 0 0 0 4]; b=[4 3 2 1]';\\
            >> format rat, backsubst(U,b)\\
            $\rightarrow$ans=
            $\begin{matrix}
                1 \\ 1/2 \\ 1/3 \\ 1/4
            \end{matrix}$\\
        }
        The reader can check that $U\backslash b$ will give the same result.

        A lower triangular system $Lx = b$ can be solved with an analogous algorithm
        called forward substitution. Here we start with the first equation to get $x_1$, then
        plug this result into the second equation and solve for $x_2$, and so on.
    \end{Solution}
\end{Example}










%
%
%



%%\section{Gaussian Elimination, Pivoting, and LU Factorization}


The reader can check that U $\backslash$ b will give the same result. 
\\

A lower triangular system $Lx = b$ can be solved with an analogous algorithm
called \textbf{forward substitution}. Here we start with the first equation to get $x_1$, then
plug this result into the second equation and solve for $x_2$, and so on.
\\
\\
EXERCISE FORTHEREADER 7.18: (a) Write a function M-file called \texttt{x=fwdsubst (L, b)} 
 , that will input a lower triangular matrix L, a column vector b
of the same dimension, and will output the column vector x solution of the system
$Lx = b$ solved by forward substitution, (b) Use this program to solve the system
$Lx = b$ , where $L = U'$, and U and b are as in Example 7.11.
\\

We now introduce the \textbf{three elementary row operations (EROs)} that can be
performed on augmented matrices. 
\\
\begin{tabular}{l l l}
   \\
  & (i)  	&	Multiply a row by a nonzero constant.  \\
  & (ii)	&	Switch two rows.  \\
  & (iii)	&	Add a multiple of one row to a different row. \\
 
\end{tabular}
\\
\\
\\
\textbf{EXAMPLE 7.12:}~~~~(a)~~~~Consider the following augmented matrix: 
\\
$\displaystyle
Ab=\left[
\begin{array}{rrr}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
\end{array}
\right]
$
. Perform ERO (iii) on this matrix by adding the multiple 
\\
-2 times row 1 to row 2
\\
(b) Perform this same ERO on $I_3$ , the $3\times 3$ identity matrix, to obtain a matrix
$M$, and multiply this matrix $M$ on the left of $Ab$. What do you get?
\\
\\
SOLUTION: Part (a): -2 times row 1 of $Ab$ is-2[l 2- 3 5]= [-2 -4 6 -10].
	Adding this row vector to row 2 of $Ab$, produces the new matrix:
\\
$$\left[\begin{array}{cccc}
1 & 2 & -3 & 5\\
0 & 2 & 7 & -11\\
0 & 4 & 7 & 8
\end{array}\right].$$
\\
Part (b): Performing this same ERO on $I_3$ produces the matrix 
$\displaystyle
M=\left[
\begin{array}{rrr}
1 & 0 & 0 \\
-2 & 1 & 0 \\
0 & 0 & 1 \\
\end{array}
\right],
$
\\
which when multiplied by Ab gives 


$\displaystyle
M(Ab) =\left[
\begin{array}{rrr}
1 & 0 & 0 \\
-2 & 1 & 0 \\
0 & 0 & 1 \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{rrrr}
1 & 2 & -3 & 5 \\
2 & 6 & 1 & -1 \\
0 & 4 & 7 & 8\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrrr}
1 & 2 & -3 & 5 \\
0 & 2 & 7 & -11 \\
0 & 4 & 7 & 8 \\
\end{array}
\right],
$
\\
\\
and we are left with the same matrix that we obtained in part (a). This is no
coincidence, as the following theorem shows. 
\\
\\
\textbf{THEOREM 7.4:} \textit{(Elementary Matrices}) Let $A$ be any $n\times m$ matrix and $I=I_n$
denote the identity matrix. If $B$ is the matrix obtained from $A$ by performing any
particular elementary row operation, and $M$ is the matrix obtained from $I$ by
performing this same elementary row operation, then $B = MA$. Also, the matrix
$M$ is invertible, and its inverse is the matrix that results from $I$ by performing
the inverse elementary row operation on it (i.e., the elementary row operation that
will transform $M$ back into $I$ ). 
\\

Such a matrix $M$ is called an \textbf{elementary matrix}. This result is not hard to
prove; we refer the reader to any good linear algebra textbook, such as those
mentioned in the last section
\\

It is easy to see that any of these EROs, when applied to the augmented matrix of
a linear system, will not alter the solution of the system. Indeed, the first ERO
corresponds to simply multiplying the first equation by a nonzero constant (the
equation still represents the same line, plane, hyperplane, etc.). The second ERO
merely changes the order in which the equations are written; this has no effect on
the (joint) solution of the system. To see why the third ERO does not alter
solutions of the system is a bit more involved, but not difficult. Indeed, suppose
for definiteness that a multiple (say, 2) of the first row is added to the second.
This corresponds to a new system where all the equations are the same, except for
the second, which equals the old second equation plus twice the first equation.
Certainly if we have all of $x_1,x_2,\dots,x_n$ satisfying the original system, then they
will satisfy the new system. Conversely, if all of the equations of the new system
are solved by $x_1,x_2,\dots,x_n$, then this already gives all but the second equation of
the original system. But the second equation of the old system is gotten by
subtracting twice the first equation of the new system from the second equation (of
the new system) and so must also hold.
\\

Each of the EROs is easily programmed into an M-file. We do one of 
them and
leave the other two as exercises.
\\

\subsubsection{PROGRAM 7.5:}Function M-file for elementary row operation (ii): switching two rows.

\begin{lstlisting}[numbers=none]
function B=rowswitch(A,i,j) 
->Imputs: a matrix A, and row indicos i and j
->outputs: the matrix gotton from A by interchanging row i and row j
[m,n]=size(A);
if i<l|i>m|j<l|j>m
	error('Invalid index')
end
B=A;
if i==j
	return
end
B(i,:)=A(j,:);
B(j,:)=A(i,:); 
\end{lstlisting}

It may seem redundant to have included that if-branch for detecting invalid
indices, since it would seem that no one in their right mind would use the program
with, say, an index equaling 10 with an $8\times 8$ matrix. This program, however,
might get used to build more elaborate programs, and in such a program it may not
always be crystal clear whether some variable expression is a valid index. 
\\
\\
EXERCISE FOR THE READER 7.19: Write similar function M-files
\texttt{B=rowmult (A,i,c)} for ERO (i) and \texttt{B=rowcomb (A,i,j,c)} for ERO (iii).
The first program will produce the matrix $B$ resulting from $A$ by multiplying the $i$th
row of the latter by $c$ the second program should replace the $i$th row of $A$ by $c$
times the $j$th row plus the $i$th row. 
\\

We next illustrate the Gaussian elimination algorithm, the partial pivoting
feature, as well as the $LU$ decomposition, by means of a simple example. This
will give the reader a feel for the main concepts of this section. Afterward we will
develop the general algorithms and comment on some consequences of floating
point arithmetic. Remember, the goal of Gaussian elimination is to transform,
using only EROs, a (nonsingular) system into an equivalent one that is upper
triangular; the latter can then be solved by back substitution.
\\
\\
\textbf{EXAMPLE 7.13:} We will solve the following linear system $Ax = b$ using
Gaussian elimination (without partial pivoting):
\\
$$\
\begin{cases} 
x_1 + 3x_2 - x_3 = 2\\
2x_1 + 5x_2 - 2x_3 = 3\\
3x_1 + 6x_2 + 9x_3 = 39
\end{cases}.$$
\\
SOLUTION: For convenience, we will work instead with the corresponding
augmented matrix Ab for this system $Ax = b$ : 
\\
$$\displaystyle
Ab=\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
2 & 5 & -2 & 3\\
3 & 6 & 9 & 39 \\
\end{array}
\right].
$$
\\
For notational convenience, we denote the entries of this or any future augmented
matrix as $a_{ij}$. In computer codes, what is usually done is that at each step the new
matrix overwrites the old one to save on memory allocation (not to mention having
to invent new variables for unnecessary older matrices). Gaussian elimination
starts with the first column, clears out (makes zeros) everything below the main
diagonal entry, then proceeds to the second column, and so on. 
\\

We begin by zeroing out the entry $a_{2l} = 2$ \texttt{: Ab=rowcomb (Ab, 1 , 2, -2).}
\\
$$\displaystyle
Ab\rightarrow M_1(Ab)\left[
\begin{array}{rrr}
1 & 0 & 0  \\
-2 & 1 & 0 \\
0 & 0 & 1  \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
2 & 5 & -2 & 3\\
3 & 6 & 9 & 39 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
0 & -1 & 0 & -1\\
3 & 6 & 9 & 39 \\
\end{array}
\right],
$$
where $M_1$, is the corresponding elementary matrix as in Theorem 7.4. In the same
fashion, we next zero out the next (and last) entry $a_{31} = 3$ of the first column:
\texttt{Ab=rowcomb(Ab,1,3,-3).}
\\
$$\displaystyle
Ab\rightarrow M_2(Ab)\left[
\begin{array}{rrr}
1 & 0 & 0  \\
0 & 1 & 0 \\
-3 & 0 & 1  \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
0 & -1 & 0 & -1\\
3 & 6 & 9 & 39 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
0 & -1 & 0 & -1\\
0 & -3 & 12 & 33 \\
\end{array}
\right],
$$
\\
We move on to the second column. Here only one entry needs clearing, namely
$a_{32} = -3$ . We will always use the row with the corresponding diagonal entry to
clear out entries below it. Thus, to use the second row to clear out the entry
$a_{32} = -3$ in the third row, we should multiply the second row by -3 since
$-3\cdot a_{22}=-3\cdot(-1) = 3$ added to $a_{32}=-3$ would give zero: \texttt{Ab=rowcomb(Ab,2,3,-3)}
$$\displaystyle
Ab\rightarrow M_3(Ab)\left[
\begin{array}{rrr}
1 & 0 & 0  \\
0 & 1 & 0 \\
0 & -3 & 1  \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
0 & -1 & 0 & -1\\
0 & -3 & 12 & 33 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrr:r}
1 & 3 & -1 & 2 \\
0 & -1 & 0 & -1\\
0 & 0 & 12 & 36 \\
\end{array}
\right].
$$
\\
We now have an augmented matrix representing an equivalent upper triangular
system. This system (and hence our original system) can now be solved by the
back substitution algorithm:
\\
\begin{lstlisting}[numbers=none,frame=none]
>> U=Ab(:,1:3); b=Ab(:,4);
>> x=backsubst(U,b)
-> x =		2
		1
		3
\end{lstlisting}
NOTE: One could have gone further and similarly cleared out the above diagonal
entries and then used the first ERO to scale the diagonal entries to each equal one.
This is how one gets to the reduced row echelon form.
\\

To obtain the resulting $LU$ decomposition, we form the product of all the
elementary matrices that were used in the above Gaussian elimination:
$M=M_3M_2M_l$. From what was done, we have that $MA=U$, and hence
\\

$A=M^{-1}(MA)=M^{-1}U=(M_3M_2M_1)^{-1}U=M_1^{-1}M_2^{-1}M_3^{-1}U\equiv LU,$
\\
\\
where we have defined $L=M_1^{-1}M_2^{-1}M_3^{1}$.We have used, in the second-to-last
equality, the fact that the inverse of a product of invertible matrices is the product
of the inverses in the reverse order (see Exercise 7). From Theorem 7.4, each of
the inverses $M_1^{-1}$, $M_2^{-1}$, and $M_3^{-1}$is also an elementary matrix corresponding to
the inverse elementary row operation of the corresponding original elementary
matrix; and furthermore Theorem 7.4 tells us how to multiply such matrices to
obtain 
\\
$$\displaystyle
L=M_1^{-1}M_2^{-1}M_3^{-1}\left[
\begin{array}{rrr}
1 & 0 & 0  \\
2 & 1 & 0 \\
0 & 0 & 1  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{rrr}
1 & 0 & 0  \\
0 & 1 & 0 \\
3 & 0 & 1  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{rrr}
1 &  0& 0 \\
0 & 1 & 0 \\
0 & 3 & 1  \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrr}
1 &  0& 0 \\
2 & 1 & 0 \\
3 & -3 & 1  \\
\end{array}
\right].
$$
\\

We now have a factorization of the coefficient matrix $A$ as a product $LU$ of a
lower triangular and an upper triangular matrix:
\\
$$\displaystyle
A=Ab=\left[
\begin{array}{rrr}
1 & 3 & -1  \\
2 & 5 & -2 \\
3 & 6 & 9  \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{rrr}
1 & 0 & 0  \\
2 & 1 & 0 \\
3 & 3 & 1  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{rrr}
1 & 3 & -1 \\
0 & -1 & 0 \\
0 & 0 & 12  \\
\end{array}
\right]
=LU.
$$
\\
This factorization, which easily came from the Gaussian elimination algorithm, is
a preliminary form of what is known as the $LU$ factorization of $A$. Once such a
factorization is known, any other (nonsingular) system $Ax = c$, having the same
coefficient matrix $A$, can be easily solved in two steps. To see this, rewrite the
system as $LUx = c$. First solve $Ly = c$ by forward substitution (works since $L$ is
lower triangular), then solve $Ux = y$ by back substitution (works since $U$ is upper
triangular). Then x will be the desired solution (Proof: $Ax = (LU)x = L(Ux) =
Ly = c)$.
\\

We make some observations. Notice that we used only one of the three EROs to
perform Gaussian elimination in the above example. Part of the reason for this is
that none of the diagonal entries encountered was zero. If this had happened we
would have needed to use the \texttt{rowswitch} ERO in order to have nonzero
diagonal entries. (This is always possible if the matrix $A$ is nonsingular.) In
Gaussian elimination, the diagonal entries that are used to clear out the entries
below (using \texttt{rowcomb}) are known as \textbf{pivots}. The \textbf{partial pivoting} feature,
which is often implemented in Gaussian elimination, goes a bit further to assure
(by switching the row with the pivot with a lower row, if necessary) that the pivot
is as large as possible in absolute value. In exact arithmetic, this partial pivoting
has no effect whatsoever, but in floating point arithmetic it can most certainly cut
back on errors. The reason for this is that if a pivot turned out to be nonzero, but
very small, then its row would need to be multiplied by very large numbers to
clear out moderately sized numbers below the pivot. This may cause other
numbers in the pivot's row to get multiplied into very large numbers that, when
mixed with much smaller numbers, can lead to floating point errors. We will soon
give an example to demonstrate this phenomenon. 
\\
\\
\textbf{EXAMPLE 7.14}: Solve the linear system $Ax = b$ of Example 7.13 using
Gaussian elimination with partial pivoting.
\\
\\
SOLUTION: The first step would be to switch rows 1 and 3 (to make $\vert a_{11} \vert$ as
large as possible):\texttt{Ab=rowswitch (Ab, 1,3)}. 
\\
$$\displaystyle
Ab\rightarrow P_1(Ab)\left[
\begin{array}{ccc}
0 & 0 & 1  \\
0 & 1 & 0 \\
1 & 0 & 0  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{ccc:c}
1 & 3 & -1 & 2  \\
2 & 5 & -2 & 3\\
3 & 6 & 9 & 39\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{ccc:c}
3 & 6 & 9 &  39\\
2 & 5 & -2 & 3\\
1 & 3 & -1 & 2\\
\end{array}
\right].
$$
\\
(We will denote elementary matrices resulting from the \texttt{rowswitch} ERO by
$P_i's$) Next we pivot on the $a_{11} = 3$ entry to clear out the entries below it. To
clear out $a_{21}=2$ , we will do \texttt{Ab=rowcomb (Ab,
1,2,-2/3 ) } (i.e., to clear out $a_{21}=2$ we multiply row 1 by $-a_{21}/a_{11}=-2/$ 3 and add this to row 2) Similarly,to clear out $a_{31}=1$ we will do \texttt{>>Ab=rowcomb (Ab, 1,3,-1/3)}.Combining both ofthese elementary matrices into a single matrix $M_1$, we
may now write the result of these two EROs as follows:
\\
$$\displaystyle
Ab\rightarrow M_1(Ab)\left[
\begin{array}{ccc}
1 & 0 & 0  \\
-2/3 & 1 & 0 \\
-1/3 & 0 & 1  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{ccc:c}
3 & 6 & 9 & 39  \\
2 & 5 & -2 & 3\\
1 & 3 & -1 & 2\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{ccc:c}
3 & 6 & 9 &  39\\
0 & 1 & -8 & -23\\
1 & 1 & -4 & -11\\
\end{array}
\right].
$$
\\
The pivot $a_{22} = 1$ is already as large as possible so we need not switch rows and
can clear out the entry $a_{32} = 1$ by doing \texttt{Ab=rowcomb (Ab, 2,3,-1)}:
\\ 
$$\displaystyle
Ab\rightarrow M_2(Ab)\left[
\begin{array}{ccc}
1 & 0 & 0  \\
0 & 1 & 0 \\
0 & -1 & 1  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{ccc:c}
3 & 6 & 9 & 39  \\
0 & 1 & -8 & -23\\
0 & 1 & -4 & -11\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{ccc:c}
3 & 6 & 9 &  39\\
0 & 1 & -8 & -23\\
0 & 0 & 4 & 12\\
\end{array}
\right],
$$
\\
and with this the elimination is complete. 
\\

Solving this (equivalent) upper triangular system will again yield the above
solution. We note that this produces a slightly different factorization: From  $M_2M_1PA=U$, where $U$ is the left part of the final augmented matrix, we proceed
as in the previous example to get $PA=(M_2^{-1}M_1^{-1})U\equiv LU$,i.e.,
\\
$$\displaystyle
PA=\left[
\begin{array}{ccc}
0 & 0 & 1  \\
0 & 1 & 0 \\
1 & 0 & 0  \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{ccc}
1 & 3 & -1   \\
2 & 5 & -2 \\
3 & 6 & 9 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{ccc}
1 & 0 & 0 \\
2/3 & 1 & 0 \\
1/3 & 1 & 1 \\
\end{array}
\right]
\cdot
\displaystyle
\left[
\begin{array}{ccc}
3 & 6 & 9 \\
0 & 1 & -8 \\
0 & 0 & 4 \\
\end{array}
\right]
=LU.
$$
\\
This is the $LU$ factorization of the matrix $A$. We now explain the general
algorithm of \textbf{Gaussian elimination with partial pivoting} and the $LU$
factorization. In terms of EROs and the back substitution, the resulting algorithm
is quite compact.
\\
\\
\textbf{Algorithm for Gaussian Elimination with Partial Pivoting:} Given a linear
system $Ax=b$ with $A$ an $n\times n$ nonsinguiar matrix, this algorithm will solve for
the solution vector $x$. The algorithm works on the $n\times (n + 1)$ augmented matrix
$\displaystyle
\left[
\begin{array}{c:c}
A & B 
\end{array}
\right]
$, which we denote by $Ab$, but whose entries we still denote by $a_{ij}$. 
\begin{center}
\begin{tabular}{|l|}
\hline
if $\vert a_{kk}\vert =0$, exit program with message "A is singular".\\
for $i=k+1$ to $n$\\
$m_{ik} =a_{ik}/a_{kk}$\\
$A = rowcomb(A,k,i, -m_{ik})$\\
end i\\
end k\\
if $\vert a_{nn}\vert=0$, exit program with message'
Apply the back substitution algorithm
triangular) to get solution to the system. \\
\hline
\end{tabular}
\end{center}

Without the interchanging rows step (unless to avoid a zero pivot), this is
\textbf{Gaussian elimination} without partial pivoting. From now on, we follow the
standard convention of referring to Gaussian elimination with partial pivoting
simply as "Gaussian elimination," since it has become the standard algorithm for
solving linear systems.
\\

The algorithm can be recast into a matrix factorization algorithm for $A$. Indeed,
at the $k$th iteration we will, in general, have an elementary matrix $P_k$
corresponding to a row switch or permutation, followed by a matrix $M_k$ that
consists of the product of each of the elementary matrices corresponding to the
"rowcomb" ERO used to clear out entries below $a_{kk}$ . Letting $U$ denote the upper
triangular matrix left at the end of the algorithm, we thus have: 
\\
$$M_{n-1}P_{n-1}\dots M_2P_2M_1P_1A=U .$$ 
\\
The \textbf{LU factorization} (or the \textbf{LU decomposition}) of A, in general, has the form
(see Section 4.4 of [GoVL-83]):
\\
\begin{equation}
PA=LU ,
\end{equation}
Where
\begin{equation}
P=P_{n-1}P_{n-2}\dots P_2P_1~~~~and~~~~	 L=P(M_{n-1}P_{n-1}\dots M_1P_1)^{-1} ,
\end{equation}
and $L$ is lower triangular.\footnote{The permutation matrix $P$ in (29) cannot, in general, be dispensed with; see Exercise 6.}
 Also, by Theorem 7.4, the matrix $PA$ corresponds to
sequentially switching the rows of the matrix $A$, first corresponding to $P_1$, next
by $P_2$, and so on. Thus the $LU$ factorization $A$, once known, leads to a quick
and practical way to solve any linear system $Ax = b$. First, permute the order of
the equations as dictated by the permutation matrix $P$ (do this on the augmented
matrix so that $b's$ entries get permuted as well), relabel the system as $Ax=b$, and
rewrite it as $LUx = b$. First solve $Ly = c$ by forward substitution (works since $L$is lower triangular), then solve $Ux = y$ by back substitution (works since $U$ is
upper triangular). Then $x$ will be the desired solution (Proof:
\begin{equation}
PA=LU\Rightarrow Ax=P^{-1}LUx=P^{-1}L(Ux)=P^{-1}L(y)=p^{-1}Pb=b.)
\end{equation}
\\

This approach is useful if it is needed to solve a lot of linear systems with the
same coefficient matrix $A$. For such situations, we mention that MATLAB has a
built-in function \texttt{lu} to compute the $LU$ factorization of a nonsingular matrix $A$.
The syntax is as follows:
\begin{center}
\begin{tabular}{|l|l|}
\hline
&For a square singular matrix A, this command will output the lower\\
$[L, U, P]=lu(A)$&triangular matrix L, the upper triangular matrix \texttt{U}, and the permutation\\
$\rightarrow$ &matrix \texttt{P} of the $LU$ factorization (29) and (30) of the matrix A\\
\hline
\end{tabular}
\end{center}
For example, applying this command to the matrix A of Example 7.14 gives: 
\begin{lstlisting}[numbers=none,frame=none]
>>A=[ l 3 -1 ; 2 5 -2 ; 3 6 9] ; format rat , [L, U, P]=lu(A)
->L=			1	0	0
		   2/3	1	0
		   1/3	1	1
		   
U=			3	6	9
			0	1  -8
			0	0	4
			
P=			0	0	1
			0	1	0
			1	0	0
\end{lstlisting}

We now wish to translate the above algorithm for Gaussian elimination into a
MATLAB program. Before we do this, we make one remark about MATLAB's
built-in function max, which we have encountered previously in its default format
(the first syntax below):
\begin{center}
\begin{tabular}{|l|l|}
\hline
max(v)$\rightarrow$ &For a vector v, this command will give the maximum of its\\
&components.\\
\hline
[max, index]=max(v)&With an optional second output variable (that must be declared),\\
$\rightarrow$ &max(v) will also give the first index at which this maximum value occurs.\\
\hline
\end{tabular}
\end{center}
Here, $v$ can be either a row or a column vector. A simple example will illustrate
this functionality. 
\begin{lstlisting}[numbers=none,frame=none]
>>v=[1 -3 5 -7 9 -11]; 
>>max(v)								->ans = 9 
>>[max, index ] = max(v) 			->max = 9, index = 5
>>[max, index ] = max (abs (v)) 		->max = 11, index = 6 
\end{lstlisting}
Another useful tool for programming M-files is the \texttt{error} command:
 \begin{center}
\begin{tabular}{|l|l|}
\hline
error('message')$\rightarrow$&If this command is encountered with an execution of any M-file, the M-\\
&file stops running immediately and displays the message .\\
\hline
\end{tabular}
\end{center}
\textbf{PROGRAM 7.6:} Function M-file for Gaussian elimination (with partial pivoting) to solve
the linear system Ax = b> where A is a square nonsingular matrix. This program calls on
the previous programs backsubst, rowswitch, and rowcomb
\begin{lstlisting}[numbers=none]
function x=gausselim(A,b) 
%Inputs: Square matrix A, and column vector b of same dimension
%Output: Column vector solution x of linear system Ax - b obtained
%by Gaussian elimination with partial pivoting, provided coefficient.
%matrix A is nonsingular.
[n,n]=size(A);
Ab=[A';b']'; form augmented matrix for system

for k=l:n
	[biggest, occured] = max(abs(Ab(k:n,k))); 
	if biggest == 0
	error('the coefficient mattix is numerically singular') 
	end 
	m=k+occured-l; 
	Ab=rowswitch(Ab,k, m);
	for j=k+1:n	
		Ab=rowcomb(Ab,k,j,-Ab(j,k)/Ab(k, k) ) ;
	end 
end 
% BACK SUBSTITUTION
x=backsubst (Ab(:,1:n ),Ab(:,n+1) );
\end{lstlisting}
EXERCISE FOR THE READER 7.20: Use the program gausseli m to resolve
the Hubert systems of Example 7.9 and Exercise for the Reader 7.16, and compare
to the results of the left divide method that proved most successful in those
examples. Apart from additional error messages (regarding condition numbers),
how do the results of the above algorithm compare with those of MATLAB's
default system solver? 
\\

We next give a simple example that will demonstrate the advantages of partial
pivoting
\\

\subsubsection{EXAMPLE 7.15:}
Consider the following linear system: 
$\displaystyle
\left[
\begin{array}{cc}
10^{-10} & 1  \\
1 & 2  \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{c}
X_1  \\
X_2 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{c}
1  \\
3 \\
\end{array}
\right],
$
whose exact solution (starts) to look like
$
\displaystyle
\left[
\begin{array}{c}
X_1 \\
X_2 \\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{c}
1.0020... \\
.9989... \\
\end{array}
\right].
$
\\
(a) Using floating point arithmetic with three significant digits and chopped
arithmetic, solve the system using Gaussian elimination with partial pivoting.
\\
(b) Repeat part (a) in the same arithmetic, except without partial pivoting. 
\\
\\
SOLUTION: For each part we show the chain of augmented matrices. Recall,
after each individual computation, answers are chopped to three significant digits
before any subsequent computations.
Part (a): (with partial pivoting)
\\
$$
\displaystyle
\left[
\begin{array}{cc:c}
.001 & 1 & 1   \\
1 & 2 & 3 \\
\end{array}
\right]
\begin{array}{c}  \longrightarrow \\ _{rowswitch( A,1,2) }
\end{array}
\displaystyle
\left[
\begin{array}{cc:c}
1 & 2 & 3   \\
.001 & 1 & 1\\
\end{array}
\right]
\begin{array}{c}  \longrightarrow \\ _{rowcomb( A,1,2,-.001)}
\end{array}
\displaystyle
\left[
\begin{array}{cc:c}
1 & 2 & 3   \\
0 & .998 & .997\\
\end{array}
\right].
$$
Now we use back substitution: $x_2 = .997 / .998 = .998, x_1 = (3 - 2x_2 )/1
= (3 -1.99) /1 = 1.01$. Our computed answer is correct to three decimals in $x_2$, but
has a relative error of about 0.0798\% in $x_1$.
\\
Part (b): (without partial pivoting)
\\ 
$$
\displaystyle
\left[
\begin{array}{cc:c}
.001 & 1 & 1   \\
1 & 2 & 3 \\
\end{array}
\right]
\begin{array}{c}  \longrightarrow \\ _{rowcomb(A,1,2,-1000)}
\end{array}
\displaystyle
\left[
\begin{array}{cc:c}
1 & 1 & 1   \\
0 & -997 & -997\\
\end{array}
\right]
$$
\\
Back substitution now gives $x_2 = -997 /- 998 = .998 , x_1 = (1 - 1\cdot x_2) /1
= (1-.998)/1 = .002$. The relative error here is unacceptably large, exceeding
100\% in the second component! 
\\
\\
EXERCISE FOR THE READER 7.21: Rework the above example using rounded 
arithmetic rather than chopped, and keeping all else the same. 
\\

We close this section with a bit of information on flop counts for the Gaussian 
elimination algorithm. Note that the partial pivoting adds no flops since permuting 
rows involves no arithmetic. We will assume the worst-case scenario, in that none 
of the entries that come up are zeros. In counting flops, we refer to the algorithm 
above, rather than the MATLAB program. For $k = 1$, we will need to perform 
$n -1$ divisions to compute the multipliers $m_{il}$ , and for each of these multipliers, 
we will need to do a \texttt{rowcomb}, which will involve $n$ multiplications and $n$
additions/subtractions. (Note: Since the first column entry will be zero and need 
not be computed, we are only counting columns 2 through $n+1$ of the augmented 
matrix.) Thus, associated with the pivot $a_{11}$ , we will have to do $n-1$ divisions, 
$( n -1) n$ multiplications, and $( n -1 ) n$ additions/subtractions. Grouping the 
divisions and multiplications together, we see that at the $k = 1$ (first) iteration, we 
will need $(n-1) + (n-1)n = (n-1)(n + 1)$ multiplications/divisions and $(n-1 )n$ 
additions/subtractions. In the same fashion, the calculations associated with the 
pivot $a_{22}$ will involve $n - 2$ divisions plus $(n - 2)(n -1)$ multiplications which is 
$(n-2)n$ multiplications/divisions and $(n-2)(n-1 )$ additions/ subtractions. 
Continuing in this fashion, when we get to the pivot $a_{kk}$, we will need to do 
$(n - k)(n - k + 2)$ multiplications/divisions and $(n - k)(n - k + 1)$ additions/ 
subtractions. Summing from $k =1$ to $n - 1$ gives the following:
\\
\\
Total multiplications/divisions $\equiv M(n)= \sum_{k=1}^{n-1}(n-k)(n-k+2)$,
\\
\\
Total additions/subtractions $\equiv A(n)= \sum_{k=1}^{n-1}(n-k)(n-k+1)$.
\\
\\
Combining these two sums and regrouping gives:
\\
\\
Grand total flops $$\equiv F(n)= \sum_{k=1}^{n-1}(n-k)(2(n-k)+3)=2\sum_{k=1}^{n-1}(n-k)^2+3\sum_{k=1}^{n-1}.$$
\\
\\
If we reindex the last two sums, by substituting $j = n - k$, then as $k$ runs from 
1 through $n - 1$, so will $j$ (except in the reverse order), so that 
\\
\\
$$F(n)=2\sum_{j=1}^{n-1}j^2+3\sum_{j=1}^{n-1}j.$$
\\
\\
We now invoke the power sum identities (23) and (24) to evaluate the above two 
sums (replace n with $n - 1$ in the identities) and thus rewrite the flop count $F(n)$ 
as:
\\
\\
\begin{center}
$F(n)=\dfrac{1}{3}(n-1)n(2n-1)+\dfrac{3}{2}(n-1)n=\dfrac{2}{3}n^3+3
 +$lower power terms.
\end{center}
The "lower power terms" in the above flop counts can be explicitly computed 
(simply multiply out the polynomial on the left), but it is the highest order term 
that grows the fastest and thus is most important for roughly estimating flop 
counts. The flop count does not include the back substitution algorithm; but a 
similar analysis shows flop count for the back substitution to be just $n_2$
 (see the 
Exercise for the Reader 7.22), and we thus can summarize with the following 
result.
\\
\\
PROPOSITION 7.5: \textit{(Flop Counts for Gaussian Elimination)} In general, the 
number of flops needed to perform Gaussian elimination to solve a nonsingular 
system $Ax = b$ with an $n\times n$ coefficient matrix $A$ is
\begin{center}
$\dfrac{2}{3}n^3$+lower power terms.
\end{center}
EXERCISE FOR THE READER 7.22: Show that for the back substitution 
algorithm, the number of multiplications/divisions will be $(n^2
 + n)/2$, and the 
number of additions/subtractions will be $(n^2
 -n)/2$. Hence, the grand total flops 
required will be $n^2$.
\\

By using the natural algorithm for computing the inverse of a matrix, a similar 
analysis can be used to show the flop count for finding the inverse of a matrix of a 
nonsingular $n/times n$ matrix to be 
\begin{center}
$(8/3)n^3$+lower power terms,
\end{center}
or, in other words, essentially four times that for a single Gaussian elimination 
(see Exercise 16). Actually, it is possible to modify the algorithm (of Exercise 16)
to a more complicated one that can bring this flop count down to
$2n^3$+ lower power terms; but this is still going to be a more expensive and error prone method than Gaussian elimination, so we reiterate: For solving a single 
general linear system, Gaussian elimination is the best all-around method.
\\

The next example will give some hard evidence of the rather surprising fact that 
the computer time required (on MATLAB) to perform an addition/subtraction is 
about the same as that required to perform a multiplication/division.
\subsubsection{EXAMPLE 7.16:}
In this example, we perform a short experiment to record the 
time and flops required to add 100 pairs of random floating point numbers. We 
then do the related experiment involving the same number of divisions. 
\\
\\
\begin{lstlisting}[numbers=none,frame=none]
>>A=rand(100); B=rand(100);, 
>>tic, for i=1:100 , C=A+B; end, toc
\end{lstlisting}
$\rightarrow$ Elapsed time is 5.778000 seconds.
\\
\\
\begin{lstlisting}[numbers=none,frame=none]
>>tic, for i=1:100, C=A./B;, end, toc
\end{lstlisting}
$\rightarrow$  Elapsed time is 5.778000 seconds.
\\
\\
The times are roughly of the same magnitude and, indeed, the flop counts are 
identical and close to the actual number of mathematical operations performed. 
The reason for the discrepancy in the latter is that, as previously mentioned, a flop 
is "approximately" equal to one arithmetic operation on the computer; and this is 
the most useful way to think about a flop. 
\rule{485pt}{2pt}
\subsubsection{EXERCISES 7.5:}\noindent NOTE: As mentioned in the text, we take "Gaussian elimination" to mean Gaussian elimination with 
partial pivoting. 

\begin{enumerate}
\item Solve each of the following linear systems $Ax=b$ using three-digit chopped arithmetic and 
Gaussian elimination (i) without partial pivoting and then (ii) with partial pivoting. Finally redo 
the problems (iii) using MATLAB's left divide operator, and then (iv) using exact arithmetic (any 
method).
\\
\\
\begin{tasks}(2)
		\task $\displaystyle
A=\left[
\begin{array}{cc}
2 & 9    \\
1 & 7.5 \\
\end{array}
\right],
\displaystyle
b=\left[
\begin{array}{cc}
2     \\
-3 \\
\end{array}
\right]$
		\task $\displaystyle
A=\left[
\begin{array}{cc}
.99 & .98    \\
101 & 100 \\
\end{array}
\right],
\displaystyle
b=\left[
\begin{array}{cc}
1    \\
-1 \\
\end{array}
\right]
$
		\task $\displaystyle
A=\left[
\begin{array}{ccc}
2 & 3 &-1   \\
4 & 2 & -1 \\
-8 & 2 & 0 \\
\end{array}
\right],
\displaystyle
b=\left[
\begin{array}{c}
0 \\
21 \\
-4
\end{array}
\right]
$
	\end{tasks}
\item Parts (a) through (c): Repeat all parts of Exercise 1 using two-digit rounded arithmetic. 
\item For each square matrix specified, find the $LU$ factorization of the matrix (using Gaussian 
elimination). Do it first using (i) three-digit chopped arithmetic, then using (ii) exact arithmetic; 
and finally (iii) compare these with the results using MATLAB's built-in function \texttt{lu}.
\\ 
(a) The matrix $A$ in Exercise 1, part (a). 
\\
(b) The matrix $A$ in Exercise 1, part (b). 
\\
(c) The matrix $A$ in Exercise 1, part (c).
\item Parts (a) through (c): Repeat all parts of Exercise 3 using two-digit rounded arithmetic in (i).
\item Consider the following linear system involving the $3\times$ 3 Hubert matrix $H_3$ as the coefficient 
matrix:
$$\left\{\begin{array}{ccccccc}
x_1&+&\dfrac{1}{2}x_2&+&\dfrac{1}{3}x_3&=&2\\
\dfrac{1}{2}x_1&+&\dfrac{1}{3}x_2&+&\dfrac{1}{4}x_3&=&0\\
\dfrac{1}{2}x_1&+&\dfrac{1}{4}x_2&+&\dfrac{1}{5}x_3&=&-1
\end{array} \right.$$

(a) Solve the system using two-digit chopped arithmetic and Gaussian elimination without 
partial pivoting.
\\ 
(b) Solve the system using two-digit chopped arithmetic and Gaussian elimination. 
\\
(c) Solve the system using exact arithmetic (any method). 
\\
(d) Find the $LU$ decomposition of the coefficient matrix $H_3$ by using 2-digit chopped 
arithmetic and Gaussian elimination. 
\\
(e) Find the exact $LU$ decomposition of $H_3$ .
\item
(a) Find the $LU$ factorization of the matrix
$\displaystyle
A=\left[
\begin{array}{cc}
0 & 1    \\
1 & 0 \\
\end{array}
\right]$.
\\
(b) Is it possible to find a lower triangular matrix $L$ and an upper triangular matrix $U$ (not 
necessarily those in part (a)) such that $A - LU ?$  Explain why or why not. 
\item
Suppose that $M_1,M_2,\dots ,M_k$ are invertible matrices of the same size. Prove that their product is 
invertible with $(M_1\cdot M_2\dots M_k)^{-1}=M_k^{-1}\dots M_2^{-1}\cdot M_1^{-1}$.In words, "The inverse of the product is 
the reverse-order product of the inverses."
\item
(Storage and Computational Savings in Solving Tridiagonal Systems) Just as with any 
(nonsingular) matrix, we can apply Gaussian elimination to solve tridiagonal systems:
\\ 
\begin{equation}
\displaystyle
A=\left[
\begin{array}{ccccccc}
 d_1&a_1 & & & & & \\
 b_2&d_2 &a_2 & & &$\begin{huge}
 0
 \end{huge} $& \\
 &b_3 &d_3 &a_3 & & & \\
 & &b_4& d_4 &a_4 & \\
 & & &\ddots &\ddots & \ddots & \\
 &$ \begin{huge}
 0
 \end{huge}$& & &b_{n-1} &d_{n-1} &a_{n-1} \\
 & & & & &b_n &d_n \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{c}
x_1\\
x_2\\
x_3\\
x_4\\
\vdots \\
x_{n-1}\\
x_n\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{c}
r_1\\
r_2\\
r_3\\
r_4\\
\vdots \\
r_{n-1}\\
r_n\\
\end{array}
\right].
\end{equation}
Here, $d$'s stand for diagonal entries, $b$'s for below-diagonal entries, $a$'s for above-diagonal entries, 
and $r$'s for right-side entries. We can greatly cut down on storage and unnecessary mathematical 
operations with zero by making use of the special sparse form of the tridiagonal matrix. The main 
observation is that at each step of the Gaussian elimination process, we will always be left with a 
banded matrix with perhaps one additional band above the $a$'s diagonal. (Think about it, and 
convince yourself. The only way a switch can be done in selecting a pivot is with the row 
immediately below the diagonal pivot entry.) Thus, we may wish to organize a special algorithm 
that deals only with the tridiagonal entries of the coefficient matrix. 
(a) Show that the Gaussian elimination algorithm, with unnecessary operations involving zeros 
being omitted, will require no more than $8(n-1)$ flops (multiplications, divisions, additions, 
subtractions), and the corresponding back substitution will require no more than $5n$ flops. Thus 
the total number of flops for solving such a system can be reduced to less than $13n$. 
(b) Write a program, $\texttt{x = tridiaggauss (d,b,a,r)}$that inputs the diagonal vector d (of 
length n) and the above and below diagonal vectors $a$ and $b$ (of length $n -1$ ) of a nonsingular 
tridiagonal matrix, the column vector $r$ and will solve the tridiagonal system (31) using the 
Gaussian elimination algorithm but which overwrites only the four relevant diagonal vectors (described above; you need to create an $n - 2$ length vector for the extra diagonal) and the vector 
$r$ rather than on the whole matrix. The output should be the solution column vector $x$. 
(c) Test out your algorithm on the system (31) with $n = 2, n = 100, n - 500, and n = 1000$ using 
the following data in the matrices $d_i=4,a_i= 1,b_i=1, r = [1 -1 1 -1 \dots]$ and compare results 
and flop counts with MATLAB's left divide. You should see that your algorithm is much more 
efficient. 
\\
\textbf{Note:} The upper bound $13n$ on flops indicated in part (a) is somewhat liberal; a more careful 
analysis will show that the coefficient 13 can actually be made a bit smaller (How small can you 
make it?) But even so, the savings on flops (not to mention storage) are incredible. If we 
compare $13n$ with the bound $2n^3/3$, for large values of w, we will see that this modified method 
will allow us to solve extremely large tridiagonal systems that previously would have been out of 
the question. For example, when $n = 10,000$, this modified method would require storage of 
$2n + 2(n-l) = 39,998$ entries and less than $13n = 130,000$ flops (this would take a few 
seconds on MATLAB even on a weak computer); whereas the ordinary Gaussian elimination 
would require the storage of $n^2
 +n = 100,010,000$ entries and approximately $2n^3/3=6.66\dots \times 10^{11}$ flops, an unmanageable task!
\item
\textit{(The Thomas Method, an Even Faster Way to Solve Tridiagonal Systems)} By making a few extra 
assumptions that are usually satisfied in most tridiagonal systems that arise in applications, it is 
possible to slightly modify the usual Gaussian elimination algorithm to solve the triadiagonal 
system (31) in just $8n - 7$ flops (compared to the upper bound $13n$ of the last problem). The 
algorithm, known as the \textbf{Thomas method,}\footnote{
 The method is named after the renowned physicist Llewellyn H. Thomas; but it was actually 
discovered independently by several different individuals working in mathematics and related 
disciplines. W.F. Ames writes in his book [Ame-77] (p. 52): "The method we describe was discovered 
independently by many and has been called the Thomas algorithm. Its general description first 
appeared in widely distributed published form in an article by Bruce et al. [BPRR-53]." }
 differs from the usual Gaussian elimation by scaling 
the diagonal entry to equal I at each pivot, and by not doing any row changes (i.e., we forgo 
partial pivoting, or assume the matrix is of a form that makes it unnecessary; see Exercise 10). 
This will mean that we will have to keep track only of the above-diagonal entries (the $a$'s vector) 
and the right-side vector $r$. The Thomas method algorithm thus proceeds as follows:
\\
\\
\textit{Step 1:} (Results from \texttt{rowmult}$(A,1,1/d_1 )): a_1 = a_1/d_1,r_1=r_1/d_1$. 
\\
(We could also add $d_1-1$, but since the diagonal entries will always be scaled to equal one, we 
do not need to explicitly record this change.) 
\\
\\
\textit{Steps k=2 through n-1:}(Results from rowcomb$(A,k-1,k,-b_k,)$ and then \texttt{rowscale}$(A,k,1/(d_k-b_k a_{k-1})))$: 
\\
$$ a_k=a_k/(d_k-b_k a_{k-1}),r_k=(r_k-b_k r_{k-1})/(d_k-b_k a_{n-1})$$
\\
\textit{Step n:} (Results from same procedure as in steps 2 through $n-1$, but there is no $a_n$ ): 
\\
$$r_n=(r_n-b_n r_{n-1})/(d_n-b_n a_{n-1}).$$
\\
This variation of Gaussian elimination has transformed the tridiagonal system into an upper 
triangular system with the following special form:
\\
$$\displaystyle
A=\left[
\begin{array}{ccccc}
 1& a_1& & & \\
 & 1&a_2 &$\begin{huge}
 0
 \end{huge}$ & \\
 & & \ddots &\ddots & \\
 &$\begin{huge}
 0
 \end{huge}$ & &1 &a_{n-1} \\
 & & & &1 \\
\end{array}
\right]
\displaystyle
\left[
\begin{array}{c}
x_1\\
x_2\\
\vdots \\
x_{n-1}\\
x_n\\
\end{array}
\right]
=
\displaystyle
\left[
\begin{array}{c}
r_1\\
r_2\\
\vdots \\
r_{n-1}\\
r_n\\
\end{array}
\right].
$$
\\
for which the back substitution algorithm takes on the particularly simple form: $x_n-r_n$; then for 
$k=n-1,n-2,\dots ,2,1:x_k=r_k-a_kx_{k+1}$.
\\
(a) Write a MATLAB M-file, \texttt{x=thomas (d,b,a,r)} that performs the Thomas method as 
described above to solve the tridiagonal system (31). The inputs should be the diagonal vector $d$
(of length $n$) and the above and below diagonal vectors $a$ and $b$ (of length $n -1$) of a nonsingular 
tridiagonal matrix, and the column vector $r$. The output should be the computed solution, as a 
column vector $x$. Write your program so that it overwrites only the vectors $a$ and $r$.
\\ 
(b) Test out your program on the systems of part (c) of Exercise 8, and compare results and flop 
counts with those for MATLAB's left divide solver. If you have done part (c) of Exercise 8, 
compare also with the results from the program of the previous exercise. 
\\
(c) Do a flop count on the Thomas method to show that the total number of flops needed is 
$8n-7$.
\\

\textbf{NOTE:}
Looking over the Thomas method, we see that it assumes that $d_1\neq 0$, and $d_k\neq b_k a_{k-1}$ (for k 
= 2 through $n$). One might think that to play it safe, it may be better to just use the slightly more 
expensive modification of Gaussian elimination described in the previous exercise, rather than risk 
running into problems with the Thomas method. For at most all applications, it turns out that the 
requirements for the Thomas method indeed are satisified. Such triadiagonal systems come up 
naturally in many applications, in particular in finite difference schemes for solving differential 
equations. One safe approach would be to simply build in a deferral to the previous algorithm in cases 
where the Thomas algorithm runs into a snag. 
\item
We say that a square matrix $A=[a_{ij}]$ \textbf{is strictly diagonally dominant (by columns)} if for each 
index $k, 1\leq k\leq\neq n$, the following condition is met:
\begin{equation}
\vert a_{kk}\vert > \sum_{j=1\atop j\neq k}^n\vert a_{kj}\vert.
\end{equation}
\\
This condition merely states that each diagonal entry is larger, in absolute value, than the sum of 
the absolute values of all of the other entries in its column.
\\
(a) Explain why when Gaussian elimination is applied to solve a linear system $Ax=b$ whose 
coefficient matrix is strictly diagonally dominant by columns, then no row changes will be 
required. 
\\
(b) Explain why the $LU$ factorization of a diagonally dominant by columns matrix $A$ will not 
have any permutation matrix.
\\ 
(c) Explain why the requirements for the Thomas method (Exercise 9) will always be met if the 
coefficient matrix is strictly diagonally dominant by columns.
\\
(d) Which, if any, of the above facts will continue to remain true if the strict diagonal dominance 
condition (32) is weakened to the following? 
\\
$$
\vert a_{kk}\vert > \sum_{j=k+1}^n\vert a_{kj}\vert.
$$
\\
(That is, we are now only assuming that each diagonal entry is larger, in absolute value, than the 
sum of the absolute values of the entries that lie in the same column but below it.)
\\
\item 
Discuss what conditions on the industries must hold in order for the technology matrix $M$ of the 
Leontief input/output model of Exercise 10 from Section 7.4 to be diagonally dominant by 
columns (see the preceding exercise).
\\
\item
\textit{(Determinants Revisited: Effects of Elementary Row/Column Operations on Determinants)} 
Prove the following facts about determinants, some of which were previewed in Exercise 10 of 
Section 7.1.
\\ 
(a) If the matrix $B$ is obtained from the square matrix $A$ by multiplying one of the latter's rows by 
a number $c$ (and leaving all other rows the same, i.e., \texttt{B=rowmult (A,i,c))} , then $det(B)= 
cdet(A)$. 
\\
(b) If the matrix $B$ is obtained from the square matrix $A$ by adding a multiple of the ith row of $A$ to the $j$th row $(i \neg j )$ (i.e., \texttt{B=rowcomb (A,i,j,c))}, then det($B$) = det($A$). 
\\
(c) If the matrix $B$ results from the matrix $A$ by switching two rows of the latter (i.e., 
\texttt{B=rowswitch (A,i,j))}, then det(	B) = -det(A). 
\\
(d) If two rows of a square matrix are the same, then det($A$) = 0.
\\ 
(e) If $B$ is the transpose of $A$, then det($B$) = det($A$).
\\
\textbf{Note:} In light of the result of part (e), each of the statements in the other parts regarding the 
effect of a row operation on a determinant has a valid counterpart for the effect of the 
corresponding column operation on the determinant.
\\ 
\textbf{Suggestions:}
You should make use of identity (20) $det(a) = det(,4)det(5)$, as well as 
Proposition 7.3 and Theorem 7.4. The results of (a), (b), and (c) can then be proved by 
calculating determinants of certain elementary matrices. The only difficult thing is for part (c) to 
show that the determinant of a permutation matrix gotten from the identity matrix by switching 
two rows equals -1 . One way this can be done is by an appropriate (but not at all obvious) matrix 
factorization. Here is one way to do it for the (only) $2\times 2$ permutation matrix:
$$
\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]=\left[\begin{array}{cc}
1 & -1 \\
0 & 1
\end{array}\right]\left[\begin{array}{ll}
1 & 0 \\
1 & 1
\end{array}\right]\left[\begin{array}{cc}
1 & 0 \\
0 & -1
\end{array}\right]\left[\begin{array}{ll}
1 & 1 \\
0 & 1
\end{array}\right].
$$
(Check this!) All of the matrix factors on the right are triangular so the determinants of each are 
easily computed by multiplying diagonal entries (Proposition 7.3), so using (20), we get
$$
\operatorname{det}\left(\left[\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right]\right)=1 \cdot 1 \cdot(-1) \cdot 1=-1.
$$
In general, this argument can be made to work for any permutation matrix (obtained by switching 
two rows of the identity matrix), by carefully generalizing the factorization. For example, here is 
how the factorization would generalize for a certain $3\times 3$ permutation matrix: 
$$
\left[\begin{array}{lll}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{array}\right]=\left[\begin{array}{ccc}
1 & 0 & -1 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
1 & 0 & 1
\end{array}\right]\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & -1
\end{array}\right]\left[\begin{array}{lll}
1 & 0 & 1 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{array}\right].
$$
Part (d) can be proved easily from part (c); for part (e) use mathematical induction and cofactor 
expansion.
\\
\item (Determinants Revisited: A Better Way to Compute Them) The Gaussian elimination algorithm provides us with an efficient way to compute determinants. Previously, the only method we gave to compute them was by cofactor expansion, which we introduced in Chapter 4 . But we saw that this was an extremely expensive way to compute determinants. A new idea is to use the Gaussian climination algorithm to transform a square matrix $A$ into an upper triangular matrix. From the previous exercise, each time a \texttt{rowcomb} is done, there will be no effect on the determinant, but each time a \texttt{rowswitch} is done, the determinant is negated. By Proposition 7.3, the determinant of the diagonal matrix is just the product of the diagonal entries. Of course, in the Gaussian elimination algorithm, the column vector $b$ can be removed (if all we are interested in is the determinant). Also, if a singularity is detected, the algorithm should exit and assign $\operatorname{det}(A)=0$.\\
(a) Create a function $M$-file, called \texttt{y= gaussdet }$(A)$, that inputs a square matrix $M$ and outputs the determinant using this algorithm.\\
(b) Test your program out by computing the determinants of matrices with random integer entries from $-9$ to 9 of sizes $3 \times 3,8 \times 8,20 \times 20$, and $80 \times 80$ (you need not print the last two matrices) that you can construct using the M-file randint of Exercise for the Reader 7.2. Compare the results, computing times and (if you have Version 5) flop counts with those for MATLAB's builtin det function applied to the same matrices.\\
(c) Go through an analysis similar to that done at the end of the section to prove a result similar to that of Proposition $7.5$ that will give an estimate of the total flop counts for this algorithm, with the highest-order term being accurate.\\
(d) Obtain a similar flop count for the cofactor expansion method and compare with the answer you got in (c). (The highest-order term will involve factorials rather than powers.)\\
(e) Use your answer in (c) to obtain a flop count for the amount of flops needed to apply Cramer's rule to solve a nonsingular linear system Ax = b with A being an n x n nonsingular 
matrix. 
\item (a) Prove Proposition 7.3.\\
(b) Prove an analogous formula for the determinant of a square matrix that is upper-left triangular in the sense that all entries above the off-main diagonal are zeros. More precisely, prove that any matrix of the following form,
$$
A=\left[\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 n} \\
a_{21} & a_{22} & a_{23} & & \\
\vdots & & \diagup &  & \\
a_{n-1,1} & a_{n-1,2} & & ${\huge 0}$ & \\
a_{n 1} & & & &
\end{array}\right] \text {, }
$$
has determinant given by $\operatorname{det}(A)=(-1)^{k} a_{1 n} \cdot a_{2, n-1} \cdot a_{3, n-2} \cdots a_{n-1,2} \cdot a_{n 1}$
where $n=2 k+i(i=0,1)$.
\\
\textbf{Suggestion:} Proceed by induction on $n$, where $A$ is an $n \times n$ matrix. Use cofactor expansion along an appropriate row (or column).
\item
(a) Write a function $M$-file, call it \texttt{[L, U, P]= mylu (A)}, that will compute the $LU$ factorization of an inputted nonsingular matrix $A$.\\
(b) Apply this function to each of the three coefficient matrices in Exercise 1 as well as the Hilbert matrix $\mathrm{H}_{3}$, and compare the results (and flop counts) to those with MATLAB's built-in function \texttt{lu}\. From these comparisons, does your program seem to be as efficient as MATLAB's?
\item
(a) Write a function M-file, call it \texttt{B=myinv(A)}, that will compute the inverse of an inputted nonsingular matrix $A$, and otherwise will output the error message: "Matrix detected as numerically singular." Your algorithm should be based on the following fact (which follows from the way that matrix multiplication works). To find an inverse of an $n \times n$ nonsingular matrix $A$, it is sufficient to solve the following $n$ linear equations:
$$
A x^{1}=\left[\begin{array}{c}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right], A x^{2}=\left[\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], A x^{3}=\left[\begin{array}{c}
0 \\
0 \\
1 \\
\vdots \\
0
\end{array}\right], \cdots A x^{n}=\left[\begin{array}{c}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right],
$$
where the column vectors on the right sides of these equations are precisely the columns of the $n \times n$ identity matrix. It then would follow that 
$\displaystyle
A=\left[
\begin{array}{c:c:c:c:c}
x^{1}&x^{2}& x^{3}&\cdots& x^{n}\\
\end{array}
\right]=I$, so that the desired inverse of $A$ is the matrix 
$\displaystyle
A_{-1}=\left[
\begin{array}{c:c:c:c:c}
x^{1}&x^{2}& x^{3}&\cdots& x^{n}\\
\end{array}
\right]$. 
Your algorithm should be based on the $L U$ decomposition, so it gets computed once, rather than doing a complete Gaussian elimination for each of the $n$ equations.\\
(b) Apply this function to each of the three coefficient matrices in Exercise 1 as well as the Hilbert matrix $\mathrm{H}_{4}$, and compare the results \texttt{inv}. From these comparisons, does your program seem to be as efficient as MATLAB's?\\
(c) Do a flop count similar to the one done for Proposition $(and flop counts) to those with MATLAB's built-in function 7.5$ for this algorithm.
Note: For part (a), feel free to use MATLAB's built-in function $1 u$; see the comments in the text about how to use the $L U$ factorization to solve linear systems.
\end{enumerate}


\section{VECTOR AND MATRIX NORMS, ERROR ANALYSIS, AND EIGENDATA}
In the last section we introduced the Gaussian elimination (with partial pivoting) 
algorithm for solving a nonsingular linear system
\begin{equation}
Ax=b
\end{equation}
where $A=\left[a_{i j}\right]$ is an $n \times n$ coefficient matrix, $b$ is an $n \times 1$ column vector, and $x$ is the $n \times 1$ column vector of variables whose solution is sought. This algorithm is the best all-around general numerical method for solving the linear system (33), but its performance can vary depending on the coefficient matrix $A$. In this section we will present some practical estimates for the error of the computed solution that will allow us to put some quality control guarantee on the answers that we obtain from (numerical) Gaussian elimination. We need to begin with a practical way to measure the "sizes" of vectors and matrices. We have already used the Euclidean length of a vector $v$ to measure its size, and norms will be a generalization of this concept. We will introduce norms for vectors and matrices in this section, as well as the so-called condition numbers for square matrices. Shortly we will use norms and condition numbers to give precise estimates for the error of the computed solution of (33) (using Gaussian elimination). We will also explain some ideas to try when a system to be solved is poorly conditioned. The theory on modified algorithms that can deal with poorly conditioned systems contains an assortment of algorithms that can perform well if the (poorly conditioned) matrix takes on a special form. If one has the Student Version of MATLAB (or has the Symbolic Toolbox) there is always the option of working in exact arithmetic or with a fixed but greater number of significant digits. The main (and only) disadvantage of working in such arithmetic is that computations move a lot slower, so we will present some concrete criteria that will help us to decide when such a route might be needed. The whole subject of error analysis and refinements for numerically solving linear systems is quite vast and we will not be delving too deeply into it. For more details and additional results, the interested reader is advised to consult one of the following references (listed in order of increasing mathematical sophistication): [Atk-89], [Ort-90], [GoVL-83].
\\

The Euclidean "length" of an $n$-dimensional vector $X =$ [$x_1$  $X_2$ $\dots$ $x_n$] is defined 
by:
\begin{equation} \
\operatorname{len}(x)=\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}.
\end{equation}
For this definition it is immaterial whether $x$ is a row or column vector. For example, if we are working in two dimensions and if the vector is drawn in the $x y-$ plane from its tail at $(x, y)=(0,0)$ to its tip $(x, y)=\left(x_{1}, x_{2}\right)$, then len $(x)$ is (in most cases) the hypotenuse of a right triangle with legs having length $\left|x_{1}\right|$ and $\left|x_{2}\right|$, and so the formula (34) becomes the Pythagorean theorem. In the remaining cases where one of $x_{1}$ or $x_{2}$ is zero, then len $(x)$ is simply the absolute value of the other coordinate (in this case also the length of the vector $x$ that will lie on either the $x$-or $y$-axis.) From what we know about plane geometry, we can deduce that len $(x)$ has the following properties:
\begin{subequations}
\begin{equation}
\text { len }(x) \geq 0 \text {, and len }(x)=0 \text { if and only if } x=0 \text { (vector), } 
\end{equation}
\begin{equation}
\text { len }(c x)=|c| \text { len }(x) \text { for any scalar } c, 
\end{equation}
\begin{equation}
\text { len }(x+y) \leq \operatorname{len}(x)+\operatorname{len}(y) \text { (Triangle Inequality). }
\end{equation}
\end{subequations}
\\
Property (35A) is clear (even in n dimensions). Property (35B) corresponds to the 
geometric fact that when a vector is multiplied by a scalar, the length gets 
multiplied by the absolute value of the scalar (we learned this early in the chapter). 
The triangle inequality (35C) corresponds to the geometric fact (in two 
dimensions) that the length of any side of any triangle can never exceed the sum of 
the lengths of the other two sides. These properties remain true for general n-dimensional vectors (see Exercise 11 for a more general result).
\\
\\
A vector norm for $n$-dimensional (row or column) vectors $x=\left[\begin{array}{llll}x_{1} & x_{2} & \cdots & x_{n}\end{array}\right]$ is a way to associate a nonnegative number (default notation: $\|x\|$ ) with the vector $x$ such that the following three properties hold:
\begin{subequations}
\begin{equation}
\|x\| \geq 0,\|x\|=0 \text { if and only if } x=0 \text { (vector), }
\end{equation}
\begin{equation}
\|c x\|=|c|\|x\| \text { for any scalar } c 
\end{equation}
\begin{equation}
\|x+y\| \leq\|x\|+\|y\| \text { (Triangle Inequality) }
\end{equation}
\end{subequations}

We have merely transcribed the properties (35) to obtain these three axioms (36) 
for a norm. It turns out that there is an assortment of useful norms, the 
aforementioned Euclidean norm being one of them. The one we will use most in 
this section is the so-called \textbf{max norm} (also known as the \textbf{infinity norm}) and this 
is defined as follows:

\begin{equation}
\|x\|=\|x\|_{\infty}=\max \left\{\left|x_{i}\right|, 1 \leq i \leq n\right\}=\max \left\{\left|x_{1}\right|,\left|x_{2}\right|, \cdots,\left|x_{n}\right|\right\}.
\end{equation}

The proper mathematical notation for this vector norm is $\|x\|_{\infty}$, but since it will be our default vector norm we will often denote it by $\|x\|$ for convenience. The max norm is the simplest of all vector norms, so working with it will allow the complicated general concepts from error analysis to be understood in the simplest possible setting. The price paid for this simplicity will be that some of the resulting error estimates that we obtain using the max norm may be somewhat more liberal than those obtained with other, more complicated norms. Both the max and Euclidean norms are easy to compute on MATLAB (e.g., for the max norm of $x$ we could simply type max \texttt{(abs(x))}, but (of course) MATLAB has built-in functions for both of these vector norms and many others.
$$
\begin{array}{|l|l|l|}
\hline \text { norm }(x) \rightarrow & \text { Computes the length norm len }(x) \text { of a (row or column) vector } x . \\
\hline
\text { norm }(x, \text { inf }) \rightarrow &\quad \text { Computes the max norm } \mid x \mid \text { of a (row or column) vector } x .\\
\hline
\end{array}
$$
EXAMPLE 7.17: For the two four-dimensional vectors $x=[1,0,-4,6]$ and $y=[3,-4,1,-3]$ find the following:\\
(a) $\operatorname{len}(x), \operatorname{len}(y), \operatorname{len}(x+y)$\\
(b) $\|x\|,\|y\|,\|x+y\|$
\\
\\
SOLUTION: First we do these computations by hand, and then redo them using MATLAB.\\
Part (a): Using (34) and since $x=[4,-4,-3,3]$ we get that $\operatorname{len}(x)=\sqrt{1^{2}+0^{2}+(-4)^{2}+6^{2}}=\sqrt{53}=7.2801 \ldots, \operatorname{len}(y)=\sqrt{3^{2}+(-4)^{2}+1^{2}+(-3)^{2}}$ $=\sqrt{35}=5.9160 \ldots$, and len $(x+y)=\sqrt{4^{2}+(-4)^{2}+(-3)^{2}+3^{2}}=\sqrt{50}=7.0710 \ldots$\\
Part (b): Using (37), we compute: $\|x\|=\max \{|1|,|0|,|-4|,|6|\}=6,\|y\|=$ $\max \{|3|,|-4|,|1|,|-3|\}=4$, and $\|x+y\|=\max \{|4|,|-4|,|-3|,|3|\}=4$.
\\

These computations give experimental evidence of the validity of the triangle 
inequality in this special case. We now repeat these same computations using 
MATLAB:
\\
\begin{lstlisting}[numbers=none,frame=none]
>>x=[l 0 -4 6] ; y=[3 -4 1 -3] ; 
>>norm(x), norm(y), norm(x+y)		->ans = 7.2801 5.9161 7.0711 
>>norm(x, inf) , norm(y,inf) , norm (x+y, inf) 		->ans = 6 4 4
\end{lstlisting}
EXERCISE FOR THE READER 7.23: Show that the max norm as defined by 
(37) is indeed a vector norm by verifying the three vector norm axioms of (36).
\\
\begin{figure}[H]
\begin{wrapfigure}[60]{l}{0.25\textwidth}
\includegraphics[width=0.25\textwidth]{img_1}\end{wrapfigure}
Given any vector norm, we define an \textbf{associated matrix 
norm} by the following:
\\
\begin{equation}
\|A\| \equiv \max \left\{\frac{\|A x\|}{\|x\|}, x \neq 0 \text { (vector) }\right\}.
\end{equation}
For any nonzero vector $x$, its norm $\|x\|$ will be a positive number (by $(36 \mathrm{~A})$ ); the transformed vector $A x$ will be another vector and so will have a norm $\|A x\|$. The norm of the matrix $A$ can be thought of as the maximum magnification factor by which the transformed vector $A x^{\prime}$ s norm will have changed from the original vector $x$ 's norm; see Figure 7.35.
\vspace{70pt}
\caption{Graphic for the matrix norm definition (38). The matrix $A$ will transform $x$ 
into another vector Ax (of same dimension if $A$ is square). The norm of $A$ is the maximum 
magnification that the transformed vector $Ax$ norm will have in terms of the norm of $x$.}
\end{figure}
It is interesting that matrix norms, despite the daunting definition (38), are often easily computed from other formulas. For the max vector norm, it can be shown that the corresponding matrix norm $(38)$, often called the \textbf{infinity matrix norm}, is given by the following formula:
\\
\begin{equation}
\|A\|=\max _{1 \leq i \leq n}\left\{\sum_{j=1}^{n}\left|a_{i j}\right|\right\}=\max _{1 \leq i \leq n}\left\{\left|a_{i 1}\right|+\left|a_{i 2}\right|+\cdots+\left|a_{i n}\right|\right\}.
\end{equation}
This more practical definition is simple to compute: We take the sum of each of the absolute values of the entries in each row of the matrix $A$, and $\|A\|$ will equal the maximum of these "row sums." MATLAB has a command that will do this computation for us:
$$
\begin{array}{|l|l|l|}
\hline 
\texttt{norm(A,inf) ->}
&
\text {Computes the infinity norm $\Vert A\Vert$ of a matrix A.}\\
\hline
\end{array}
$$
One simple but very important consequence of the definition (38) is the following inequality:
\begin{equation}
\|A x\| \leq\|A\|\|x\| \text { (for any matrix } A \text { and vector } x \text { of compatible size). }
\end{equation}
To see why $(40)$ is true is easy: First if $x$ is the zero vector, then so is $A x$ and so both sides of $(40)$ are equal to zero. If $x$ is not the zero vector then by (38) we have $\|A x\| /\|x\| \leq\|A\|$, so we can multiply both sides of this inequality by the positive number $\|x\|$ to produce (40).\\
EXAMPLE 7.18: Let $A=\left[\begin{array}{ccc}1 & 2 & -1 \\ 0 & 3 & -1 \\ 5 & -1 & 1\end{array}\right]$ and $x=\left[\begin{array}{c}1 \\ 0 \\ -2\end{array}\right] .$ Compute $\|x\|$, $\|A x\|$, and $\|A\|$ and check the validity of $(40)$.
\\
SOLUTION: Since $A x=\left[\begin{array}{l}3 \\ 2 \\ 3\end{array}\right]$, we obtain: $\|x\|=2,\|A x\|=3$, and using (39),
$
\|A\|=\max \{1+2+|-1|, 0+3+|-1|, 5+|-1|+1\}=\max \{4,4,7\}=7 . \quad \text { Certainly }
\|A x\| \leq\|A\|\|x\| \text { holds here }(3 \leq 7 \cdot 2) \text {. }
$
\\
\\
EXERCISE FOR THE READER 7.24: Prove the following two facts about matrix norms: For two $n \times n$ matrices $A$ and $B$ :\\
(a) $\|A B\| \leq\|A\|\|B\|$.\\
(b) If $A$ is nonsingular, then
$\left\|A^{-1}\right\|=\left(\min _{x \neq 0} \frac{\|A x\|^{-1}}{\|x\|}\right)^{-1}.$\\

With matrix norms introduced, we are now in a position to define the condition number of a nonsingular (square) matrix. For such a matrix $A$, the condition number of $A$, denoted by $\kappa(A)$, is the product of the norm of $A$ and the norm of $A^{-1}$, i.e.,\\
\begin{equation}
\kappa(A)=\text { condition number of } A \equiv\|A\| \|A^{-1} \| .
\end{equation}
\\
By convention, for a singular matrix $A$, we define $\kappa(A)=\infty ,\footnote{Sometimes this condition number is denoted $\kappa_{\infty}(A)$ to emphasize that it derives from the infinity vector and matrix norm. Since this will be the only condition number that we use, no ambiguity should arise by our adopting this abbreviated notation.
}$ Unlike the determinant, a large condition number is a reliable indicator that a square matrix is \textbf{nearly singular} (or \textbf{poorly conditioned}); and condition numbers will be a cornerstone in many of the error estimates for linear systems that we give later in this section. Of course, the condition number depends on the vector norm that is being used (which determines the matrix norm), but unless explicitly stated otherwise, we will always use the infinity vector norm (and the associated matrix norm and condition numbers). To compute the condition number directly is an expensive computation in general, since it involves computing the inverse $A^{-1}$. There are good algorithms to estimate condition numbers relatively quickly to any degree of accuracy. We will forgo presenting such algorithms, but will take the liberty of using the following MATLAB built-in function for computing condition numbers:
\\
$$
\begin{array}{|l|l|}
\hline 
\texttt{cond (A, inf) ->}
&
\text {Computes and outputs the condition number (with respect to 
}\\
&
\text {the infinity vector norm) of the square matrix A.}\\
\hline
\end{array}
$$
\\
The condition number has the following general properties (actually valid for condition numbers arising from any vector norm):
\\
\begin{equation}
\kappa(A) \geq 1, \text { for any square matrix } A \text {. }
\end{equation}
\\
If $D$ is a diagonal matrix with nonzero diagonal entries: $d_{1}, d_{2}, \cdots, d_{n}$, then
\\
\begin{equation}
\kappa(D)=\frac{\max \left\{\left|d_{i}\right|\right\}}{\min \left\{\left|d_{i}\right|\right\}}.
\end{equation}
\\
If A is a square matrix and c a nonzero scalar, then
\\
\begin{equation}
\text { then } \kappa(c A)=\kappa(A) \text {. }
\end{equation}
\begin{figure}[H]
\begin{wrapfigure}[60]{r}{0.4\textwidth}
\includegraphics[width=0.4\textwidth]{img_2}\caption{Heuristic diagram 
showing the distance from a nonsingular 
matrix A to the set of all singular matrices 
(line). }\end{wrapfigure}


In particular, from (43) it follows that $\kappa(I)=1$. The proofs of these identities will be left to the exercises. Before giving our error analysis results (for linear systems), we state here a theorem that shows, quite quantitatively, that nonsingular matrices with large condition numbers are truly very close to being singular. Recall that the singular square matrices are precisely those whose determinant is zero. For a given $n \times n$ nonsingular matrix $A$, we think of the distance from $A$ to the set of all $n \times n$ singular matrices to be $\min \{\|S-A\|: \operatorname{det}(S)=0\}$. (Just as with absolute values, the norm of a difference of matrices is taken to be the distance between the matrices.) We point out that $\min _{\operatorname{det}(S)=0}\|S-A\|$ can be thought of as the distance from $A$ to the set of singular matrices. (See Figure 7.36.)
\end{figure}
\vspace{85pt}
THEOREM 7.6: (\textit{Geometric Characterization of Condition Numbers}) If $A$ is any $n \times n$ nonsingular matrix, then we have:
\\
\begin{equation}
\frac{1}{\kappa(A)}=\frac{1}{\|A\|} \cdot \min _{\operatorname{de}(S)=0}\|S-A\|=\frac{1}{\|A\|} .
\end{equation}
\\

Like all of the results we state involving matrix norms and condition numbers, this one is true, in general, for whichever matrix norm (and resulting condition number) we would like to use. A proof can be found in the paper [Kah-66]. This theorem suggests some of the difficulties in trying to numerically solve systems having large condition numbers. Gaussian elimination involves many computations and each time we modify our matrix, because of roundoff errors, we are actually dealing with matrices that are close to but not the same as the actual (mathematically exact) matrices. The theorem shows that for poorly conditioned matrices (i.e., ones with large condition numbers), this process is extremely sensitive since even a small change in a poorly conditioned matrix could result in one that is singular!
\\

We close with an example that will review some of the concepts about norms and condition numbers that have been introduced.
\\
\\
 EXAMPLE 7.19: Consider the matrix:  $A=\left[\begin{array}{cc}
7 & -4 \\
-5 & 3
\end{array}\right]$.
\\
(a) Is there a $(2 \times 1)$ vector $x$ such that: $\|A x\|>8\|x\|$ ? If yes, find one; otherwise explain why one does not exist.\\
(b) Is there a nonzero vector $x$ such that $\|A x\| \geq 12\|x\|$ ? If so, find one; otherwise explain why one does not exist.\\
(c) Is there a singular matrix $S=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ (i.e., $a d-b c=0$ ) such that $\|S-A\| \leq 0.2$ ? If so, find one; otherwise explain why one does not exist.\\
(d) Is there a singular matrix $S=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ (i.e., $a d-b c=0$ ) such that $\|S-A\| \leq 0.05 ?$ If so, find one; otherwise explain why one does not exist.
\\
\\
SOLUTION: Parts (a) and (b): Since $\|A\|=7+4=11$, it follows from (38) that there exist (nonzero) vectors $x$ with $\|A x\| /\|x\|=11$ or, put differently (multiply by $\|x\| ) \|A x\|=11\|x\|$, but there will not be any nonzero vectors $x$ that will make this equation work if 11 gets replaced by any larger number. (The maximum amount that matrix multiplication by $A$ can magnify the norm of any nonzero vector $x$ is 11 times.) Thus part (a) will have a vector solution but part (b) will not. To find an explicit vector $x$ that will solve part (a), we will actually do more and find one that undergoes the maximum possible magnification $\|A x\|=11\|x\|$. The procedure is quite simple (and general). The vector $x$ will have entries being either 1 or $-1$. To find such an appropriate vector $x$, we simply identify the row of $A$ that gives rise to its norm being 11; this would be the first row (in general if more than one row gives the norm, we can choose either one). We simply choose the signs of the $x$-entries so that when they are multiplied in order by the corresponding entries in the just-identified row of $A$, all products are positive. In other words, if an entry in the special row of $A$ is positive, take the corresponding component of $x$ to be 1 ; if the special row entry of $A$ is negative, take the corresponding component of $x$ to be $-1$. In our case the special row of $A$ is (the first row) $[7-4]$, and so in accordance we take $x=[1-1]$ '. The first entry of the vector $A x$ is $\left[\begin{array}{ll}7 & -4\end{array}\right] \cdot[1-1]^{\prime}=7(1)-4(-1)=7+4=11$, so $\|x\|=1$ and $\|A x\|=11$ (actually, this shows only $\|A x\| \geq 11$ since we have not yet computed the other component of $A x$, but from what was already said, we know $\|A x\| \leq 11$, so that indeed $\|A x\|=11)$. This procedure easily extends to any matrices of any size.
\\
\\
Parts (c) and (d): We rewrite equation (45) to isolate the distance from $A$ to the singular matrices (simply multiply both sides by $\|A\|$ ):
\\
$$
\min _{\operatorname{det}(S)=0}\|S-A\|=\frac{\|A\|}{\kappa(A)}
$$
\\
Appealing to MATLAB to compute the right side (and hence the distance from $A$ to singulars):
\\
\begin{lstlisting}[frame=none,numbers=none]
>>A=[7  -4 ;-5  3]
>>norm(A,inf)cond(A,inf)
->ans =         0.0833
\end{lstlisting}
Since this distance is less than $0.2$, the theorem tells us that there is a singular matrix satisfying the requirement of part (c) (the theorem unfortunately does not help us to find one), but there is no singular matrix satisfying the more stringent requirements of part (d). We use \textit{ad hoc} methods to find a specific matrix $S$ that satisfies the requirements of part (c). Note that $\operatorname{det} A=7 \cdot 3-(-5) \cdot(-4)=1$. We will try to tweak the entries of $A$ into those of a singular matrix $S=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ with determinant $a d-b c=0$. The requirement of the distance being less than $0.2$ means that our perturbations in each row must add up (in absolute value) to at most 0.2. Let's try tweaking 7 to $a=6.9$ and 3 to $d=2.9$ (motive for this move: right now $A$ has $a d=21$, which is one more than $b c=20$; we need to tweak things so that $a d$ is brought down a bit and $b c$ is brought up to meet it). Now we have $a d$ $=20.01$, and we still have a perturabtion allowance of $0.1$ for both entries $b$ and $c$ and we need only bring $b c$ up from its current value of 20 to $20.01$. This is easythere are many ways to do it. For example, keep $c=-5$ and solve $b c=20.01$, which gives $c=20.01 /-5=-4.002$ (well within the remaining perturbation allowance). In summary, the matrix $S=\left[\begin{array}{cc}6.9 & -4.002 \\ -5 & 2.9\end{array}\right]$ meets the requirements that were asked for in part (c). Indeed $S$ is singular (its determinant was arranged to be zero), and the distance from this matrix to $A$ is
\\
$$
\|S-A\|=\left\|\left[\begin{array}{cc}
6.9 & -4.002 \\
-5 & 2.9
\end{array}\right]-\left[\begin{array}{cc}
7 & -4 \\
-5 & 3
\end{array}\right]\right\|=\left\|\left[\begin{array}{cc}
-.1 & -.002 \\
0 & -.1
\end{array}\right]\right\|=.102<0.2
$$
\\
NOTE: The matrix $S$ that we found was actually quite a bit closer to $A$ than what was asked for. Of course, the closer that we wish to find a singular matrix to the ultimate distance, the harder we will have to work with such \textit{ad hoc} methods. Also, the idea used to construct the "extremal" vector $x$ can be modified to give a proof of identity (39); this task will be left to the interested reader as Exercise $10 .$
\\

When we use Gaussian elimination to solve a nonsingular linear system (33): $A x=b$, we will get a \textbf{computed solution} vector $z$ that will, in general, differ from the \textbf{exact (mathematical) solution} $x$ by the \textbf{error term} $\Delta x$ :
\\
\begin{center}
\includegraphics[width=0.4\textwidth]{img_3}
\end{center}
The main goal for the error analysis is to derive estimates for the size of the error (vector) term: $\|\Delta x\|$. Such estimates will give us quality control on the computed solution $z$ to the linear system.
\\
\\
\textbf{Caution}: It may seem that a good way to measure the quality of the computed solution is to look at the size (norm) of the so-called residual vector:
\begin{equation}
r=\text { residual vector } \equiv b-A z \text {. }
\end{equation}
Indeed, if $z$ were the exact solution $x$ then the residual would equal the zero vector. We note the following different ways to write the residual vector:
\begin{equation}
r \equiv b-A z=A x-A z=A(x-z)=A(x-(x+\Delta x))=A(-\Delta x)=-A(\Delta x)
\end{equation}
in particular, the residual is simply the (negative of) the matrix $A$ multiplied by the error term vector. The matrix $A$ may distort a large error term into a much smaller vector thus making the residual much smaller than the actual error term. The following example illustrates this phenomenon (see Figure $7.37$ ).
\begin{SCfigure}[][h]
\caption{Heuristic illustration 
showing the unreliability of the residual 
as a gauge to measure the error. This 
phenomenon is a special case of the 
general principle that a function having a 
very small derivative can have very 
close outputs resulting from different 
inputs spread far apart. }
\includegraphics[scale=.75]{img_4}
\end{SCfigure}
\\
EXAMPLE 7.20: Consider the following linear system  $Ax=b$: 
$$
\left[\begin{array}{cc}
1 & 2 \\
1.0001 & 2
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
3 \\
3.0001
\end{array}\right] \text {. }
$$
This system has (unique) exact solution $x=[1,1]^{\prime}$. Let's consider the (poor) approximation $z=[3,0]^{\prime}$. The (norm of the) error of this approximation is $\|x-z\|=\left\|[-2,1]^{\prime}\right\|=2$, but the residual vector,
$$
r=b-A z=\left[\begin{array}{c}
3 \\
3.0001
\end{array}\right]-\left[\begin{array}{cc}
1 & 2 \\
1.0001 & 2
\end{array}\right]\left[\begin{array}{l}
3 \\
0
\end{array}\right]=\left[\begin{array}{c}
3 \\
3.0001
\end{array}\right]-\left[\begin{array}{c}
3 \\
3.0003
\end{array}\right]=\left[\begin{array}{c}
0 \\
-.0002
\end{array}\right]
$$
has a much smaller norm of only $0.0002$. This phenomenon is also depicted in Figure 7.37.
\\

Despite this drawback about the residual itself, it can be manipulated to indeed give us a useful error estimate. Indeed, from (47), we may multiply left sides by $A^{-1}$ to obtain:
$$
r=A(-\Delta x) \Rightarrow-\Delta x=A^{-1} r .
$$
If we now take norms of both sides and apply (40) , we can conclude that:
$$
\|\Delta x\|=\|-\Delta x\|=\left\|A^{-1} r\right\| \leq\left\|A^{-1}\right\|\|r\|.
$$
(We have used the fact that $\|-v\|=\|v\|$ for any vector $v$; this follows from norm axiom (36B) using $c=-1$.) We now summarize this simple yet important result in the following theorem.
\\
\\
\textbf{THEOREM 7.7}: (Error Bound via Residual) If $z$ is an approximate solution to the exact solution $x$ of the linear system (33) $A x=b$, with $A$ nonsingular, and $r=b-A z$ is the residual vector, then
\begin{equation}
\text { error } \equiv\|x-z\| \leq\left\|A^{-1}\right\|\|r\| . .=
\end{equation}
REMARK: Using the condition number $\kappa(A)=\|A\| A^{-1} \|$, Theorem $7.7$ can be reformulated as
\begin{equation}
\|x-z\| \leq \kappa(A) \frac{\|r\|}{\|A\|}
\end{equation}
\textbf{EXAMPLE 7.21}: Consider once again the linear system $A x=b$ of the preceding example:
$$
\left[\begin{array}{cc}
1 & 2 \\
1.0001 & 2
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2}
\end{array}\right]=\left[\begin{array}{c}
3 \\
3.0001
\end{array}\right].
$$
If we again use the vector $z=[3,0]^{\prime}$ as the approximate solution, then, as we saw in the last example, the error $=2$ and the residual is $r=[0,-0.0002]^{\prime}$. The estimate for the error provided in Theorem 9.2, is (with MATLAB's help) found to be 4 :
\begin{lstlisting} [frame=none,numbers=none]
>>A=[ l 2; 1.0001 2] ; r=[0 -.0002];
>>norm(inv(A), inf) *norm(r,inf)      ->ans = 4.0000 
\end{lstlisting}
Although this estimate for the error is about as far off from the actual error as the approximation $z$ is from the actual solution, as far as an estimate for the error is concerned, it is considered a decent estimate. An estimate for the error is considered good if it has approximately the same order of magnitude (power of 10 in scientific notation) as the actual error.
\\

Using the previous theorem on the error, we obtain the following analogous result for the relative error.
\\
\\
\textbf{THEOREM 7.8}: (Relative Error Bound via Residual) If $z$ is an approximate solution to the exact solution $x$ of the linear system (33) $A x=b$, with $A$ nonsingular, $b \neq 0$ (vector), and $r=b-A z$ is the residual vector, then
\begin{equation}
\text { relative error } \equiv \frac{\|x-z\|}{\|x\|} \leq \frac{\|A\|\left\|A^{-1}\right\|}{\|b\|}\|r\| \text {. }
\end{equation}
REMARK: In terms of condition numbers, Theorem $7.8$ takes on the more appealing form:
\begin{equation}
\frac{\|x-z\|}{\|x\|} \leq \kappa(A) \frac{\|r\|}{\|b\|}
\end{equation}
\textit{Proof of Theorem 7.8}: We first point out that $x \neq 0$ since $b \neq 0$ (and $A x=b$ with $A$ nonsingular). Using identity $(40)$, we deduce that:
$$
\|b\|=\|A x\| \leq\|A\|\|x\| \Rightarrow \frac{1}{\|x\|} \leq \frac{\|A\|}{\|b\|} .
$$
We need only multiply both sides of this latter inequality by $\|x-z\|$ and then apply (48) to arrive at the desired inequality:
$$
\frac{\|x-z\|}{\|x\|} \leq \frac{\|A\|}{\|b\|} \cdot\|x-z\| \leq \frac{\|A\|}{\|b\|} \cdot\left\|A^{-1}\right\|\|r\|
$$
\textbf{EXAMPLE 7.21}: (cont.) Using MATLAB to compute the right side of (50), 
\begin{lstlisting} [frame=none,numbers=none]
>>cond(A,inf) *norm(r, inf)/norm ([3 3.0001]',inf )
->ans = 4.0000 
\end{lstlisting}
Once again, this compares favorably with the true value of the relative error whose explicit value is $\|x-z\| /\|x\|=2 / 1=2$ (see Example 7.20).
\\
\\
EXAMPLE 7.22: Consider the following (large) linear system  Ax=b, with 
$$
A=\left[\begin{array}{ccccccccccc}
4 & -1 & 0 & 0 & -1 & 0 & 0 & 0 & \cdots & 0 & 0 \\
-1 & 4 & -1 & 0 & 0 & -1 & 0 & 0 & \cdots & 0 & 0 \\
0 & -1 & 4 & -1 & 0 & 0 & -1 & 0 & \cdots & & 0 \\
0 & 0 & -1 & 4 & 0 & 0 & 0 & -1 & \ddots & & 0 \\
-1 & 0 & 0 & 0 & 4 & -1 & 0 & 0 & \ddots & & 0 \\
0 & -1 & 0 & 0 & -1 & 4 & -1 & 0 & \ddots & . & 0 \\
\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & \ddots & \ddots & & \\
0 & \vdots & & & & & & & & & \\
0 & & & & \ddots & \ddots & \ddots & \ddots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & & \ddots & -1 & 0 & 0 & -1 & 4 & -1 \\
0 & 0 & 0 & \cdots & & 0 & -1 & 0 & 0 & -1 & 4
\end{array}\right], \quad b=\left[\begin{array}{c}
1 \\
2 \\
3 \\
4 \\
5 \\
6 \\
\vdots \\
798 \\
799 \\
800
\end{array}\right] .
$$
The $800 \times 800$ coefficient matrix $A$ is diagonally banded with a string of 4 's down the main diagonal, a string of $-1$ 's down each of the diagonals 4 below and 4 above the main diagonal, and each of the diagonals directly above and below the main diagonal consist of the vector that starts off with $[-1-1-1]$, and repeatedly tacks the sequence $[0-1-1-1]$ onto this until the diagonal fills. Such banded coefficient matrices are very common in finite difference methods for solving (ordinary and partial) differential equations. Despite the intimidating size of this system, MATLAB's "left divide" can take advantage of its special structure and will produce solutions with very decent accuracy as we will see below:\\
(a) Compute the condition number of the matrix $A$.\\
(b) Use the left divide (Gaussian elimination) to solve this system, and call the computed solution $z$. Use Theorem $7.7$ to estimate its error and Theorem $7.8$ to estimate the relative error. Do not print out the vector $z$ !\\
(c) Obtain a second numerical solution $z 2$, this time by left multiplying the equation by the inverse $A^{-1}$. Use Theorem $7.7$ to estimate its error and Theorem $7.8$ to estimate the relative error. Do not print out the vector $z 2$ !
\\
\\
\textbf{SOLUTION}: We first enter the matrix $A$, making use of MATLAB's useful diag function:
\begin{lstlisting} [frame=none,numbers=none]
>>A=diag(4*ones(1,800)); 
>>al=[- l - 1 -1] ; vrep=[0 - 1 - 1 -1] ; 
>>for i=1:199, al=[al, vrep] ; end %this is level +1/-1 diagonal
>>v4 = -1*ones(1,796); %this is level +4/-4 diagonal
>>A=A+diag(al,1)+diag(al,-1)+diag(v4,4)+diag(v4,-4) ;
>>A(1:8,1:8 ) %we make a quick check to see how A looks
->ans=		4 -3  0  0 -1  0  0  0
		  	 -3  4 -3  0  0 -1  0  0
		   		0 -3  4 -3  0  0 -1  0
		   		0  0 -3  4  0  0  0 -1 
		  	 -1  0  0  0  4 -3  0  0
		   	  0 -1  0  0 -3  4 -3  0 
		   	  0  0 -1  0  0 -3  4 -3 
		   	  0  0  0 -1  0  0 -3  4
\end{lstlisting}
The matrix A looks as it should. The vector b is, of course, easily constructed. 
\begin{lstlisting}[frame=none,numbers=none]
>>b=l:800; b=b'; %needed to take transpose to make b a column vector 
\end{lstlisting}
Part (a):
\begin{lstlisting}[frame=none,numbers=none]
>>c= cond(A,inf) -> c = 2.6257e + 003 
\end{lstlisting} 
With a condition number under 3000 , considering its size, the matrix $A$ is rather well conditioned.
\\
\\
Part (b): Here and in part (c), we use the condition number formulations (49) and (51) for the error estimates of Theorems $7.7$ and $7.8$.
\begin{lstlisting}[frame=none,numbers=none]
>> z=A\b; r=b-A*z; errest=c*norm(r,inf)/norm(A,inf)
-> errest = 2.4875e-010 

>> relerrest=c*norm(r,inf)/norm(b,inf)
-> relerrest = 3.7313e-012
\end{lstlisting}
Part (c):
\begin{lstlisting}[frame=none,numbers=none]
>> z2 = inv(A)*b; r2=b-A*z2; errest2=c*norm(r2,inf)/norm (A,inf)
-> errest2 = 6.8656e-009

>> relerrest2=c*norm(r2,inf)/norm(b,inf)
-> relerrest2 = 1 0298e-010
\end{lstlisting}
Both methods have produced solutions of very decent accuracy. All of the computations here were done with lightning speed. Thus even larger such systems (that are decently conditioned) can be dealt with safely with MATLAB's "left divide." The matrix in the above problem had a very high percentage of its entries being zeros. Such matrices are called sparse matrices, and MATLAB has efficient ways to store and manipulate such matrices. We will discuss this topic in the next section.
\\

For (even moderately sized) poorly conditioned linear systems, quality control of computed solutions becomes a serious issue. The estimates provided in Theorems $7.7$ and $7.8$ are just that, estimates that give a guarantee of the closeness of the computed solution to the actual solution. The actual errors may be a lot smaller than the estimates that are provided. Another more insidious problem is that computation of the error bounds of these theorems is expensive, since it involves either the norm of $A^{-1}$ directly or the condition number of $A$ (which implicitly requires computing the norm of $A^{-1}$ ). Computer errors can lead to inaccurate computation of these error bounds that we would like to use to give us confidence in our numerical solutions. The next example will demonstrate and attempt to put into perspective some of these difficulties. The example will involve the very poorly conditioned Hilbert matrix that we introduced in Section 7.4. We will solve the system exactly (using MATLAB's symbolic toolbox),\footnote{ For more on the Symbolic Toolbox, see Appendix A. This toolbox may or may not be in the version 
of MATLAB that you are using. A reduced version of it comes with the student edition. It is not 
necessary to have it to understand this example.} and thus be able to compare estimated errors (using Theorems $7.7$ and $7.8$ ) with the actual errors. We warn the reader that some of the results of this example may be shocking, but we hasten to add that the Hilbert matrix is notorious for being extremely poorly conditioned.
\\
\\
\textbf{EXAMPLE 7.23}: Consider the linear system $Ax = b$ with 
$$
A=\left[\begin{array}{ccccccc}
1 & \frac{1}{2} & \frac{1}{3} & \cdots & \frac{1}{48} & \frac{1}{49} & \frac{1}{50} \\
\frac{1}{2} & \frac{1}{3} & \frac{1}{4} & \cdots & \frac{1}{49} & \frac{1}{50} & \frac{1}{51} \\
\frac{1}{3} & \frac{1}{4} & \frac{1}{5} & \cdots & \frac{1}{50} & \frac{1}{51} & \frac{1}{52} \\
\vdots & & & \ddots & & \vdots & \vdots \\
\frac{1}{48} & \frac{1}{49} & \frac{1}{50} & \cdots & \frac{1}{96} & \frac{1}{97} & \frac{1}{98} \\
\frac{1}{49} & \frac{1}{50} & \frac{1}{51} & \cdots & \frac{1}{97} & \frac{1}{98} & \frac{1}{99} \\
\frac{1}{50} & \frac{1}{51} & \frac{1}{52} & \cdots & \frac{1}{98} & \frac{1}{99} & \frac{1}{100}
\end{array}\right], \quad b=\left[\begin{array}{c}
1 \\
2 \\
3 \\
\vdots \\
48 \\
49 \\
50
\end{array}\right] .
$$
Using MATLAB, perform the following computations.\\
(a) Compute the condition number of the $50 \times 50$ Hilbert matrix $A$ (on MATLAB using the usual floating point arithmetic).\\
(b) Compute the same condition number using symbolic (exact) arithmetic on MATLAB.\\
(c) Use MATLAB's left divide to solve this system and label the computed solution as $z$, then use Theorem $7.7$ to estimate the error. (Do not actually print the solution.)\\
(d) Solve the system by (numerically) left multiplying both sides by $A^{-1}$ and label the computed solution as $\mathrm{z} 2$; then use Theorem $7.7$ to estimate the error. (Do not actually print the solution.)\\
(e) Solve the system exactly using MATLAB's symbolic capabilities, label this exact solution as $x$, and compute the norm of this solution vector. Then use this exact solution to compute the exact errors of the two approximate solutions in parts (a) and (b).
\\
\\
SOLUTION: Since MATLAB has a built-in function for generating Hilbert matrices, we may very quickly enter the data $A$ and $b$ :
\begin{lstlisting}[frame=none,numbers=none]
>> A=hilb(50); b=l:50;b=b';
\end{lstlisting}
Part (a): We invoke MATLAB's built-in function for computing condition numbers:
\begin{lstlisting}[frame=none,numbers=none]
>> cl=cond(A,inf) 
->Warning: Matrix is close to singular or badly scaled. 
	Results may be inaccurate. RCOND = 3.615845e-020. 
> In C:\MATLABR11\toolbox\matlab\matfun\cond.m at line 44 
->c1 =5.9243e+019 
\end{lstlisting}
This is certainly very large, but it came with a warning that it may be an inaccurate answer due to the poor conditioning of the Hilbert matrix. Let's now see what happens when we use exact arithmetic.
\\
\\
Part (b): Several of MATLAB's built-in functions are not defined for symbolic objects; and this is true for the norm and condition number functions. The way around this is to work directly with the definition (41) of the condition number: $\kappa(A)=\|A\|\left\|A^{-1}\right\|$, compute the norm of $A$ directly (no computational difficulties here), and compute the norm of $A^{-1}$ by first computing $A^{-1}$ in exact arithmetic, then using the double command to put the answer from "symbolic" form into floating point form, so we can take its norm as usual (the computational difficulty is in computing the inverse, not in finding the norm).
\begin{lstlisting}[frame=none,numbers=none]
>>c=norm(double(inv(sym(A))),inf)*norm(A, inf) % "sym" declares A as 
a symbolic variable, so inv is calculated exactly; double switches 
the symbolic answer back into floating point form. 
->c1 = 4.33036 + 074 
\end{lstlisting}
The difference here is astounding! This condition number means that, although the Hilbert matrix $A$ has its largest entry being 1 (and smallest being $1 / 99$ ), the inverse matrix will have some entries having absolute values at least $4.33 \times 10^{74} / 50=8.66 \times 10^{72}$ (why?). With floating point arithmetic, however, MATLAB's computed inverse has all entries less than $10^{20}$ in absolute value, so that MATLAB's inverse is totally out in left field!
\\
\\
Part (c):
\begin{lstlisting}[frame=none,numbers=none]
>> z=A\b; r=b-A*z; 
-> Warning: Matrix is close to singular or badly scaled. 
Results may be inaccurate. RCOND = 3.615845e - 020. 
\end{lstlisting}
As expected, we get a flag about our poorly conditioned matrix.
\begin{lstlisting}[frame=none,numbers=none]
>> norm(r,inf) ->ans = 6.2943e - 005
\end{lstlisting}
Thus the residual of the computed solution is somewhat small. But the extremely 
large condition number of the matrix will overpower this residual to render the 
following useless error estimate (see (49)):
\begin{lstlisting}[frame=none,numbers=none]
>> errest=cl*norm(r, inf)/norm(A, inf) ->errest = 9.5437e+014 
\end{lstlisting}
Siece
\begin{lstlisting}[frame=none,numbers=none]
>> norm(z, inf) ->ans = 5.0466e+012 
\end{lstlisting}
this error estimate is over 100 times as large as the largest component of the 
numerical solution. Things get even worse (if you can believe this is possible) 
with the inverse multiplication method that we look at next. 
\\
\\
Part (d):
\begin{lstlisting}[frame=none,numbers=none]
>> z2=inv(A)*b; r2=b-A*z2; 
->Warning: Matrix is close to singular or badly scaled. 
	Results may be inaccurate. RCOND = 3.615845e - 020. 
>>norm(r2,inf)      ->ans = 1.6189e + 004  
\end{lstlisting}
Here, even the norm of the residual is unacceptably large. 
\begin{lstlisting}[frame=none,numbers=none]
>> errest2=cl*norm(r2,inf)/norm(A, inf)    ->errest = 2.2078e+023 
\end{lstlisting}
Part (e):
\begin{lstlisting}[frame=none,numbers=none]
>> S=sym(A); %declares A as a symbolic matrix 
>> x=S\b; %Computes exact solution of system 
>> x=double (x); %Converts x back to a floating point vector 
>> norm (x, inf) 	->ans = 7.4601e + 04
\end{lstlisting}
We see that the solution vector has some extremely large entries. 
\begin{lstlisting}[frame=none,numbers=none]
>> norm (x-z, inf)  ->ans = 7.4601e + 040 
>> norm (x-z2,inf)  ->ans = 7.4601e + 040 
>> norm (z-z2, inf) ->ans = 3.8429e + 004 
\end{lstlisting}
Comparing all of these norms, we see that the two approximations are closer to 
each other than to the exact solution (by far). The errors certainly met the 
estimates provided for in the theorem, but not by much. The (exact arithmetic) 
computation of JC took only a few seconds for MATLAB to do. Some comments 
are in order. The reader may wonder why one should not always work using exact arithmetic, since it is so much more reliable. The reasons are that it is often not 
necessary to do this—floating point arithmetic usually provides acceptable (and 
usually decent) accuracy, and exact arithmetic is much more expensive. However, 
when we get such a warning from MATLAB about near singularity of a matrix we 
must discard the answers, or at least do some further analysis. Another option 
(again using the Symbolic Toolbox of MATLAB) would be to use variable 
precision arithmetic rather than exact arithmetic. This is less expensive than exact 
arithmetic and allows us to declare how many significant digits with which we 
would like to compute. We will give some examples of this arithmetic in a few 
rare cases where MATLAB's floating point arithmetic is not sufficient to attain the 
desired accuracy (see also Appendix A). 
\\
\\
EXERCISE FOR THE READER 7.25: Repeat all parts of the previous example 
to the following linear system $Ax = b$, with: 
$$
A=\left[\begin{array}{ccccccc}
1 & 1 & 1 & \cdots & 1 & 1 & 1 \\
2^{11} & 2^{10} & 2^{9} & \cdots & 4 & 2 & 1 \\
3^{11} & 3^{10} & 3^{9} & \cdots & 9 & 3 & 1 \\
\vdots & & & \ddots & & \vdots & \vdots \\
10^{11} & 10^{10} & 10^{9} & \cdots & 100 & 10 & 1 \\
11^{11} & 11^{10} & 11^{9} & \cdots & 121 & 11 & 1 \\
12^{11} & 12^{10} & 12^{9} & \cdots & 144 & 12 & 1
\end{array}\right], \quad b=\left[\begin{array}{c}
1 \\
-2 \\
3 \\
\vdots \\
-10 \\
11 \\
-12
\end{array}\right].
$$
This coefficient matrix is the $12\times 12$ Vandermonde matrix that was introduced in 
Section 7.4 with polynomial interpolation. 
\\

We next move on to the concepts of eigenvalues and eigenvectors of a matrix. 
These concepts are most easily motivated geometrically in two dimensions, so let us begin with a $2 \times 2$ matrix $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$, and a nonzero column vector $x=\left[\begin{array}{l}x_{1} \\ x_{2}\end{array}\right]$ $\neq\left[\begin{array}{l}0 \\ 0\end{array}\right]$. We view $A$ (as in Section 7.1) as a linear transformation acting on the vector $x$. The vector $x$ will have a positive length given by the Pythagorean formula:
\begin{equation}
\operatorname{len}(x)=\sqrt{x_{1}^{2}+x_{2}^{2}}
\end{equation}
Thus $A$ transforms the two-dimensional vector $x$ into another vector $y=\left[\begin{array}{l}y_{1} \\ y_{2}\end{array}\right]$ by matrix multiplication $y=A x$. We consider the case where $y$ is also not the zero vector; this will always happen if $A$ is nonsingular. In general, when we graph the vectors $x$ and $y$ together in the same plane, they can have different lengths as well as different directions (see Figure $7.38 \mathrm{a}$ ).
\\

Sometimes, however, there will exist vectors $x$ for which $y$ will be parallel to $x$ (meaning that $y$ will point in either the same direction or the opposite direction as $x$; see Figure 7.38b ). In symbols, this would mean that we could write $y=\lambda x$ for some scalar (number) $\lambda$. Such a vector $x$ is called an eigenvector for the matrix $A$, and the number $\lambda$ is called an associated eigenvalue. Note that if $\lambda$ is positive, then $x$ and $y=\lambda x$ point in the same direction and len $(y)=\lambda \cdot \operatorname{len}(x)$, so that $\lambda$ acts as a magnification factor. If $\lambda$ is negative, then (as in Figure 7.38b) $y$ points in the opposite direction as $x$, and finally if $\lambda=0$, then $y$ must be the zero vector (so has no direction). By convention, the zero vector is parallel to any vector. This is permissible, as long as $x$ is not the zero vector. This definition generalizes to square matrices of any size.
\begin{figure}[H]
\includegraphics[width=1\textwidth]{img_5}\caption{Actions of the matrix $A=\left[\begin{array}{cc}-2 & -1 \\ 0 & 1\end{array}\right]$ on a pair of vectors: (a) (left) The (shorter) vector $x=\left[\begin{array}{l}1 \\ 1\end{array}\right]$ of length $\sqrt{2}$ gets transformed to the vector $y=A x=\left[\begin{array}{c}-3 \\ 1\end{array}\right]$ of length $\sqrt{10}$. Since the two vectors are not parallel, $x$ is not an eigenvector of $A$. (b) (right) The (shorter) unit vector $x=\left[\begin{array}{l}1 \\ 0\end{array}\right]$ gets transformed to the vector $y=A x=\left[\begin{array}{c}-2 \\ 0\end{array}\right]$ (red) of length 2 , which is parallel to $x$, therefore $x$ is an eigenvector for $A$.
}
\end{figure}
\noindent
DEFINITION: Let $A$ be an $n \times n$ matrix. An $n \times 1$ nonzero column vector $x$ is called an \textbf{eigenvector} for the matrix $A$ if for some scalar $\lambda$ we have
\begin{equation}
A x=\lambda x
\end{equation}
The scalar  $\lambda$ is called the \textbf{eigenvalue} associated with the eigenvector $x$ .
\\

Finding all eigenvalues and associated eigenvectors for a given square matrix is an important problem that has been extensively studied and there are numerous algorithms devoted to this and related problems. It turns out to be useful to know this \textbf{eigendata} for a matrix $A$ for an assortment of applications. It is actually quite easy to look at eigendata in a different way that will give an immediate method for finding it. We can rewrite equation $(53)$ as follows:
$$
A x=\lambda x \Leftrightarrow A x=\lambda I x \Leftrightarrow \lambda I x-A x=0 \Leftrightarrow(\lambda I-A) x=0 .
$$
Thus, using what we know about solving linear equations, we can restate the eigenvalue definition in several equivalent ways as follows:
\\
$\lambda$ is an eigenvalue of $A \Leftrightarrow(\lambda I-A) x=0$ has a nonzero solution $x$ 
(which will be an eigenvector)
$$
\Leftrightarrow \quad \operatorname{det}(\lambda I-A)=0
$$
Thus the eigenvalues of $A$ are precisely the roots $\lambda$ of the equation $\operatorname{det}(\lambda I-A)=0$. If we write out this determinant with a bit more detail,
$$
\operatorname{det}(\lambda I-A)=\operatorname{det}\left[\begin{array}{cccccc}
\lambda-a_{11} & -a_{12} & -a_{13} & \cdots & -a_{1, n-1} & -a_{1 n} \\
-a_{21} & \lambda-a_{22} & -a_{23} & & & \\
-a_{31} & -a_{32} & \lambda-a_{33} & & & \vdots \\
\vdots & & & \ddots & & \vdots \\
-a_{n-1,1} & -a_{n-1,2} & -a_{n-1,3} & \cdots & \lambda-a_{n-1, n-1} & -a_{n-1, n} \\
-a_{n 1} & -a_{n 2} & -a_{n 3} & \cdots & -a_{n, n-1} & \lambda-a_{n n}
\end{array}\right],
$$
it can be seen that this expression will always be a polynomial of degree $\mathrm{n}$ in the variable $\lambda$, for any particular matrix of numbers $A=\left[a_{i j}\right]$ (see Exercise 30).This polynomial, because of its importance for the matrix $A$, is called the \textbf{characteristic polynomial} of the matrix $A$, and will be denoted as $p_{A}(\lambda)$. Thus $p_{A}(\lambda)=\operatorname{det}(\lambda I-A)$, and in summary:
\\
\\
\textbf{The eigenvalues of a matrix $A$ are the roots of the characteristic polynomial i.e., the solutions of the equation $p_{A}(\lambda)=0$.}
\\

Finding eigenvalues is an algebraic problem that can be easily solved with MATLAB; more on this shortly. Each eigenvalue will always have associated eignvectors. Indeed, the matrix equation $(\lambda I-A) x=0$ has a singular coefficient matrix when $\lambda$ is an eigenvalue (since its determinant is zero). We know from our work on solving linear equations that a singular $(n \times n)$ linear system of form $C x=0$ can have either no solutions or infinitely many solutions, but $x=0$ is (obviously) always a solution of such a linear system, and consequently such a singular system must have infinitely many solutions. To find eigenvectors associated with a particular eigenvalue $\lambda$, we could compute them numerically by applying \texttt{rref} rather than Gaussian elimination to the augmented matrix $[\lambda I-A|0]$. Although theoretically sound, this approach is not a very effective numerical method. Soon we will describe MATLAB's relevant built-in functions that are based on more sophisticated and effective numerical methods.
\\
\textbf{EXAMPLE 7.24:} For the matrix  $A=\left[\begin{array}{cc}
-2 & -1 \\
1 & 1
\end{array}\right]$ , do the following:
\\
(a) Find the characteristic polynomial  $p_{\lambda}(\lambda)$.\\
(b) Find all roots of the characteristic polynomial (i.e., the eigenvalues of  A ).\\
(c) For each eigenvalue, find an associated eigenvector.\\
SOLUTION: Part (a): $p_{A}(\lambda)=\operatorname{det}(\lambda I-A)=$
$$
\operatorname{det}\left(\left[\begin{array}{cc}
\lambda+2 & 1 \\
-1 & \lambda-1
\end{array}\right]\right)=(\lambda+2)(\lambda-1)+1=\lambda^{2}+\lambda-1
$$.
Part (b): The roots of $p_{A}(\lambda)=0$ are easily obtained from the quadratic formula:
$$
\lambda=\frac{-1 \pm \sqrt{1-4 \cdot 1 \cdot(-1)}}{2}=\frac{-1 \pm \sqrt{5}}{2} \approx-1.6180, .6180 \text {. }
$$
Part (c): For each of these two eigenvalues, let's use \texttt{rref} to find (all) associated 
eigenvectors.\\
$
\text { Case } \lambda=(-1-\sqrt{5}) / 2 \text { : }
$
\begin{lstlisting}[frame=none,numbers=none]
>> A=[-2 -1; 1 1] ; lambda=(-1-sqrt(5))/2 ; 
>> C=lambda*eye(2)-A; C(:,3)=zeros(2,1) ; 
>> rref(C) 
         1.0000 2.6180   0 
->ans=        0    0     0 
\end{lstlisting}
From the last matrix, we can read off the general solution of the system $(\lambda I-A) x=0$ (written out to four decimals):
$$
\left\{\begin{array} { l } 
{ x _ { 1 } + 2 . 6 1 8 0 x _ { 2 } = 0 } \\
{ ( = \text { any number } ) }
\end{array} \Rightarrow \left\{\begin{array}{l}
x_{1}=-2.6180 t \\
x_{2}=t
\end{array}, t=\right.\right.\text { any number }
$$
These give, for all choices of the parameter $t$ except $t=0$, all of the associated eigenvectors. For a specific example, if we take $t=1$, this will give us the eigenvector $x=\left[\begin{array}{c}-2.6180 \\ 1\end{array}\right]$. We can verify this geometrically by plotting the vector $x$ along with the vector $y=A x$ to see that they are parallel.
\\
\\
$
\text { Case } \lambda=(-1+\sqrt{5}) / 2 \text { : }
$
\\
\begin{lstlisting}[frame=none,numbers=none]
>> lambda=(-l+sqrt(5))/2 ; C=lambda*eye(2)-A; rref(C) 
->ans= 1.0000   0.3820   0 
      	 0        0      0 
\end{lstlisting}
As in the preceding case, we can get all of the associated eigenvectors. We consider the eigenvector $x=\left[\begin{array}{c}-.3820 \\ 1\end{array}\right]$ for this second eigenvalue. Since the eigenvalue is postive, $x$ and $A x$ will point in the same directions, as can be checked. Of course, each of these eigenvectors has been written in format \texttt{short}; if we wanted we could have displayed them in format long and thus written our eigenvectors with more significant figures, up to about 15 (MATLAB's accuracy limit).
\\

Before discussing MATLAB's relevant built-in functions for eigendata, we state a theorem detailing some useful facts about eigendata of a matrix. First we give a definition. Since an eigenvalue $\lambda$ of a matrix $A$ is a root of the characteristic polynomial $p_{A}(x)$, we know that $(x-\lambda)$ must be a factor of $p_{A}(x)$. Recall that the \textbf{algebraic multiplicity} of the root $\lambda$ is the highest exponent $m$ such that $(x-\lambda)^{m}$ is still a factor of $p_{A}(x)$, i.e., $p_{A}(x)=(x-\lambda)^{m} q(x)$ where $q(x)$ is a polynomial (of degree $n-m$ ) such that $q(\lambda) \neq 0$.
\\
\\
THEOREM 7.9: (Facts about Eigenvalues and Eigenvectors): Let $A=\left[a_{i j}\right]$ be an $n \times n$ matrix.\\
(i) The matrix $A$ has at most $n$ (real) eigenvalues, and their algebraic multiplicities add up to at most $n$.\\
(ii) If $u, w$ are both eigenvectors of $A$ corresponding to the same eigenvalue $\lambda$, then $u+w$ (if nonzero) is also an eigenvector of $A$ corresponding to $\lambda$, and if $c$ is a nonzero constant, then $c u$ is also an eigenvector of A.\footnote{Thus when we throw all eigenvectors associated with a particular eigenvalue $\lambda$ of a matrix A together 
with the zero vector, we get a set of vectors that is closed under the two linear operations: vector 
additon and scalar multiplication. Readers who have studied linear algebra will recognize such a set as 
a vector space; this one is called the eigenspace associated with the eigenvalue $\lambda$ of the matrix A. 
Geometrically the eigenspace will be either a line through the origin (one-dimensional), a plane through 
the origin (two-dimensional), or in general, any k-dimensional hyperplane through the origin $(k \leq n)$. }
The set of all such eigenvectors associated with $\lambda$, together with the zero vector, is called the \textbf{eigenspace} of $A$ \textbf{associated with the eigenvalue} $\lambda$.\\
(iii) The dimension of the eigenspace of $A$ associated with the eigenvalue $\lambda$, called the \textbf{geometric multiplicity of the eigenvalue} $\lambda$, is always less than or equal to the algebraic multiplicity of the eigenvalue $\lambda$.\\
(iv) In general a matrix $A$ need not have any (real) eigenvalues,\footnote{This is reasonable since, as we have seen before, a polynomial need not have any real roots (e.g., 
$x_2+1$ ). If complex numbers are considered, however, a polynomial will always have a complex root 
(this is the so-called "Fundamental Theorem of Algebra") and so any matrix will always have at least a 
complex eigenvalue. Apart from this fact, the theory for complex eigenvalues and eigenvectors 
parallels that for real eigendata.} but if $A$ is a \textbf{symmetric matrix} (meaning: $A$ coincides with its transpose matrix), then $A$ will always have a full set of $n$ real eigenvalues, provided each eigenvalue is repeated according to its geometric multiplicity.
\\
\\
The proofs of (i) and (ii) are rather easy; they will be left as exercises. The proofs of (iii) and (iv) are more difficult; we refer the interested reader to a good linear algebra textbook, such as [HoKu-71], [Kol-99] or [Ant-00]. There is an extensive theory and several factorizations associated with eigenvalue problems. We should also point out a couple of more advanced texts. The book [GoVL-83] has become the standard reference for numerical analysis of matrix computations. The book [Wil-88] is a massive treatise entirely dedicated to the eigenvalue problem; it remains the standard reference on the subject. Due to space limitations, we will not be getting into comprehensive developments of eigenalgorithms; we will merely give a few more examples to showcase MATLAB's relevant built-in functions.
\\
\\
\textbf{EXAMPLE 7.25:} Find a matrix $A$ that has no real eigenvalues (and hence no eigenvectors), as indicated in part (iv) of Theorem 7.9.
\\
\\
SOLUTION: We should begin to look for a $2 \times 2$ matrix $A$. We need to find one for which its characteristic polynomial $p_{A}(\lambda)$ has no real root. One approach would be to take a simple second-degree polynomial, that we know does not have any real roots, like $\lambda^{2}+1$, and try to build a matrix $A=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ which has this as its characteristic polynomial. Thus we want to choose $a, b, c$, and $d$ such that:
$$
\operatorname{det}\left(\begin{array}{cc}
\lambda-a & b \\
c & \lambda-d
\end{array}\right)=\lambda^{2}+1
$$
If we put $a=d=0$, and compute the determinant we get $\lambda^{2}-b c=\lambda^{2}+1$, so we are okay if $b c=-1$. For example, if we take $b=1$ and $c=-1$, we get that the matrix $A=\left[\begin{array}{cc}0 & 1 \\ -1 & 0\end{array}\right]$ has no real eigenvalues.
\\

MATLAB has the built-in function eig that can find eigenvalues and eigenvectors of a matrix. The two possible syntaxes of this function are as follows:
\\
$$
\begin{array}{|l|l|l|}
\hline \text { norm }(x) \rightarrow & \text { If A is a square matrix, this command produces a vector containing the }\\ &  \text { eigenvalues of A. Both real and complex eigenvalues are given. } x . \\
\hline
&
\text{If A is an «xn matrix, this command will create two nxn matrices. D is a } \\
\text { norm }(x, \text { inf }) \rightarrow & \text{diagonal matrix whose diagonal entries are the eigenvalues of A, and V is}\\
&\text {matrix whose columns are corresponding eigenvectors for A. For complex}\\
&
\text{eigenvalues, the corresponding eigenvectors will also be complex. }\\
\hline
\end{array}
$$
Since, by Theorem $7.9$ (ii), any nonzero scalar multiple of an eigenvector is again an eigenvector, MATLAB's eig chooses its eigenvectors to have length $=1$.
\\

For example, let's use these commands to find eigendata for the matrix of Example 7.24:
\begin{lstlisting}[frame=none,numbers=none]
>> [V,D]=eig([- 2 -1;1 1]) 
-> V= 		-0.9342    0.3568 
				   0.3568   -0.9342 
				   
->D=      -1.6180       0 
              0      0.6180
\end{lstlisting}
The diagonal entries in the matrix $D$ are indeed the eigenvalues (in short format) that we found in the example. The corresponding eigenvectors (from the columns of) $V\left[\begin{array}{c}-0.9342 \\ 0.3568\end{array}\right]$ and $\left[\begin{array}{c}0.3568 \\ -0.9342\end{array}\right]$ are different from the two we gave in that example, but can be obtained from the general form of eigenvectors that was found in that example. Also, unlike those that we gave in the example, it can be checked that these two eigenvectors have length equal to 1 .
\\
\\
In the case of an eigenvalue with geometric multiplicity greater than 1 , \text{eig} will find (whenever possible) corresponding eigenvectors that are linearly independent.\footnote{"Linear independence is a concept from linear algebra. What is relevant for the concept at hand is that 
any other eigenvector associated with the same eigenvalue will be expressible as a linear combination 
of eigenvectors that MATLAB produces. In the parlance of linear algebra, MATLAB will produce 
eigenvectors that form a basis of the corresponding eigenspaces. 
} Watch what happens when we apply the eig function to the matrix that we constructed in Example 7.25:
\begin{lstlisting}[frame=none,numbers=none]
>> [v=D]=eig((0 1;-1 0]) 
->V = 	  0.7071         0.7071
    		  0 + 0.7071i    0-0.7071i 
    		  
->D=			0 + 1.0000i		 0
				  0			         0-1.0000i 
\end{lstlisting}
We get the eigenvalues (from $D$ ) to be the complex numbers $\pm i$ (where $i=\sqrt{-1}$ ) and the two corresponding eigenvectors also have complex numbers in them. Since we are interested only in real eigendata, we would simply conclude from such an output that the matrix has no real eigenvalues.
\\

If you are interested in finding the characteristic polynomial $p_{A}(\lambda)=a_{n} \lambda^{n}+$ $+a_{n-1} \lambda^{n-1} \cdots+a_{1} \lambda+a_{0}$ of an $n \times n$ matrix $A$, MATLAB has a function \texttt{poly} that works as follows:
$$
\begin{array}{|l|l|l|}
\hline 
& \text { For an nxn matrix A, this command will produce the vector v = }\\ 
\text{poly(A)} \rightarrow&  [a_n a_{n-1} \dots a_1 a_0] \text{of the n + 1 coefficients of the $n$th-degree characteristic}  \\
&
\text{polynomial } p_{A}(\lambda)=\operatorname{det}(\lambda I-A)=a_{n} \lambda^{n}+a_{n-1} \lambda^{n-1}+\cdots+a_{1} \lambda+a_{0} \text{ of the}
\\
& \text{matrix A. }\\
\hline
\end{array}
$$
For example, for the matrix we constructed in Example $7.25$, we could use this command to check its characteristic polynomial:
\begin{lstlisting}[frame=none,numbers=none]
>> poly([ 0 l;- l 0]) 		->ans = 1   0   1 
\end{lstlisting}
which translates to the polynomial $1 \cdot \lambda^{2}+0 \cdot \lambda+1=\lambda^{2}+1$, as was desired. Of course, this command is particularly useful for larger matrices where computation of determinants by hand is not feasible. The MATLAB function \texttt{roots} will find the roots of any polynomial:
$$
\begin{array}{|l|l|l|}
\hline 
& \text { For a vector}
\left[\begin{array}{lllll}
a_{n} & a_{n-1} & \cdots & a_{1} & a_{0}
\end{array}\right] \text{of the $n + 1$ coefficients of the $n$th-} 
\\ 
\text{roots (v)} \rightarrow 
& \text{degree polynomial } p(x)=a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0} \text{ this comman}\\
&
\text{will produce a vector of length $n$ containing all of the roots of $p(x)$ (real}
\\
& \text{and complex) repeated according to algebraic multiplicity. }\\
\hline
\end{array}
$$
Thus, another way to get the eigenvalues of the matrix of Example 7.24 would be 
as followsThus, another way to get the eigenvalues of the matrix of Example 7.24 would be 
as follows:\footnote{
We mention, as an examination of the M-file will show (enter \texttt{type roots}), that the roots of a 
polynomial are found using the \texttt{eig} command on an associated matrix. 
}\begin{lstlisting}[frame=none,numbers=none]
>> roots(poly([- 2 -1; 1 1])) 
-> ans      -1.6180 
             0.6180
\end{lstlisting}
EXERCISE FOR THE READER 7.26: For the matrix $ A=\left[\begin{array}{llll}
2 & 1 & 0 & 0 \\
0 & 2 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right]$, do the following:
\\
(a) By hand, compute $p_{A}(\lambda)$, the characteristic polynomial of $A$, in factored form, and find the eigenvalues and the algebraic multiplicity of each one.\\
(b) Either by hand or using MATLAB, find all the corresponding eigenvectors for each eigenvalue and find the geometric multiplicity of each eigenvalue.\\
(c) After doing part (a), can you figure out a general rule for finding the eigenvalues of an upper triangular matrix?



\rule{485pt}{2pt}
\subsubsection{EXERCISES 7.6:}\noindent

\begin{enumerate}
\item 
For each of the following vectors $x$, find $\operatorname{len}(x)$ and find $\|x\|$.
$$
\begin{aligned}
&x=[2,-6,0,3] \\
&x=\left[\cos (n), \sin (n), 3^{n}\right](n=\text { a positive integer) } \\
&x=[1,-1,1,-1, \cdots, 1,-1] \text { (vector has } 2 n \text { components) }
\end{aligned}
$$
\item
For each of the following matrices $A$ , find the infinity norm $\Vert A \Vert$.
\begin{tasks}(2)
		\task $A=\left[\begin{array}{cc}2 & -3 \\ 1 & 6\end{array}\right]$
		\task $A=\left[\begin{array}{rrr}4 & -5 & -2 \\ 1 & 2 & 3 \\ -2 & -4 & -6\end{array}\right]$	
		\task $A=\left[\begin{array}{ccc}\cos (\pi / 4) & -\sin (\pi / 4) & 0 \\ \sin (\pi / 4) & \cos (\pi / 4) & 0 \\ 0 & 0 & 1\end{array}\right]$
		\task $A=H_{n}$ the $n \times n$ Hilbert matrix
(introduced and defined in Example 7.8)
\end{tasks}
\item
For each of the matrices $A$ (parts (a) through (d)) of Exercise 2, find a nonzero vector $x$ such that $\|A x\|=\|A\|\|x\|$.
\item
For the matrix $A=\left[\begin{array}{cc}2 & -3 \\ -2 & 4\end{array}\right]$, calculate by hand the following: $\|A\|,\left\|A^{-1}\right\|, \kappa(A)$, and then verify your calculations with MATLAB. If possible, find a singular matrix $S$ such that $\|S-A\|=1 / 3$.
\item
For the matrix $A=\left[\begin{array}{rc}1 & -1 \\ -1 & 1.001\end{array}\right]$, calculate by hand the following: $\|A\|,\left\|A^{-1}\right\|, \kappa(A)$, and then verify your calculations with MATLAB. Is there a singular matrix $S$ such that $\|S-A\|=1 / 1000 ?$ Explain.
\item
Consider the matrix $B=\left[\begin{array}{ccc}2.6 & 0 & -3.2 \\ 3 & -8 & -4 \\ 1 & 2 & -1\end{array}\right]$.\\
(a) Is there a nonzero $(3 \times 1)$ vector $x$ such that $\|B x\| \geq 13\|x\|$ ? If so, find one; otherwise explain why one does not exist.\\
(b) Is there a singular $3 \times 3$ matrix $S$ such that $\|S-B\| \leq 0.2$ ? If so, find one; otherwise explain why one does not exist.
\item
Consider the matrices: $A=\left[\begin{array}{cc}2 & -6 \\ 11 & -5\end{array}\right], B=\left[\begin{array}{ccc}7 & 1 & -4 \\ 5 & -8 & -5 \\ 4 & 4 & 4\end{array}\right]$.\\
(a) Is there a $(2 \times 1)$ vector $X$ such that: $\|A X\|>12\|X\|$ ?\\
(b) Is there a nonzero vector $X$ such that $\|A X\| \geq 16\|X\|$ ? If so, find one; otherwise explain why one does not exist.\\
(c) Is there a nonzero $(3 \times 1)$ vector $X$ such that $\|B X\| \geq 20\|X\|$ ? If so, find one; otherwise explain why one does not exist.\\
(d) Is there a singular matrix $S=\left[\begin{array}{ll}a & b \\ c & d\end{array}\right]$ (i.e., $a d-b c=0$ ) such that $\|S-A\| \leq 4.5$ ? If yes, find one; otherwise explain why one does not exist.\\
(c) Is there a singular $3 \times 3$ matrix $S$ such that $\|S-B\| \leq 2.25$ ? If so find one; otherwise explain why one does not exist.
\item
Prove identities (42), (43), and (44) for condition numbers.\\
\textbf{Suggestion:} The identity that was established in Exercise for the Reader $7.24$ can be helpful.
\item \textit{(True/False)} For each statement below, either explain why it is always true or provide a counterexample of a single situation where it is false:\\
(a) If $A$ is a square matrix with $\|A\|=0$, then $A=0$ (matrix), i.e., all the entries of $A$ are zero.\\
(b) If $A$ is any square matrix, then $|\operatorname{det}(A)| \leq \kappa(A)$.\\
(c) If $A$ is a nonsingular square matrix, then $\kappa\left(A^{-1}\right)=\kappa(A)$.\\
(d) If $A$ is a square matrix, then $\kappa\left(A^{\prime}\right)=\kappa(A)$.\\
(e) If $A$ and $B$ are same-sized square matrices, then $\|A B\| \leq\|A\| B \|$.\\
\textbf{Suggestion:} As is always recommended, unless you are sure about any of these identities, run a bunch of experiments on MATLAB (using randomly generated matrices).
\item
Prove identity (39).
Suggestion: Reread Example $7.19$ and the note that follows it for a useful idea.
\item
(A General Class of Norms) For any real number $p \geq 1$, the $p$-norm $\|\cdot\|_{p}$ defined for an $n$ dimensional vector $x$ by the equation
$$
\|x\|_{p}=\left(\sum_{i=1}^{n}\left|x_{i}\right|^{p}\right)^{1 / p}=\left(\left|x_{1}\right|^{p}+\left|x_{2}\right|^{p}+\cdots+\left|x_{n}\right|^{p}\right)^{1 / p}
$$
turns out to satisfy the norm axioms $(36 A-C)$. In the general setting, the proof of the triangle inequality is a bit involved (we refer to one of the more advanced texts on error analysis cited in the section for details).\\
(a) Show that len $(x)=\|x\|_{2}$ (this is why the length norm is sometimes called the $2-$ norm).\\
(b) Verify the norm axioms $(36 A-C)$ for the 1 -norm $\|\cdot\|_{1}$.\\
(c) For a vector $x$, is it always true that $\|x\|_{\infty} \leq\|x\|_{1}$ ? Either prove it is always true or give a counterexample of an instance of a certain vector $x$ for which it fails.\\
(d) For a vector $x$, is it always true that $\|x\|_{\infty} \leq\|x\|_{2}$ ? Either prove it is always true or give a counterexample of an instance of a certain vector $x$ for which it fails.\\
(e) How are the norms $\|\cdot\|_{1}$ and $\|\cdot\|_{1}$ related? Does one always seem to be at least as large as the other? Do (lots of) experiments with some randomly generated vectors of different sizes.\\
\textbf{Note:} Experiments will probably convince you of a relationship (and inequality) for part (c), but it might be difficult to prove, depending on your background; the interested reader can find a proof in one of the more advanced references listed in the section. The reason that infinity norm got this name is that for any fixed vector $x$, we have
$$
\lim _{p \rightarrow \infty}\|x\|_{p}=\|x\|_{\infty} \text {. }
$$
As the careful reader might have predicted, the MATLAB built-in function for the $p$-norm of a vector $x$ is norm $(x, p)$.
\item
Let $A=\left[\begin{array}{cc}-11 & 3 \\ 4 & -1\end{array}\right], b=\left[\begin{array}{l}2 \\ 0\end{array}\right] \quad z=\left[\begin{array}{l}1.2 \\ 7.8\end{array}\right]$\\
(a) Let $z$ be an approximate solution of the system $A x=b$; fond the residual vector $r$.\\
(b) Use Theorem $7.7$ to give an estimate for the error of the approximation in part (a).\\
(c) Give an estimate for the relative error of the approximation in part (a).\\
(d) Find the norm of the exact error $\| \Delta x \|$ of the approximate solution in part (a).
\item
Repeat all parts of Exercise 12 for the following matrices:
$$
A=\left[\begin{array}{cc}
-0.1 & -9 \\
11 & 1000
\end{array}\right], \quad b=\left[\begin{array}{c}
-0.1 \\
10
\end{array}\right] \quad z=\left[\begin{array}{c}
11 \\
-0.2
\end{array}\right]
$$
\item
Let $A=\left[\begin{array}{ccc}1 & 0 & 1 \\ -1 & 1 & 1 \\ -1 & -1 & 1\end{array}\right], b=\left[\begin{array}{l}1 \\ 2 \\ 3\end{array}\right]$\\
(a) Use MATLAB's "left divide" to solve the system $A x=b$.\\
(b) Use Theorems $7.7$ and $7.8$ to estimate the error and relative error of the solution obtained in part (a).\\
(c) Use MATLAB to solve the system $A x=b$ by left multiplying the equation by the inv (A).\\
(d) Use Theorems $7.7$ and $7.8$ to estimate the error and relative error of the solution obtained in part (c).\\
(e) Solve the system using MATLAB's symbolic capabilities and compute the actual errors of the solutions obtained in parts (a) and (c).
\item
Let $A$ be the $60 \times 60$ matrix whose entries are 1 's across the main diagonal and the last column, $-1$ 's below the main diagonal, and whose remaining entries (above the main diagonal) are zeros, and let $b=\left[\begin{array}{lllllll}1 & 2 & 3 & \cdots & 58 & 59 & 60\end{array}\right]^{\prime}$.\\
(a) Use MATLAB's left divide to solve the system $A x=b$ and label this computed solution as $z$. Print out only $z(37)$.\\
(b) Use Theorems $7.7$ and $7.8$ to estimate the error and relative error of the solution obtained in part (a).\\
(c) Use MATLAB to solve the system $A x=b$ by left multiplying the equation by the inv (A) and label this computed solution as $z 2$. Print out only $z 2(37)$.\\
(d) Use Theorems $7.7$ and $7.8$ to estimate the error and relative error of the solution obtained in part (c).\\
(e) Solve the system using MATLAB's symbolic capabilities, and print out $\times(37)$ of the exact solution. Then compute the norms of the actual errors of the solutions obtained in parts (a) and (c).
\item
\textit{(Iterative Refinement)} Let $z_{0}$ be the computer solution of Exercise $15(\mathrm{a})$, and let $r_{0}$ denote the corresponding residual vector. Now use the Gaussian program to solve the system $A x=r_{0}$, and call the computer solution $z_{1}$, and the corresponding residual vector $r_{1}$. Next use the Gaussian program once again to solve $A x=r_{1}$, and let $z_{2}$ and $r_{2}$ denote the corresponding approximate solution and residual vector. Now let
$$
z=z_{0}, \quad z^{\prime}=z+z_{1} \text {, and } z^{\prime \prime}=z^{\prime}+z_{2} .
$$
Viewing these three vectors as solutions of the original system $A x=b$ (of Exercise 15), use the error estimate Theorem $7.8$ to estimate the relative error of each of these three vectors. Then compute the norm of the actual errors by comparing with the exact solution of the system as obtained in part (e) of Exercise 15. See the n
\item
In theory, the iterative technique of the previous exercise can be useful to improving accuracy of approximate solutions in certain circumstances. In practice, however, roundoff errors and poorly conditioned matrices can lead to unimpressive results. This exercise explores the effect that additional digits of precision can have on this scheme.\\
(a) Using variable precision arithmetic (see Appendix A) with 30 digits of accuracy, redo the previous exercise, starting with the computed solution of Exercise 15(a) done in MATLAB's default floating point arithmetic.\\
(b) Using the same arithmetic of part (a), solve the original system using MATLAB's left divide.\\
(c) Compare the norms of the actual errors of the three approximate solutions of part (a) and the one of part (b) by using symbolic arithmetic to get MATLAB to compute the exact solution of the system.\\
\textbf{Note:} We will learn about different iterative methods in the next section.
\item
This exercise will examine the benefits of variable precision arithmetic over MATLAB's default floating point arithmetic and over MATLAB's more costly symbolic arithmetic. As in Section 7.4, we let $H_{n}=[1 /(i+j-1)]$ denote the $n \times n$ Hilbert matrix. Recall that it can be generated in MATLAB using the command \texttt{hilb (n)}.\\
(a) For the values $n=5,10,15, \cdots, 100$ create the corresponding Hilbert matrices $H_{n}$ in MATLAB as symbolic matrices and compute symbolically the inverses of each. Use \texttt{tic/toc} to record the computation times (these times will be machine dependent; see Chapter 4). Go as far as you can until your cumulative MATLAB computation time exceeds one hour. Next compute the corresponding condition numbers of each of these Hilbert matrices.\\
(b) Starting with MATLAB's default floating point arithmetic (which is roughly 15 digits of variable precision arithmetic), and then using variable precision arithmetic starting with 20 digits and then moving up in increments of $5(25,30,35, \ldots)$, continue to compute the inverses of each of the Hilbert matrices of part (a) until you get a computed inverse whose norm differs from the norm of the exact inverse in part (a) by no more than $0.000001$. Record (using \texttt{tic/toc} the computation time for the final variable precision arithmetically computed inverse, along with the number of digits used, and compare it to the corresponding computation time for the exact inverse that was done in part (a).
\item 
Prove the following inequality $\|A x\| \geq\|x\| / \|A^{-1} \|$, where $A$ is any invertible $n \times n$ matrix and $x$ is any column vector with $n$ entries.
\item
Suppose that $A$ is a $2 \times 2$ matrix with norm $\|A\|=0.5$ and $x$ and $y$ are $2 \times 1$ vectors with $\|x-y\| \leq 0.8$. Show that: $\left\|A^{2} x-A^{2} y\right\| \leq 0.2$.
\item
\textit{(Another Error Bound for Computed Solutions of Linear Systems)} For a nonsingular matrix A and a computed inverse matrix $C$ for $A^{-1}$, we define the resulting \textbf{residual matrix} as $R=I-C A$. If $z$ is an approximate solution to $A x=b$, and as usual $r=b-A z$ is the residual vector, show that
$$
\text { error } \equiv\|x-z\| \leq \frac{\|C R\|}{1-\|R\|}
$$
provided that $\|R\|<1$.\\
\textbf{Hint:} For part (a), first use the equation $I-R=C A$ to get that $(I-R)^{-1}=A^{-1} C^{-1}$ and so $A^{-1}=(I-R)^{-1} C$. (Recall that the inverse of a product of invertible matrices equals the reverse order product of the inverses.)
\item
For each of the matrices $A$ below, find the following:\\
(a) The characteristic polynomial $p_{A}(\lambda)$.\\
(b) All eigenvalues and all of their associated eigenvectors.\\
(c) The algebraic and geometric multiplicity of each eigenvalue.
\settasks{
	counter-format=,
	label-width=4ex
}
\begin{tasks}(4)
\task (i) $A=\left[\begin{array}{ll}1 & 2 \\ 2 & 1\end{array}\right]$
\task (ii) $A=\left[\begin{array}{ll}1 & 1 \\ 2 & 2\end{array}\right]$
\task (iii) $A=\left[\begin{array}{ll}1 & 2 \\ 2 & 2\end{array}\right]$
\task (iv) $A=\left[\begin{array}{rr}-1 & 0 \\ 2 & 3\end{array}\right]$
\end{tasks}
\item
Repeat all parts of Exercise 22 for the following matrices.
\settasks{
	counter-format=,
	label-width=4ex
}
\begin{tasks}(2)
\task (i) $A=\left[\begin{array}{lll}1 & 2 & 2 \\ 2 & 1 & 2 \\ 2 & 2 & 1\end{array}\right]$
\task (ii) $A=\left[\begin{array}{lll}1 & 2 & 0 \\ 2 & 1 & 2 \\ 0 & 2 & 1\end{array}\right]$
\task (iii) $A=\left[\begin{array}{lll}1 & 2 & 2 \\ 2 & 1 & 2 \\ 0 & 0 & 1\end{array}\right]$
\task (iv) $A=\left[\begin{array}{ccc}1 & 2 & 2 \\ -2 & 1 & 2 \\ -2 & -2 & 1\end{array}\right]$
\end{tasks}
\item
Consider the matrix $A=\left[\begin{array}{ccc}11 & 11 & 4 \\ 7 & 7 & -4 \\ -7 & -11 & 0\end{array}\right]$.\\
(a) Find all eigenvalues of $A$, and for each find just one eigenvector (give your eigenvectors as many integer components as possible).\\
(b) For each of the eigenvectors $x$ that you found in part (a), evaluate $y=(2 A) x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$ ? In other words, is $x$ also an eigenvector of the matrix $2 A$ ?\\
(c) Find all eigenvalues of $2 A$. How are these related to those of the matrix $A$ ?\\
(d) For each of your eigenvectors $x$ from part (a), evaluate $y=(-5 A) x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$ ? In other words, is $x$ also an eigenvector of the matrix $-5 A$ ?\\
(e) Find all eigenvalues of $-5 A$; how are these related to those of the matrix $A$ ?\\
(f) Based on your work in these above examples, without picking up your pencil or typing anything on the computer, what do you think the eigenvalues of the matrix $23 \mathrm{~A}$ would be? Could you guess also some associated eigenvectors for each eigenvalue? Check your conclusions on MATLAB.
\item
Consider the matrix $A=\left[\begin{array}{ccc}2 & 0 & 1 \\ 1 & -4 & 1 \\ 1 & 0 & 2\end{array}\right]$.\\
(a) Find all eigenvalues of $A$, and for each find just one eigenvector (give your eigenvectors as many integer components as possible).\\
(b) For each of the eigenvectors $x$ that you found in part (a), evaluate $y=A^{2} x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$ ? In other words, is $x$ also an eigenvector of the matrix $A^{2}$ ?\\
(c) Find all eigenvalues of $A^{2}$; how are these related to those of the matrix $A$ ?\\
(d) For each of your eigenvectors $x$ from part (a), evaluate $y=A^{3} x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$? In other words, is $x$ also an eigenvector of the matrix $A^{3}$ ?\\
(e) Find all eigenvalues of $A^{3}$; how are these related to those of the matrix $A$ ?\\
(f) Based on your work in the above examples, without picking up your pencil or typing anything on the computer, what do you think the eigenvalues of the matrix $A^{8}$ would be? Could you guess also some associated eigenvectors for each eigenvalue? Check your conclusions on MATLAB.
\item
Find the characteristic polynomial (factored form is okay) as well as all eigenvalues for the $n \times n$ identity matrix $I$. What are (all) of the corresponding eigenvectors (for each eigenvalue)?
\item
Consider the matrix $A=\left[\begin{array}{lll}3 & 3 & 3 \\ 3 & 2 & 4 \\ 3 & 4 & 2\end{array}\right]$.\\
(a) Find all eigenvalues of $A$, and for each find just one eigenvector (give your eigenvectors as many integer components as possible).\\
(b) For each of the eigenvectors $x$ that you found in part (a), evaluate $y=\left(A^{2}+2 A\right) x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$? ln other words, is $x$ also an eigenvector of the matrix $A^{2}+2 A$ ?\\
(c) Find all eigenvalues of $A^{2}+2 A$; how are these related to those of the matrix $A$ ?\\
(d) For each of your eigenvectors $x$ from Part (a), evaluate $y=\left(A^{3}-4 A^{2}+I\right) x$. Is it possible to write $y=\lambda x$ for some scalar $\lambda$ ? In other words, is $x$ also an eigenvector of the matrix $A^{3}-4 A^{2}+I ?$\\
(e) Find all eigenvalues of $A^{3}-4 A^{2}+I$; how are these related to those of the matrix $A$ ?\\
(f) Based on your work in the above examples, without picking up your pencil or typing anything on the computer, what do you think the eigenvalues of the matrix $A^{5}-4 A^{3}+2 A-4 I$ would be? Could you guess also some associated eigenvectors for each eigenvalue? Check your conclusions on MATLAB.

\textbf{NOTE:} \textbf{The spectrum} of a matrix $A$, denoted $\sigma(A)$, is the set of all eigenvalues of the matrix $A$. The next exercise generalizes some of the results discovered in the previous four exercises.
\item
For a square matrix $A$ and any polynomial $p(x)=a_{m} x^{m}+a_{m-1} x^{m-1}+\cdots+a_{1} x+a_{0}$, we define a new matrix $p(A)$ as follows:
$$
p(A)=a_{m} A^{m}+a_{m-1} A^{m-1}+\cdots+a_{1} A+a_{0} I .
$$
(We simply substituted $A$ for $x$ in the formula for the polynomial; we also had to replace the constant term $a_{0}$ by this constant times the identity matrix--the matrix analogue of the number
1.) Prove the following appealing formula;
$$
\sigma(p(A))=p(\sigma(A))
$$
which states that the spectrum of the matrix $p(A)$ equals the set 
$$\{p(\lambda): \lambda \text{is an eigenvalue of A\}} .$$
\item
Prove parts
(i) and (ii) of Theorem 7.9.
\item
Show that the characteristic polynomial of any $n \times n$ matrix is always a polynomial of degree $n$ in the variable $\lambda$.\\
\textbf{Suggestion:} Use induction and cofactor expansion.
\item
(a) Use the basic Gaussian elimination algorithm (Program 7.6) to solve the linear systems of Exercise for the Reader 7.16, and compare with results obtained therein.\\
(b) Use the Symbolic Toolbox to compute the condition numbers of the Hilbert matrices that came up in part (a). Are the estimates provided by Theorem $7.8$ accurate or useful?\\
(c) Explain why the algorithm performs so well with this problem despite the large condition numbers of $A$.\\
\textbf{Suggestion:} For part (c), examine what happens after the first pivot operation.
\end{enumerate}

\subsubsection{ITERATIVE METHODS 7.7:}\noindent
As mentioned earlier in this chapter, Gaussian elimination is the best all-around solver for nonsingular linear systems $A x=b$. Being a universal method, however, there are often more economical methods that can be used for particular forms of the coefficient matrix. We have already seen the tremendous savings, both in storage and in computations that can be realized in case $A$ is tridiagonal, by using the Thomas method. All methods considered thus far have been direct methods in that, mathematically, they compute the exact solution and the only errors that arise are numerical. In this section we will introduce a very different type of method called an \textbf{iterative method}. Iterative methods begin with an initial guess at the (vector) solution $x^{(0)}$, and produce a sequence of vectors, $x^{(1)}, x^{(2)}, x^{(3)}, \cdots$, which, under certain circumstances, will converge to the exact solution. Of course, in any floating point arithmetic system, a solution from an iterative method (if the method converges) can be made just as accurate as that of a direct method.
\\

In solving differential equations with so-called finite difference methods, the key numerical step will be to solve a linear system $A x=b$ having a large and sparse coefficient matrix $A$ (a small percentage of nonzero entries) that will have a special form. The large size of the matrix will often make Gaussian elimination too slow. On the other hand, the special structure and sparsity of $A$ can make the system amenable to a much more efficient iterative method. We have seen that in general Gaussian elimination for solving an $n$ variable linear system performs in $O\left(n^{3}\right)$-time. We take this as the ceiling performance time for any linear system solver. The Thomas method, on the other hand, for the very special triadiagonal systems, performed in only $O(n)$-time. Since just solving $n$ independent linear equations (i.e., with $A$ being a diagonal matrix) will also take this amount, this is the theoretical floor performance time for any linear system solver. Most of iterative methods today perform theoretically in $O\left(n^{2}\right)$-time, but in practice can perform in times closer to the theoretical floor $O(n)$-time. In recent years, iterative methods have become increasingly important and have a promising future, as increasing computer performance will make the improvements over Gaussian elimination more and more dramatic.
\\

We will describe three common iterative methods: Jacobi, Gauss-Seidel, and SOR iteration. After giving some simple examples showing the sensitivity of these methods to the particular form of $A$, we give some theoretical results that will guarantee convergence. We then make some comparisons among these three methods in flop counts and computation times for larger systems and then with Gaussian elimination. The theory of iterative methods is a very exciting and interesting area of numerical analysis. The Jacobi, Gauss-Seidel, and SOR iterative methods are quite intuitive and easy to develop. Some of the more stateof-the-art methods such as conjugate gradient methods and GMRES (generalized minimum residual method) are more advanced and would take a lot more work to develop and understand, so we refer the interested reader to some references for more details on this interesting subject: [Gre-97], [TrBa-97], and [GoVL-83]. MATLAB, however, does have some built-in functions for performing such more advanced iterative methods. We introduce these MATLAB functions and do some performance comparisons involving some (very large and) typical coefficient matrices that arise in finite difference schemes.
\\
\\
We begin with a nonsingular linear system: 
\begin{equation}
A x=b
\end{equation}
In scalar form, it looks like this:
\begin{equation}
a_{i 1} x_{1}+a_{i 2} x_{2}+\cdots a_{i n} x_{n}=b_{i} \quad(1 \leq i \leq n)
\end{equation}
Now, if we assume that each of the diagonal entries of $A$ are nonzero, then each of the equations in $(55)$ can be solved for $x_{i}$ to arrive at:
\begin{equation}
x_{i}=\frac{1}{a_{i i}}\left[b_{i}-a_{i 1} x_{1}-a_{i 2} x_{2}-\cdots-a_{i, i-1} x_{i-1}-a_{i, i+1} x_{i+1}-\cdots a_{i n} x_{n}\right](1 \leq i \leq n) .
\end{equation}
The Jacobi iteration scheme is obtained from using formula (56) with the values of current iteration vector $x^{(k)}$ on the right to create, on the left, the values of the next iteration vector $x^{(k+1)}$. We record the simple formula:
\\
\\
\textbf{Jacobi Iteration:}
\begin{equation}
x_{i}^{(k+1)}=\frac{1}{a_{i i}}\left[b_{i}-\sum_{\substack{j=1, \\ j \neq i}}^{n} a_{i j} x_{j}^{(k)}\right](1 \leq i \leq n)
\end{equation}
Let us give a (very) simple example illustrating this scheme on a small linear 
system and compare with the exact solution. 
\\
\\
\textbf{EXAMPLE 7.26:} Consider the following linear system:
$$
\begin{aligned}
&3 x_{1}+x_{2}-x_{3}=-3 \\
&4 x_{1}-10 x_{2}+x_{3}=28 .\\
&2 x_{1}+x_{2}+5 x_{3}=20
\end{aligned}
$$
(a) Starting with the vector $x^{(0)}=\left[\begin{array}{lll}0 & 0 & 0\end{array}\right]$ ', apply the Jacobi iteration scheme with up to 30 iterations until (if ever) the 2 -norm of the differences $x^{(k+1)}-x^{(k)}$ is less than $10^{-6}$. Plot the norms of these differences as a function of the iteration. If convergence occurs, record the number of iterations and the actual 2-norm error of the final iterant with the exact solution.\\
(b) Repeat part (a) on the equivalent system obtained by switching the first two equations.
\\
\\
SOLUTION: Part (a): The Jacobi iteration scheme (57) becomes:
$$
\begin{aligned}
&x_{1}^{(k+1)}=\left(-3-x_{2}^{(k)}+x_{3}^{(k)}\right) / 3 \\
&x_{2}^{(k+1)}=\left(28-4 x_{1}^{(k)}-x_{3}^{(k)}\right) /(-10) . \\
&x_{3}^{(k+1)}=\left(20-2 x_{1}^{(k)}-x_{2}^{(k)}\right) / 5
\end{aligned}
$$
The following MATLAB code will perform the required tasks:
\begin{lstlisting}[frame=none,numbers=none]
xold = [0 0 0]' ; xnew=xold; 
for k=l:30 
	xnew(l)=(-3-xold(2)+xold(3))/3; 
	xnew(2) = (28-4*xold(l)-xold(3))/(-10) ; 
	xnew(3)=(20-2*xold(l)-xold(2))/5; 
	diff(k)=norm(xnew-xold, 2) ; 
	if diff(k)<le-6 
		fprintf('Jacobi iteration has converged in %d iterations', k) 
		return 
	end 
	xold=xnew; 
end 
-> Jacobi iteration has converged in 26 iterations
\end{lstlisting}
The exact solution is easily seen to be $\left[\begin{array}{lll}1 & -2 & 4\end{array}\right]^{\prime}$. The exact 2-norm error is thus given by:
\begin{lstlisting}[frame=none,numbers=none]
>>norm(xnew-[l -2 4]',2) -> ans = 3.9913e-007
\end{lstlisting}
which compares favorably with the norm of the last difference of the iterates (i.e., 
the actual error is smaller): 
\begin{lstlisting}[frame=none,numbers=none]
>> diff(k) ->ans = 8.9241e-007
\end{lstlisting}
We will see later in this section that finite difference methods typically exhibit linear convergence (if they indeed converge); the quality of convergence will thus depend on the asymptotic error constant (see Section $6.5$ for the terminology). Due to this speed of the decay of errors, an ordinary plot will not be so useful (as the reader should verify), so we use a log scale on the $y$-axis. This is accomplished by the following MATLAB command:
$$
\begin{array}{|l|l|l|}
\hline
& \text{If x and y are two vectors of the same size, this will produce}\\ 
\text { semilogy(x , y) } \rightarrow & \text { a plot where the .y-axis numbers are logarithmically spaced }\\ &  \text { rather than equally spaced as with \texttt{plot(x, y)}. } x . \\
\hline
\text { semilogy(x , y) } \rightarrow & \text{Works as the above command, but now the x-axis numbers }\\
&\text {are logarithmically spaced.}\\
\hline
\end{array}
$$
The required plot is now created with the following command and the result is 
shown in Figure 7.39(a).
\begin{lstlisting}[frame=none,numbers=none]
>>semilogy(l:k,diff(1:k)) 
\end{lstlisting}
Part (b): Switching the first two equations of the given system leads to the 
following modified Jacobi iteration scheme:
$$
\begin{aligned}
&x_{1}^{(k+1)}=\left(28+10 x_{2}^{(k)}-x_{3}^{(k)}\right) / 4 \\
&x_{2}^{(k+1)}=-3-3 x_{1}^{(k)}+x_{3}^{(k)} \\
&x_{3}^{(k+1)}=\left(20-2 x_{1}^{(k)}-x_{2}^{(k)}\right) / 5
\end{aligned} .
$$
In the above MATLAB code, we need only change the two lines for xnew(54) and 
xnew(55) accordingly: 
\begin{lstlisting}[frame=none,numbers=none]
xnew(l)=(28+10*xold(2)-xold(3))/4; 
xnew(2)=-3-3*xold(l)+xold(3); 
\end{lstlisting}
Running the code, we see that this time we do not get convergence. In fact, a 
semilog plot will show that quite the opposite is true, the iterates badly diverge. 
The plot, obtained just as before, is shown in Figure 7.39(b). We will soon show 
how such sensitivities of iterative methods depend on the form of the coefficient 
matrix. 
\begin{figure}[H]
\includegraphics[width=1\textwidth]{img_6}\caption{(a) (left) Plots of the 2-norms of the differences of successive iterates in the 
Jacobi scheme for the linear system of Example 7.26, using the zero vector as the initial 
iterate. The convergence is exponential, (b) (right) The corresponding errors when the 
same scheme is applied to the equivalent linear system with the first two equations being 
permuted. The sequence now badly diverges, showing the sensitivity of iterative methods 
to the particular form of the coefficient matrix. 
}
\end{figure}
The code given in the above example can be easily generalized into a MATLAB 
M-file for performing the Jacobi iteration on a general system. This task will be 
delegated to the following exercise for the reader.
\\
\\
EXERCISE FOR THE READER 7.27: (a) Write a function M-file, \texttt{[x,k,diff]=jacobi(A,b,x0,tol,kmax}, that performs the Jacobi iteration on the linear system $A x=b$. The inputs are the coefficient matrix $\mathrm{A}$, the inhomogeneity (column) vector $b$, the seed (column) vector $x 0$ for the iteration process, the tolerance \texttt{tol}, which will cause the iteration to stop if the 2-norms of successive iterates become smaller than \texttt{tol}, and \texttt{kmax}, the maximum number of iterations to perform. The outputs are the final iterate $x$, the number of iterations performed $k$, and a vector \texttt{diff} that records the 2-norms of successive differences of iterates. If the last three input variables are not specified, default values of $\times 0=$ the zero column vector, \texttt{tol} $=1 \mathrm{e}-10$, and $\mathrm{kmax}=100$ are used.\\
(b) Apply the program to recover the data obtained in part (a) of Example 7.26. If we reset the tolerance for accuracy to $1 \mathrm{e}-10$ in that example, how many iterations would the Jacobi iteration need to converge?
\\

If we compute the values of $x^{(k+1)}$ in order, it seems reasonable to update the values used on the right side of $(57)$ sequentially, as they become available. This modification in the scheme gives the Gauss-Seidel iteration. Notice that the Gauss-Seidel scheme can be implemented so as to roughly cut in half the storage requirements for the iterates of the solution vector $x$. Although the M-file we present below does not take advantage of such a scheme, the interested reader can easily modify it to do so. Futhermore, as we shall see, the Gauss-Seidel scheme almost always outperforms the Jacobi scheme.
\\
\\
\textbf{Gauss-Seidel Iteration:}
\begin{equation}
x_{i}^{(k+1)}=\frac{1}{a_{i i}}\left[b_{i}-\sum_{j=1}^{i-1} a_{i j} x_{j}^{(k+1)}-\sum_{j=i+1}^{n} a_{i j} x_{j}^{(k)}\right](1 \leq i \leq n).
\end{equation}
We proceed to write an M-file that will apply the Gauss-Seidel scheme to solving 
the nonsingular linear system (54). 
\\
\\
\textbf{PROGRAM 7.7:} A function M-file
$$\text{\texttt{[x,k,diff]=gaussseidel(A,b,xO,tol,kmax)} }$$
that performs the Gauss-Seidel iteration on the linear system $A x=b$. The inputs are the coefficient matrix $A$, the inhomogeneity (column) vector $b$, the seed (column) vector $x 0$ for the iteration process, the tolerance \texttt{tol}, which will cause the iteration to stop if the 2 norms of successive iterates become smaller than \texttt{tol}, and \texttt{kmax}, the maximum number of iterations to perform. The outputs are the final iterate $x$, the number of iterations performed $k$, and a vector \texttt{diff} that records the 2-norms of successive differences of iterates. If the last two input variables are not specified, default values of \texttt{tol} $=1 \mathrm{e}-10$ and $\mathrm{kmax}=100$ are used.
\begin{lstlisting}[numbers=none]
function [x, k, diff] = gaussseidel(A,b,x0,tol,kmax)
%performs the Gauss-Seidel iteration on the linear system Ax = b.
%Inputs: the coefficient matrix 'A', the inhomogeneity (column)
%vector 'b',the seed (column) vector 'x0' for the iteration process,
%the tolerance 'tol' which will cause the iteration to stop if the
%2-norms of differences of. successive iterates becomes smaller than 
%'tol', and 'kmax' that is the maximum number of iterations to
%perform.
%Outputs: the final iterate 'x', the number of iterations performed
%'k' and a vector 'diff' that records the 2-norms of successive
%differences of iterates.
%if either of the lase three input variables are not specified,
%default values of x0-zero column vector, tol-le-10 and kmax=100 
%are used.

%assign default input variables, as necessary
if nargin<3, xO=zeros (size (b) ) ; end
if nargin<4, tol=le-10; end
if nargin<5, kmax=100; end

if min(abs(diag(A)))<eps
	error('Coefficient matrix has zero diagonal entries, iteration 
							cannet be performed.\r')
end

[n m]=size(A); 
x=x0; 
k=l; diff=[]; 

while k<=kmax 
	norm=0; 
	for i=l:n 
		oldxi=x(i); x(i)=b(i); 
		for j=[l:i-l i+l:n] 
			x(i)=x(i)-A(i,j)*x(j); 
		end 
		x(i)-x(i)/A(i,i); 
		norm=norm+(oldxi-x(i))^2;
	end 
	diff(k)=sqrt(norm); 
	if diff(k)<tol
			fprintf('Gauss-Seidel iteration has converged in %d 
							iterations/r,k)
			return 
	end
	k=k+1;
end
 fprintf('Gauss-Seidel iteration failed to converge./r') 

\end{lstlisting}
\textbf{EXAMPLE 7.27:} For the linear system of the last example, apply Gauss-Seidel iteration with initial iterate being the zero vector and the same tolerance as that used for the last example. Find the number of iterations that are now required for convergence and compare the absolute 2-norm error of the final iterate with that for the last example.
\\
\\
SOLUTION: Reentering, if necessary, the data from the last example, create corresponding data for the Gauss-Seidel iteration using the preceding M-file:
\begin{lstlisting}[frame=none,numbers=none]
>>[xGS, kGS, diffGS] = gaussseidel(A,b,zeros(size(b)),le-6); 
->Gauss-Seidel iteration has converged in 17 iterations
\end{lstlisting}
Thus with the same amount of work per iteration, Gauss-Seidel has done the job in only 17 versus 26 iterations for Jacobi.
\\
\\
Looking at the absolute error of the Gauss-Seidel approximation,
\begin{lstlisting}[frame=none,numbers=none]
>>norm(xGS-[l -2 4]' , 2)     ->ans = 1.4177e - 007 
\end{lstlisting}
we see it certainly meets our tolerance goal of le-6 (and, in fact, is smaller than that for the Jacobi iteration).
\\

The Gauss-Seidel scheme can be extended to include a new parameter, $\omega$, that will allow the next iterate $x^{(k+1)}$ to be expressed as a linear combination of the current iterate $x^{(k)}$ and the Gauss-Seidel values given by (58). This gives a family of iteration schemes, collectively known as SOR (successive over relaxation) whose iteration schemes are given by the following formula:
\\
\\
\textbf{SOR Iteration:} 
\begin{equation}
x_{i}^{(k+1)}=\frac{\omega}{a_{i i}}\left[b_{i}-\sum_{j=1}^{i-1} a_{i j} x_{j}^{(k+1)}-\sum_{j=i+1}^{n} a_{i j} x_{j}^{(k)}\right]+(1-\omega) x_{i}^{(k)} \quad(1 \leq i \leq n)
\end{equation}
The parameter $\omega$, called the relaxation parameter, controls the proportion of the Gauss-Seidel update versus the current iterate to use in forming the next iterate. We will soon see that for SOR to converge, we will need the relaxation parameter to satisfy $0<\omega<2$. Notice that when $\omega=1$, SOR reduces to Gauss-Seidel. For certain values of $\omega$, SOR can accelerate the convergence realized in Gauss-Seidel.
\\

With a few changes to the Program 7.7, a corresponding M-file for SOR is easily created. We leave this for the next exercise for the reader.

With a few changes to the Program 7.7, a corresponding M-file for SOR is easily created. We leave this for the next exercise for the reader.
\\
\\
EXERCISE FOR THE READER 7.28: (a) Write a function M-file, \texttt{$[x, k$, diff $]=$ sorit $(A, b$, omega, $x 0$, tol, kmax $)$}, that performs the SOR iteration on the linear system $A x=b$. The inputs are the coefficient matrix A, the inhomogeneity (column) vector b, the relaxation parameter omega, the seed (column) vector $\times 0$ for the iteration process, the tolerance \texttt{tol}, which will cause the iteration to stop if the 2-norms of successive iterates become smaller than \texttt{tol}, and \texttt{kmax}, the maximum number of iterations to perform. The outputs are the final iterate $x$, the number of iterations performed $k$, and a vector \texttt{diff} that records the 2-norms of successive differences of iterates. If the last three input variables are not specified, default values of $\times 0=$ the zero column vector, \texttt{tol} = $1 \mathrm{e}-10$, and $\mathrm{kmax}=100$ are used.
\\
(b) Apply the program to recover the solution obtained in Example 7.27.\\
(c) If we use $\omega=0.9$, how many iterations would the SOR iteration need to converge?
\\
\\
\textbf{EXAMPLE 7.28:} Run a set of SOR iterations by letting the relaxation parameter run from $0.05$ to $1.95$ in increments of $0.5$. Use a tolerance for error $=1 \mathrm{e}-6$, but set $k \max =1000$. Record the number of iterations needed for convergence (if there is convergence) for each value of the $\omega$ (up to 1000 ) and plot this number as a function of $\omega$.
\\
\\
SOLUTION: We can use the M-file sorit of Exercise for the Reader $7.28$ in conjunction with a loop to easily obtain the needed data.
\begin{lstlisting}[frame=none,numbers=none]
>> omega=0.05:.05:1.95; 
>> length (omega) ->ans = 39 
>> for i=l:39 
[xSOR, kSOR(i), diffSOR] = sorit(A,b,omega(i),zeros(size(b)),.. . 
			    				le-6,1000); 
end 
\end{lstlisting}

The above loop has overwritten all but the iteration counters, which were recorded as a vector. We use this vector to locate the best value (from among those in our vector omega) to use in SOR.
\begin{lstlisting}[frame=none,numbers=none]
>> [mink ind]=min(kSOR) 				->mink = 9, ind = 18 
>> omega (18) 				->ans = 0.9000
\end{lstlisting}
\begin{multicols}{2}
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{img_7}\caption{Graph of the number of iterations required for convergence (to a tolerance of $(e-6$ ) using SOR iteration as a function of the relaxation parameter $\omega$. The $k$-values are truncated at 1000 . Notice from the graph that the convergence for GaussSeidel $(\omega=1)$ can be improved.
}
\end{figure}
\columnbreak
\noindent
Thus we see that the best value of $\omega$ to use (from those we tested) is $\omega=0.9$, which requires only nine iterations in the SOR scheme, nearly a $50 \%$ savings over Gauss-Seidel. The next two commands will produce the desired plot of the required number of iterations needed in SOR versus the value of the parameter $\omega$. The resulting plot is shown in Figure $7.40$.
\begin{lstlisting}[frame=none,numbers=none]
>> plot(omega, kSOR),
>> axis([0 2 0 1000])
\end{lstlisting}
Figure $7.41$ gives a plot that compares the convergences of three methods: Jacobi, Gauss-Seidel, and SOR (with our pseudo-optimal value of $\omega$ ). The next exercise for the reader will ask to reproduce this plot.
\end{multicols}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{img_8}\caption{Comparison of the errors versus the number of iterations for each of the three iteration methods: Jacobi (o), Gauss-Seidel (*), and SOR (x).
}
\end{figure}
EXERCISE FOR THE READER 7.29: Use MATLAB to reproduce the plot of Figure 7.41. The key in the upper-right corner can be obtained by using the "Data Statistics" tool from the "Tools" menu of the MATLAB graphics window once the three plots are created.
\\

Of course, even though the last example has shown that SOR can converge faster than Gauss-Seidel, the amount of work required to locate a good value of the parameter greatly exceeded the actual savings in solving the linear system of Example 7.26. In the SOR iteration the value of $\omega=0.9$ was used as the relaxation parameter.
\\

There is some interesting research involved in determining the optimal value of $\omega$ to use based on the form of the coefficient matrix. What is needed to prove such results is to get a nice formula for the eigenvalues of the matrix (in general an impossible problem, but for special types of matrices one can get lucky) and then compute the value of $\omega$ for which the corresponding maximum absolute value of the eigenvalues is as small as possible. A good survey of the SOR method is given in [You-71]. A sample result will be given a bit later in this section (see Proposition 7.14).
\\

We now present a general way to view iteration schemes in matrix form. From this point of view it will be a simple matter to specialize to the three forms we gave above. More importantly, the matrix notation will allow a much more natural way to perform error analysis and other important theoretical tasks.
\\

To cast iteration schemes into matrix form, we begin by breaking the coefficient matrix $A$ into three pieces:
\begin{equation}
A=D-L-U
\end{equation}
\noindent
where $D$ is the diagonal part of $A, L$ is (strictly) lower triangular and $U$ is the (strictly) upper triangular. In long form this (60) looks like:
$$
A=\left[\begin{array}{ccccc}
a_{11} & & & & \\
& a_{22} & & 0 & \\
& & a_{33} & & \\
& 0 & & \ddots & \\
& & & & a_{n n}
\end{array}\right]
-
\left[\begin{array}{ccccc}
0 & & & & \\
-a_{21} & 0 & & 0 & \\
-a_{31} & -a_{32} & 0 & & \\
& & & \ddots & \\
-a_{n 1} & -a_{n 2} & -a_{n 3} & \cdots & 0
\end{array}\right]
-
\left[\begin{array}{ccccc}
0 & -a_{12} & -a_{13} & \cdots & -a_{1 n} \\
& 0 & -a_{23} & \cdots & -a_{2 n} \\
& & 0 & \ddots & \vdots \\
& 0 & & \ddots & -a_{n-1, n}\\
&   & &        & 0\\
\end{array}\right]
$$
\\

This decomposition is actually quite simple. Just take $D$ to be the diagonal matrix with the diagonal entries equal to those of $A$, and take $L / U$ to be, respectively, the strictly lower/upper triangular matrix whose nonzero entries are the opposites of the corresponding entries of $A$.
\\

Next, we will examine the following general (matrix form) iteration scheme for solving the system (54) $A x=b$ :
\begin{equation}
B x^{(k+1)}=(B-A) x^{(k)}+b,
\end{equation}
where $B$ is an invertible matrix that is to be determined. Notice that if $B$ is chosen so that this iteration scheme produces a convergent sequence of iterates: $x^{(k)} \rightarrow \tilde{x}$, then the limiting vector $\tilde{x}$ must solve (54). (Proof: Take the limit in (61) as $k \rightarrow \infty$ to get $B \tilde{x}=(B-A) \tilde{x}+b=B \tilde{x}-A \tilde{x}+b \Rightarrow A \tilde{x}=b$.) The matrix $B$ should be chosen so that the linear system is easy to solve for $x^{(k+1)}$ (in fact, much easier than our original system $A x=b$ lest this iterative scheme would be of little value) and so that the convergence is fast.
\\

To get some idea of what sort of matrix $B$ we should be looking for, we perform the following error analysis on the iterative scheme (61). We mathematically solve $(61)$ for $x^{(k+1)}$ by left multiplying by $B^{-1}$ to obtain:
$$
x^{(k+1)}=B^{-1}(B-A) x^{(k)}+B^{-1} b=\left(I-B^{-1} A\right) x^{(k)}+B^{-1} b .
$$
Let $x$ denote the exact solution of $A x=b$ and $e^{(k)}=x^{(k)}-x$ denote the error vector of the $k$ th iterate. Note that $-\left(I-B^{-1} A\right) x=-x+B^{-1} b$. Using this in conjunction with the last equation, we can write:
$$
\begin{aligned}
e^{(k+1)}=x^{(k+1)}-x &=\left(I-B^{-1} A\right) x^{(k)}+B^{-1} b-x \\
&=\left(I-B^{-1} A\right) x^{(k)}-\left(I-B^{-1} A\right) x \\
&=\left(I-B^{-1} A\right)\left(x^{(k)}-x\right) \\
&=\left(I-B^{-1} A\right) e^{(k)}
\end{aligned}
$$
We summarize this important error estimate:
\begin{equation}
e^{(k+1)}=\left(I-B^{-1} A\right) e^{(k)}
\end{equation} 
From (62), we see that if the matrix $\left(I-B^{-1} A\right)$ is "small" (in some matrix norm), then the errors will decay as the iterations progress.\footnote{It is helpful to think of the one-dimensional case, where everything in (62) is a number. If $\left(I-B^{-1} A\right)$ is less than one in absolute value, then we have exponential decay, and furthermore, the decay is faster when absolute values of the matrix are smaller. This idea can be made to carry over to matrix situations. The corresponding needed fact is that all of the eigenvalues (real or complex) of the matrix $\left(I-B^{-1} A\right)$ are less than one in absolute value. In this case, it can be shown that we have exponential decay also for the iterative scheme, regardless of the initial iterate. For complete details, we refer to [Atk-89] or to [GoVL-83].
} But this matrix will be small if $B^{-1} A$ is "close" to $I$, which in turn will happen if $B^{-1}$ is "close" to $A^{-1}$ and this translates to $B$ being close to $A$.
\\
\\
Table $7.1$ summarizes the form of the matrix $B$ for each of our three iteration schemes introduced earlier. We leave it as an exercise to show that with the matrices given in Table $7.1,(61)$ indeed is equivalent to each of the three iteration schemes presented earlier (Exercise 20).
\\
\\
\textbf{TABLE 7.1:} Summary of matrix formulations of each of the three iteration schemes: Jacobi, Gauss-Seidel, and SOR.
\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{img_9}
\end{figure}
\noindent
Thus far we have done only experiments with iterations. Now we turn to some of the theory.
\\
\\
\textbf{THEOREM 7.10:} (Convergence Theorem) Assume that $A$ is a nonsingular (square) matrix, and that $B$ is any nonsingular matrix of the same size as $A$. The (real and complex) eigenvalues of the matrix $I-B^{-1} A$ all have absolute value less than one if and only if the iteration scheme $(61)$ converges (to the solution of $A x=b)$ for any initial seed vector $x^{(0)}$.
\\

For a proof of this and the subsequent theorems in this section, we refer the interested reader to the references [Atk-89] or to [GoVL-83]. MATLAB's eig function is designed to produce the eigenvalues of a matrix. Since, as we have pointed out (and seen in examples), the Gauss-Seidel iteration usually converges faster than the Jacobi iteration, it is not surprising that there are examples where the former will converge but not the latter (see Exercise 8). It turns out that there are examples where the Jacobi iteration will converge even though the GaussSeidel iteration will fail to converge.
\\
\\
\textbf{EXAMPLE 7.29:} Using MATLAB's \texttt{eig} function for finding eigenvalues of a matrix, apply Theorem $7.10$ to check to see if it tells us that the linear system of Example $7.26$ will always lead to a convergent iteration method with each of the three schemes: Jacobi, Gauss-Seidel, and SOR with $\omega=0.9$. Then create a plot of the maximum absolute value of the eigenvalues of $\left(I-B^{-1} A\right)$ for the SOR method as $\omega$ ranges from $0.05$ to $1.95$ in increments of $0.5$, and interpret.
\\
\\
SOLUTION: We enter the relevant matrices into a MATLAB session: 
\begin{lstlisting}[frame=none,numbers=none]
>> A=[3 1 -1; 4 -10 1;2 1 5] ; D=diag(diag(A)); 
>> L=[0 0 0;- 4 0 0;- 2 - 1 0J; U=D-L-A; I = eye(3) ; 
>> % Jacobi 
>> max (abs (eig (I-inv (D) *A))) ->ans = 0.5374 
>> % Gauss Seide l 
>> max (abs (eig (I-inv (D-L) *A) ) ) ->ans = 0.3513 
>> % SOR onega - 0.9> 
>> max (abs (eig (I-inv (D/. 9-L) *A) ) ) -.ans = 0.1301		
\end{lstlisting}
The three computations give the maximum absolute values of the eigenvalues of $\left(I-B^{-1} A\right)$ for each of the three iteration methods. Each maximum is less than one, so the theorem tells us that, whatever initial iteration vector we choose for any of the three schemes, the iterations will always converge to the solution of $A x=b$. Note that the faster converging methods tend to correspond to smaller maximum absolute values of the eigenvalues of $\left(I-B^{-1} A\right) ;$ we will see a corroboration of this in the next part of this solution.
\\
\\
The next set of commands produces a plot of the maximum value of the eigenvalues of $\left(I-B^{-1} A\right)$ for the various values of $\omega$, which is shown in Figure $7.42$.
\begin{lstlisting}[frame=none,numbers=none]
>> omega=0.05:.05:1.95; 
>> for i=l:length(omega) 
	rad(i)=max(abs(eig(I-... 
	inv(D/omega(i)-L)*A))); 
end 
>> plot(omega, rad,'0-')
\end{lstlisting}
\begin{multicols}{2}
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{img_10}
\end{figure}
\columnbreak
\captionof{figure}{Illustration of the maximum absolute value of the eigenvalues of the matrix $I-B^{-1} A$ of Theorem $7.10$ for the SOR method (see Table 7.1) for various values of the relaxation parameter $\omega$. Compare with the corresponding number of iterations needed for convergence (Figure $7.40$ ).}
\end{multicols}
The above theorem is quite 
universal in that it applies to all 
situations. The drawback is that it 
relies on the determination of 
eigenvalues of a matrix. This 
eigenvalue problem can be quite a difficult numerical problem, especially for very 
large matrices (the type that we would want to apply iteration methods to). 
MATLAB's ei g function may perform unacceptably slowly in such cases and/or 
may produce inaccurate results. Thus, the theorem has limited practical value. 
We next give a more practical result that gives a sufficient condition for 
convergence of both the Jacobi and Gauss-Seidel iterative methods.
\\

Recall that an $n \times n$ matrix $A$ is \textbf{strictly diagonally dominant (by rows)} if the absolute value of each diagonal entry is greater than the sum of the absolute values of all other entries in the same row, i.e.,
$$
\left|a_{i i}\right|>\sum_{\substack{j=i \\ j \neq i}}^{n}\left|a_{i j}\right|, \text { for } i=1,2, \ldots, n.
$$
THEOREM 7.11: (Jacobi and Gauss-Seidel Convergence Theorem) Assume that $A$ is a nonsingular (square matrix). If $A$ is strictly diagonally dominant, then the Jacobi and Gauss-Seidel iterations will converge (to the solution of $A x=b$ ) for any initial seed vector $x^{(0)}$.
\\

Usually, the more diagonally dominant $A$ is, the faster the rate of convergence will be. Note that the coefficient matrix of Example $7.26$ is strictly diagonally dominant, so that Theorem $7.11$ tells us that no matter what initial seed vector $x^{(0)}$ we started with, both the Jacobi and Gauss-Seidel iteration schemes would produce a sequence that converges to the solution. Although we knew this already from Theorem $7.10$, note that, unlike the eigenvalue condition, the strict diagonal dominance was trivial to verify (by inspection). There are matrices $A$ that are not strictly diagonally dominant but for which both Jacobi and Gauss-Seidel schemes will always converge. For an outline of a proof of the Jacobi part of the above result, see Exercise 23 .
\\

For the SOR method (for other values of $\omega$ than 1) there does not seem to be such a simple useful criterion. There is, however, another equivalent condition in the case of a symmetric coefficient matrix $A$ with positive diagonal entries. Recall that an $n \times n$ matrix $A$ is symmetric if $A=A^{\prime}$; also $A$ is positive definite provided that $x^{\prime} A x>0$ for any nonzero $n \times 1$ vector $x$.
\\
\\
\textbf{THEOREM 7.12:} (SOR Convergence Theorem) Assume that $A$ is a nonsingular (square matrix). Assume that $A$ is symmetric and has positive diagonal entries. For any choice of relaxation parameter $0<\omega<2$, the SOR iteration will converge (to the solution of $A x=b$ ) for any initial seed vector $x^{(0)}$, if and only if $A$ is positive definite.
\\

Matrices that satisfy the hypotheses of Theorems $7.11$ and $7.12$ are actually quite common in numerical solutions of differential equations. We give a typical example of such a matrix shortly. For reference, we collect in the following theorem two equivalent formulations for a symmetric matrix to be positive definite, along with some necessary conditions for a matrix to be positive definite. Proofs can be found in [Str-88], p. 331 (for the equivalences) and [BuFa-01], p. 401 (for the necessary conditions).
\\
\\
\textbf{THEOREM 7.13:} (Positive Definite Matrices) Suppose that $A$ is a symmetric $\boldsymbol{n} \times \boldsymbol{n}$ matrix.
\begin{enumerate}[label=(\alph*),align=left]
\item
The following two conditions are each equivalent to $A$ being positive definite:
\begin{enumerate}[label=(\roman*),align=left]
\item
All eigenvalues of $A$ are positive, or
\item
The determinants of all upper-left submatrices of $A$ have positive determinants.
\end{enumerate}
\item
If $A$ is positive definite, then each of the following conditions must hold:
\begin{enumerate}[label=(\roman*),align=left]
\item
$A$ is nonsingular.
\item
$a_{i i}>0$ for $i=1,2, \ldots, n$.
\item
$a_{i i} a_{i j}>a_{i j}^{2}$ whenever $i \neq j$.
\end{enumerate}
\end{enumerate}

We will be working next with a certain class of sparse matrices, which is typical of those that arise in finite difference methods for solving partial differential equations. We study such problems and concepts in detail in [Sta-05]; here we only very briefly outline the connection.
\\

The matrix we will analyze arises in solving the so-called Poisson boundary value problem on the two-dimensional unit square $\{(x, y): 0 \leq x, y \leq 1\}$, which asks for the determination of a function $u=u(x, y)$ that satisfies the following partial differential equation and boundary conditions:
$$
\left\{\begin{array}{l}
-\Delta u=f(x, y) \text {, inside the square: } 0<x, y<1 \\
u(x, y)=0, \text { on the boundary: } x=0,1 \text { or } y=0,1
\end{array}\right.
$$
Here $\Delta u$ denotes the Laplace differential operator $\Delta u=u_{x x}+u_{y y}$. The finite difference method "discretizes" the problem into a linear system. If we use the same number $N$ of grid points both on the $x$ - and the $y$-axis, the linear system $A x=b$ that arises will have the $N^{2} \times N^{2}$ coefficient matrix shown in (63).
\\

In our notation, the partition lines break the $N^{2} \times N^{2}$ matrix up into smaller $N \times N$ block matrices $\left(N^{2}\right.$ of them). The only entries indicated are the nonzero entries that occur on the five diagonals shown. Because of their importance in applications, matrices such as the one in (63) have been extensively studied in the context of iterative methods. For example, the following result contains some very practical and interesting results about this matrix.
\begin{equation}
\left[\begin{array}{c|c|c|c}
 \begin{array}{cccc}
4 & -1 & & \\
-1 & 4 & \ddots & \\
& \ddots & \ddots & -1 \\
& & -1 & 4
\end{array}& 
\begin{array}{cccc}
-1 & & & \\
& -1 & & \\
& & \ddots & \\
& & & -1
\end{array} &  
& \\
\hline 
\begin{array}{llll}
1 & & & \\
& -1 & & \\
& & \ddots & \\
& & & -1
\end{array} 
& 
\begin{array}{cccc}
4& -1& & \\
-1 & 4 & \ddots & \\
& \ddots & \ddots & -1 \\
& & -1 & 4
\end{array}
&
\begin{array}{llll}
\ddots& & & \\
 & \ddots & & \\
& & \ddots & \\
& & & \ddots
\end{array}
&   \\
\hline 
& 
\begin{array}{llll}
\ddots& & & \\
 & \ddots & & \\
& & \ddots & \\
& & & \ddots
\end{array}
& 
\begin{array}{llll}
\ddots& & & \\
 & \ddots & & \\
& & \ddots & \\
& & & \ddots
\end{array}
& 
\begin{array}{cccc}
-1 & & & \\
& -1 & & \\
& & \ddots & \\
& & & -1
\end{array}
\\
\hline  &  & \begin{array}{cccc}
-1 & & & \\
& -1 & & \\
& & \ddots & \\
& & & -1
\end{array} &   \begin{array}{cccc}
4 & -1 & & \\
-1 & 4 & \ddots & \\
& \ddots & \ddots & -1 \\
& & -1 & 4
\end{array} \\
\end{array}\right]
\end{equation}
\textbf{PROPOSITION 7.14:} Let $A$ be the $N^{2} \times N^{2}$ matrix (63).\\
(a) $A$ is positive definite (so SOR will converge by Theorem 7.12) and the optimal relaxation parameter $\omega$ for an SOR iteration scheme for a linear system $A x=b$ is as follows:
$$
\omega=\frac{2}{1+\sin \left(\frac{\pi}{N+1}\right)}
$$
(b) With this optimal relaxation parameter, the SOR iteration scheme works on order of $N$ times as fast as either the Jacobi or Gauss-Seidel iteration schemes. More precisely, the following quantities $R_{f}, R_{G s}, R_{\text {soe }}$ indicate the approximate number of iterations that each of these three schemes would need, respectively, to reduce the error by a factor of $1 / 10$ :
$$
R_{J} \approx 0.467(N+1)^{2}, R_{G S}=\frac{1}{2} R_{J} \approx 0.234(N+1)^{2} \text {, and } R_{\text {SOR }} \approx 0.367(N+1)
$$
\\

In our next example we compare the different methods by solving a very large fictitious linear system $A x=b$ involving the matrix (63). This will allow us to make exact error comparisons with the true solution.
\\

A proof of the above proposition, as well as other related results, can be found in Section $8.4$ of [StBu-93]. Note that since $A$ is not (quite) strictly diagonally dominant, the Jacobi/Gauss-Seidel convergence theorem (Theorem 7.11) does not apply. It turns out that the Jacobi iteration scheme indeed converges, along with SOR (and, in particular, the Gauss-Seidel method); see Section $8.4$ of [StBu-93].
\\

Consider the matrix $A$ shown in (63) with $N=50$. The matrix $A$ has size $2500 \times 2500$ so it has $6.25$ million entries. But of these only about $5 N^{2}=12,500$ are nonzero. This is about $0.2 \%$ of the entries, so $A$ is quite sparse.
EXERCISE FOR THE READER 7.30: Consider the problem of multiplying the matrix $A$ in (63) (using $N=50$ ) by the vector $x=\left[\begin{array}{lllllllll}1 & 2 & 1 & 2 & 1 & 2 & \cdots & 1 & 2\end{array}\right]^{\prime}$.\\
(a) Compute (by hand) the vector $b \equiv A x$ by noticing the patterns present in the multiplication.\\
(b) Get MATLAB to compute $b=A x$ by first creating and storing the matrices $A$ and $x$ and performing a usual matrix multiplication. Use \texttt{tic/toc} to time the parts of this computation.\\
(c) Store only the five nonzero diagonals of (as column vectors): \texttt{d,a1,aN b1,bN}( $d$ stands for main diagonal, $a$ for above-main diagonal, $b$ for below-main diagonal). Recompute $b$ by suitably manipulating these 5 vectors in conjunction with $x$ Use \texttt{tic/toc} to time the computation and compare with that in part (b).\\
(d) Compare all three answers. What happens to the three methods if we bump $N$ up to 100 ?
\\
\\
Shortly we will give a general development on the approach hinted at in part (c) of the above Exercise for the Reader 7.30.
\\

As long as the coefficient matrix $A$ is not too large to be stored in a session, MATLAB's left divide is quite an intelligent linear system solver. It has special more advanced algorithms to deal with positive definite coefficient matrices, as well as with other special types of matrices. It can numerically solve systems about as large as can be stored; but the accuracy of the numerical solutions obtained depends on the condition number of the matrix, as explained earlier in this chapter.\footnote{Depending on the power of the computer on which you are running MATLAB's as well as the other processes being run, computation times and storage capacities can vary. At the time of writing this section on the author's $1.6 \mathrm{MHz}, 256 \mathrm{MB}$ RAM Pentium IV PC, some typical limits, for random (dense) matrices, are as follows: The basic Gaussian elimination (Program 7.6) starts taking too long (toward an hour) when the size of the coefficient matrix gets larger than $600 \times 600$; for it to take less. than about one minute the size should be less than about $250 \times 250$. Before memory runs out, on the other hand, matrices of sizes up to about $6000 \times 6000$ can be stored, and MATLAB's left divide can usually (numerically) solve them in a reasonable amount of time (provided that the condition number is moderate). To avoid redundant storage problems, MATLAB does have capabilities of storing sparse matrices. Such functionality introduced at the end of this section. Taking advantage of the structure of sparse banded matrices (which are the most important ones in numerical differential equations) will enable us to solve many such linear systems that are quite large, say up to about $50,000 \times 50,000$. Such large systems often come up naturally in numerical differential equations.
} The next example shows that even with all that we know about the optimal relaxation parameter for the special matrix (63), MATLAB's powerful left divide will still work more efficiently for a very large linear system than our SOR program. After the example we will remedy the situation by modifying the SOR program to make it more efficient for such banded sparse matrices.
\\
\\
\textbf{EXAMPLE 7.30:} In this example we do some comparisons in some trial runs of solving a linear system $A x=b$ where $A$ is the matrix of $(63)$ with $N=50$, and the vectors $x$ and $b$ are as in the preceding Exercise for the Reader $7.30$. Having the exact solution will allow us to look at the exact errors resulting from any of the methods.\\
(a) Solve the linear system by using MATLAB's left divide (Gaussian elimination). Record the computation time and error of the computed solution.\\
(b) Solve the system using the Gauss-Seidel program gaussseidel (Program 7.7) with the default number of iterations and initial vector. Record the computation time and error. Repeat using 200 iterations.\\
(c) Solve again using the SOR program sorit (from Exercise for the Reader 7.28) with the optimal relaxation parameter $\omega$ given in Proposition 7.14. Record the computation time and error. Repeat using 200 iterations.\\
(d) Reconcile the data of parts (b) and (c) with the results of part (c) of Proposition $7.14$.
\\
\\
SOLUTION: We first create and store the relevant matrices and vectors:
\begin{lstlisting}[frame=none,numbers=none]
>> x=ones(2500,1); x(2:2:2500,1)=2; 
>> A=4*eye(2500); 
>> vl=-l*ones (49,1) ; vl=[vl;0]; %seed vector for sub/super diagonals 
>> secdiag=vl; 
>> for i = 1:49 
	if i<49 
		secdiag=[secdiag;vl] ; 
	else 
		secdiag=[secdiag;vl(1:49)] ; 
	end 
end 
>> A=A+diag(secdiag,1)+diag(secdiag,-1)-diag(ones(2450,1),50)... 
-diag(ones(2450,l),-50); 
>> b=A*x; 
\end{lstlisting}
Part (a):
\begin{lstlisting}[frame=none,numbers=none]
>> tic , xMATLAB=A\b; toc ->elapsed_time = 9.2180 
>> max(xMATLAB-x) ->ans = 6.2172e - 015
\end{lstlisting}
Part (b): 
\begin{lstlisting}[frame=none,numbers=none]
>> tic , [xGS, k, diffJ=gaussseidel(A,b); toc 
->Gauss-Seidel iteration failed to converge.->elapsed time = 181.6090 
>> max(abs(xGS-x)) ->ans = 1.4353 

>> tic , [xGS2, k, diff] = gaussseidel(A,b,zeros(size(b)), le-10,200); 
toe 
->Gauss-Seidel iteration failed to converge.->elapsed_time = 374.5780 
>> max (abs (xGS2-x) )->ans = 1.1027
\end{lstlisting}
Part (c):
\begin{lstlisting}[frame=none,numbers=none]
>> tic , [xSORr k, diff]=sorit(A,b , 2/(1+sin(pi/51))) ; toc 
->SOR iteration failed to converge. ->elapsed_time = 186.7340 
>> max(abs(xSOR-x)) ->ans = 0.0031 

>> tic , [xSOR2, k, diff]=sorit(A,b , 2/(1+sin(pi/51)),.. . 
				zeros(size(b)) , le-10,200) ; toc 

->SOR iteration failed to converge, ->elapsed_time = 375.2650 
>> max(abs(xSOR2-x)) ->ans = 1.1885e - 008 
\end{lstlisting}
Part (d): The above data shows that our iteration programs pale in performance when compared with MATLAB's left divide (both in time and accuracy). The attentive reader will realize that both iteration programs do not take advantage of the sparseness of the matrix (63). They basically run through all of the entries in this large matrix for each iteration. One other thing that should be pointed out is that the above comparisons are somewhat unfair because MATAB's left divide is a compiled code (built-into the system), whereas the other programs were interpreted codes (created as external programs-M-files). After this example, we will show a way to make these programs perform more efficiently by taking advantage of the special banded structure of such matrices. The resulting modified programs will then perform more efficiently than MATLAB's left divide, at least for the linear system of this example.
\\

Using $N=50$ in part (b) of Proposition $7.14$, we see that in order to cut errors by a factor of 10 with the Gauss-Seidel method, we need approximately $0.234 \cdot 51^{2} \approx 609$ additional iterations, but for the SOR with optimal relaxation parameter the corresponding number is only $.367 .51 \approx 18.7$. This corroborates well with the experimental data in parts (b) and (c) above. For Gauss-Seidel, we first used 100 iterations and then used 200 . The theory tells us we would need over 600 more iterations to reduce the error by $90 \%$. Using 100 more iterations resulted in a reduction of error by about $23 \%$. On the other hand, with SOR, the 100 additional iterations gave us approximately $100 / 18.7 \approx 5.3$ reductions in the errors each by factors of $1 / 10$, which corresponds nicely to the (exact) error shrinking from about $3 \mathrm{e}-3$ to $1 \mathrm{e}-8$ (literally, about $5.3$ decimal places!).
\\

In order to take advantage of sparsely banded matrices in our iteration algorithms, we next record here some elementary observations regarding multiplications of such matrices by vectors. MATLAB is enabled with features to easily manipulate and store such matrices. We will now explore some of the underlying concepts and show how exploiting the special structure of some sparse matrices can greatly expand the sizes of linear systems that can be effectively solve. MATLAB has its own capability for storing and manipulating general sparse matrices; at the end of the section we will discuss how this works.
\\

The following (nonstandard) mathematical notations will be convenient for the present purpose: For two vectors $v, w$ that are both either row-or columnvectors, we let $v \otimes w$ denote their juxtaposition. For the pointwise product of two vectors of the same size, we use the notation $v \odot w=\left[v_{i} w_{i}\right]$. So, for example, if $v=\left[\begin{array}{lll}v_{1} & v_{2} & v_{3}\end{array}\right]$, and $w=\left[\begin{array}{lll}w_{1} & w_{2} & w_{3}\end{array}\right]$ are both $3 \times 1$ row vectors, then $v \otimes w$ is the $6 \times 1$ row vector $\left[\begin{array}{llllll}v_{1} & v_{2} & v_{3} & w_{1} & w_{2} & w_{3}\end{array}\right]$ and $v \odot w$ is the $3 \times 1$ row vector $\left[v_{1} w_{1} v_{2} w_{2} v_{3} w_{3}\right]$. We also use the notation $0_{n}$ to denote the zero vector with $n$ components. We will be using this last notation only in the context of juxtapositions so that whether $0_{n}$ is meant to be a row vector or a column vector will be clear from the context.\footnote{We point out that these notations are not standard. The symbol $\otimes$ is usually reserved for the socalled tensor product.
}
\\
\\
LEMMA 7.15: (a) Let $S$ be an $n \times n$ matrix whose $k$ th superdiagonal $(k \geq 1)$ is made of the entries (in order) of the $(n-k) \times 1$ vector $v$, i.e.,
$$
S=\left[\begin{array}{ccccccccc}
0 & 0 & \cdots & 0 & v_{1} & 0 & & \cdots & 0 \\
0 & 0 & 0 & \cdots & 0 & v_{2} & 0 & & \vdots \\
\vdots & \vdots & 0 & \ddots & \cdots & 0 & v_{3} & \ddots & 0 \\
& & \vdots & 0 & & \ddots & \ddots & \ddots & 0 \\
\vdots & & & & \ddots & & & 0 & v_{n-k} \\
\vdots & & & & & \ddots & & & 0 \\
0 & \vdots & \vdots & & & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & 0 & \cdots & \cdots & 0 & 0 & 0
\end{array}\right],
$$
where $v=\left[\begin{array}{llll}v_{1} & v_{2} & \cdots & v_{n-k}\end{array}\right]$. (In MATLAB's notation, the matrix $S$ could be entered as diag $(v, k)$, once the vector $v$ has been stored.) Let $x=\left[\begin{array}{lllll}x_{1} & x_{2} & x_{3} & \cdots & x_{n}\end{array}\right]^{\prime}$ be any $n \times 1$ column vector. The following relation then holds:
\begin{equation}
S x=\left(v \otimes 0_{k}\right) \odot\left(\left[\begin{array}{llll}
x_{k+1} & x_{k+2} & \cdots & x_{n}
\end{array}\right] \otimes 0_{k}\right)
\end{equation}
(b) Analogously, if $S$ is an $n \times n$ matrix whose $k$ th subdiagonal $(k \geq 1)$ is made of the entries (in order) of the $(n-k) \times 1$ vector $v$, i.e.,
$$
S=\left[\begin{array}{ccccccccc}
0 & 0 & \cdots & & & & \cdots & 0 & 0 \\
\vdots & 0 & 0 & \cdots & & & & & 0 \\
0 & \vdots & 0 & 0 & \cdots & & & & \vdots \\
v_{1} & 0 & \vdots & 0 & 0 & \cdots & & & \\
0 & v_{2} & 0 & & \ddots & \ddots & & & \\
\vdots & 0 & v_{3} & 0 & & \ddots & & & \\
\vdots & & \ddots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\
0 & \vdots & \vdots & 0 & v_{n-k-1} & 0 & 0 & \cdots & 0 \\
0 & 0 & 0 & 0 & 0 & v_{n-k} & 0 & \cdots & 0
\end{array}\right],
$$
where $v=\left[\begin{array}{llll}v_{1} & v_{2} & \cdots & v_{n-k}\end{array}\right]$ and $x=\left[\begin{array}{lllll}x_{1} & x_{2} & x_{3} & \cdots & x_{n}\end{array}\right]^{\prime}$ is any $n \times 1$ column vector, then
\begin{equation}
S x=\left(0_{k} \otimes v\right) \odot\left(0_{k} \otimes\left[\begin{array}{llll}
x_{1} & x_{2} & \cdots & x_{k}
\end{array}\right]\right)
\end{equation}

The proof of the lemma is left as Exercise 21 . The lemma can be easily applied to greatly streamline all of our iteration programs for sparse banded matrices. The next exercise for the reader will ask the reader to perform this task for the SOR iteration scheme.
\\
\\
EXERCISE FOR THE READER 7.31:
(a) Write a function M-file with the following syntax:
\\
$$\texttt{[x,k,diff] = sorsparsediag(diags, inds, b, omega, xO, tol, kmax)}$$
that will perform the SOR iteration to solve a nonsingular linear system $A x=b$ in which the coefficient matrix $A$ has entries only on a sparse set of diagonals. The first two input variables are \texttt{diags}, an $n \times j$ matrix where each column consists of the entries of $A$ on one of its diagonals (with extra entries at the end of the column being zeros), and \texttt{inds}, a $1 \times j$ vector of the corresponding set of indices for the diagonals (index zero corresponds to the main diagonal and should be first). The remaining input and output variables will be exactly as in the M-file \texttt{sorit} of Exercise for the Reader 7.28. The program should function just like the \texttt{sorit} M-file, with the only exceptions being that the stopping criterion for the norms of the difference of successive iterates should now be determined by the infinity norm\footnote{The infinity norm of a vector $x$ is simply, in MATLAB's notation, $\max ($ abs $(x))$. This is a rather superficial change in the M-file, merely to allow easier performance comparisons with MATLAB's left divide system solver.
} and the default number of iterations is now 1000 . The algorithm should, of course, be based on formula (59) for the SOR iteration, but the sum need only be computed over the index set (\texttt{inds}) of the nonzero diagonals. To this end, the above lemma should be used in creating this M-file so that it will avoid unnecessary computations (with zero multiplications) as well as storage problems with large matrices.
(b) Apply the program to redo part (c) of Example 7.30.
\\
\\
\textbf{EXAMPLE 7.31:} (a) Invoke the M-file \texttt{sorsparsediag} of the preceding exercise for the reader to obtain SOR numerical solutions of the linear system of Example $7.30$ with error goal $5 \mathrm{e}-15$ (roughly.MATLAB's machine epsilon) and compare the necessary runtime with that of MATLAB's left divide, which was recorded in that example.
(b) Next use the program to solve the linear system $A x=b$ with $A$ as in (63) with $N=300$ and $b=\left[\begin{array}{lllllll}1 & 2 & 1 & 2 & \ldots & 1 & 2\end{array}\right]^{\prime}$. Use the default tolerance $1 \mathrm{e}-10$, then, looking at the last norm difference (estimate for the actual error) use Proposition $7.14$ to help to see how much to increase the maximum number of iterations to ensure convergence of the method. Record the runtimes. The size of $A$ is $90,000 \times 90,000$ and so it has over 8 billion entries. Storage of such a matrix would require a supercomputer.
\\
\\
\textbf{SOLUTION:} Part (a): We first need to store the appropriate data for the matrix $A$. Assuming the variables created in the last example are still in our workspace, this can be accomplished as follows:
\begin{lstlisting}[frame=none,numbers=none]
>> diags=zeros(2500,5); 
>> diags (:,l)=4*ones(2500,1); 
>> diags(1:2499,2:3)=[secdiag secdiag]; 
>> diags(l:24 50,4:5)=-ones(2450,2); 
>> inds=[0 1 -1 50 -50]; 

>> tic, [xSOR, k, diff]=sorsparsediag(diags, inds,b,... 
					2/(l+sin(pi/51)) , zeros(size(b)), 5e-15); toc 
->SOR iteration has converged in 308 iterations 	->elapsed_time = 1.3600 
>> max(abs(xSOR-x)) 	->ans = 2.3537e - 014
\end{lstlisting}
Our answer is quite close to machine precision (there were roundoff errors) and the answer obtained by MATLAB's left divide. The runtime of the modified SOR program is now, however, significantly smaller than that of the MATLAB solver. We will see later, however, that when we store the matrix $A$ as a sparse matrix (in MATLAB's syntax), the left divide method will work at comparable speed to our modified SOR program.
\\
\\
Part (b): We first create the input data by suitably modifying the code in part (a):
\\
\\
\begin{lstlisting}[frame=none,numbers=none]
>> b=ones(90000, 1); b(2:2:90000,1)=2; 
>> vl=-l*ones(299, 1); vl=[vl;0]; %seed vector for sub/super diagonals 
>> secdiag=vl; 
>> for i=l:299 
	if i<299 
		secdiag=[secdiag; vl]; 
	else 
		secdiag=[secdiag;vl(1:299)]; 
	end 
end 
>> diags=zeros(90000,5); 
>> diags(:,l)=4*ones(90000,1) ; 
>> diags(1:89999,2:3)=[secdiag secdiag]; 
>> diags(l:89700,4:5)= [-ones(89700,1) -ones(89700,1)]; 
>> inds=[0 1 -1 300 -300]; 
>> tic, [xSORbig, k, diff] =sorsparsediag (diags, inds,b, ... 
		2/(l+sin(pi/301))) ; toc 
->SOR iteration failed to converge, ->elapsed_time = 167.0320 
>> diff(k-l ) ->ans = 1.3845e-005 
\end{lstlisting}
We need to reduce the current error by a factor of $1 \mathrm{e}-5$. By Proposition $7.14$, this means that we should bump up the number of iterations by a bit more than $5 R_{\text {SOR }} \approx 5 \cdot 0.367 \cdot 301 \approx 552$. Resetting the default number of iterations to be 1750 (from 1000 ) should be sufficient. Here is what transpires:
\begin{lstlisting}[frame=none,numbers=none]
>> tic, [xSORbig, k, diff]=sorsparsediag(diags, inds,b,... 
				2/(l+sin(pi/301)), zeros(size(b)), le-10, 1750); toc
->SOR iteration has converged in 1620 iterations ->elapsed_time = 290.1710 
>> diff(k -1) ->ans = 1.0550e - 010
\end{lstlisting}
We have thus solved this extremely large linear system, and it only took about three minutes!
\\

As promised, we now give a brief synopsis of some of MATLAB's built-in, state-of-the-art iterative solvers for linear systems $A x=b$. The methods are based on more advanced concepts that we briefly indicated and referenced earlier in the section. Mathematical explanations of how these methods work would lie outside the focus of this book. We do, however, outline the basic concept of \textbf{preconditioning}. As seen early in this section, iterative methods are very sensitive to the particular form of the coefficient matrix (we gave an example where simply switching two rows of $A$ resulted in the iterative method diverging when it originally converged). An invertible matrix (usually positive definite) $M$ is used to precondition our linear system when we apply the iterative method instead to the equivalent system: $M^{-1} A x=M^{-1} b$. Often, preconditioning a system can make it more suitable for iterative methods. For details on the practice and theory of preconditioning, we refer to Part II of [Gre-97], which includes, in particular, preconditioning techniques appropriate for matrices that arise in solving numerical PDEs. See also Part IV of [TrBa-97].
\\

Here is a detailed description of MATLAB's function for the so-called p\textbf{reconditioned conjugate gradient method}, which assumes that the coefficient matrix $A$ is symmetric positive definite.
$$
\begin{array}{|l|l|l|}
\hline
& \text{Performs the preconditioned gradient method to solve}\\ 
& \text { the linear system } A x=b \text {, where the } N \times N\\
& \text { coefficient matrix } A \text { must be symmetric positive }\\
& \text { definite and the preconditioner } M=M 1 * M 2 \text {. Only }\\
\texttt{x=pcg(A,b,tol,kmax,Ml,M2 , xO)}\rightarrow& \text { the first two input variables are required; any tail }\\
& \text { sequence of input variables can be omitted. The }\\
& \text { default values of the optional variables are as follows: }\\
& \texttt{tol}\text{=le-6}, \operatorname{kmax}=\min (N, 20), M 1=M 2=I\\
& \text { (identity matrix), and } x 0=\text { the zero vector. Setting }\\
& \text { any of these optional input variables equal to [ ] }\\
& \text { gives them their default values. }\\
\hline
\end{array}
$$

$$
\begin{array}{|l|l|l|}
\hline
& \text{Works as above but returns additional output \texttt{flag:}}\\
& \text{\texttt{flag = 0} means \texttt{pcg} converged to the desired}\\
\texttt{[x,flag]}& \text{tolerance \texttt{tol} within \texttt{kmax} iterations; \texttt{flag = 1}} \\
\texttt{=pcg(A,b,tol,kmax,Ml,M2,xO)}\rightarrow & \text{means \texttt{pcg} iterated \texttt{kmax} times but did not converge. } \\
& \text{For a detailed explanation of other flag values, type } \\
& \texttt{help peg.} \\
\hline
\end{array}
$$
We point out that with the default values $M 1=M 2=I$, there is no conditioning and the method is called the conjugate gradient method.
\\

Another powerful and and more versatile method is the \textbf{generalized minimum residual method (GMRES)}. This method works well for general (nonsymmetric) linear systems. MATLAB's syntax for this function is similar to the above, but there is one additional (optional) input variable:

$$
\begin{array}{|l|l|l|}
\hline
& \text{Performs the generalized minimum residual method}\\
& \text{to solve the linear system } Ax = b,\text{ with }\\
\texttt{x=gmres(A,b,restart,tol,} & \text{preconditioner\texttt{ M = M1*M2}. Only the first two input }\\
\texttt{kmax,Ml,M2,xO)}\rightarrow& \text{variables are required; any tail sequence of input}\\
& \text{variables can be omitted. The default values of the}\\
& \text{optional variables are as follows: restart = [ ] }\\
& \text{(unrestarted method) tol = le-6, kmax = min(N,20),}\\
& \text{\texttt{M1 = M2 = I} (identity matrix), and JCO = the zero }\\
& \text{vector. Setting any of these optional input variables }\\
& \text{equal to [ ], gives them their default values. An}\\
& \text{optional second output variable \texttt{flag} will function in}\\
& \text{a similar fashion as with \texttt{pcg}. }\\
\hline
\end{array}
$$
\textbf{EXAMPLE 7.32:} (a) Use pcg to resolve the linear system of Example 7.30, with the default settings and flag. Repeat by resetting the tolerance at $1 e-15$ and the maximum number of iterations to be 100 and then 200 . Record the runtimes and compare these and the errors to the results for the SOR program of the previous example.
\\
(b) Repeat part (a) with gmres. 
\\
\\
SOLUTION: Assume that the matrices and vectors remain in our workspace (or recreate them now if necessary). We need only follow the above syntax instructions for \texttt{pcg}:
\\
\\
Part (a): 
\begin{lstlisting}[frame=none,numbers=none]
>> tic , [xpeg, flagpcg]=pcg(A,b) ; toc		->elapsed_time = 3.2810
>> max(abs(xpeg-x))		->ans = 1.5007 
>> flagpcg 		->flagpcg = 1
\end{lstlisting}
The flag being $=1$ means after 20 iterations, pcg did not converge within tolerance (le-5), a fact that we knew from the exact error estimate.
\begin{lstlisting}[frame=none,numbers=none]		
>> tic , [xpeg, flagpcg]=pcg (A,b,5e-15, 100); toc		->elapsed_time = 15.8900  
>> max(abs(xpcg-x))		->ans = 4.5816e - 006
>> flagpcg		->flagpcg = 1 


>> tic, [xpeg, flagpcg)=pcg(A,b,5e-15, 200); toc		->elapsedjime = 29.7970 
>> flagpcg		->flagpcg = 0
>> max (abs(xpeg-x))		->ans = 3.2419 - 014 
\end{lstlisting}
The flag being $=0$ in this last run shows we have convergence. The max norm is a different one from the 2 -norm used in the M-file; hence the slight discrepancy. Notice the unconditioned conjugate gradient method converged in fewer iterations than did the optimal SOR method, and in much less time than the original sorit program. The more efficient \texttt{sorsparsediag} program, however, got the solution in by far the shortest amount of real time. Later, we will get a more equitable comparison when we store $A$ as a sparse matrix.
\\
Part (b): 
\begin{lstlisting}[frame=none,numbers=none]
>> tic , (xgmres, flaggmres]=gmres(A,b, [],[] , 200); toc
>> tic , [xgmres, f lagcfmres] =gmres (A,b) ; toc		->elapsed_time = 2.3280
>> max(abs(xgmres-x))		->ans = 1.5002 
>>flaggmres		->flaggmres = 1 

>> tic, [xgmres, flaggmres]=gmres(A,b, [],5e-15 , 100); toe
->elapsed_time = 37.1250
>> max (abs (xgmres-x))		->ans = 9.2037 - 013
\end{lstlisting}
The results for GMRES compare well with those for the preconditioned conjugate gradient method. The former method converges a bit more slowly in this situation. We remind the reader that the conjugate gradient method is ideally suited for positive definite matrices, like the one we are dealing with.
\\

Figure $7.43$ gives a nice graphical comparison of the relative speeds of convergence of the five iteration methods that have been introduced in this section. An exercise will ask the reader to reconstruct this MATLAB graphic.

\begin{figure}[H]
\includegraphics[width=1\textwidth]{img_11}
\caption{Comparison of the convergence speed of the various iteration methods in the solution of the linear system $A x=b$ of Example $7.30$ where the matrix $A$ is the matrix (63) of size $2500 \times 2500$. In the SOR method the optimal relaxation parameter of Proposition $7.14$ was used. Since we did not invoke any conditioning, the preconditioned conjugate gradient method is simply referred to as the conjugate gradient method. The errors were measured in the infinity norm.}
\end{figure}
In the construction of the above data, the program \texttt{sorsparsediag} was used to get the SOR data and, despite the larger number of iterations than GMRES and the conjugate gradient method, the SOR data was computed more quickly. The \texttt{sorsparsediag} program is easily modified to construct similar programs for the Jacobi and Gauss-Seidel iterations (of course Gauss-Seidel could simply be done by setting $\omega=0$ in the SOR program), and such programs were used to get the data for these iterations. Note that the GMRES and conjugate gradient methods take several iterations before errors start to decrease, unlike the SOR method, but they soon catch up. Note also the comparable efficiencies between the GMRES and conjugate gradient methods.
\\

We close this chapter with a brief discussion of how to store and manipulate sparse matrices directly with MATLAB. Sparse matrices in MATLAB can be stored using three vectors: one for the nonzero entries, the other two for the corresponding row and column indices. Since in many applications sparse matrices will be banded, we will explain only a few commands useful for the creation and storage of such sparse matrices. Enter \texttt{help sparse} for more detailed information. To this end, suppose that we have an $n \times n$ banded matrix $A$ and we wish to store it as a sparse matrix. Let the indices corresponding to the nonzero bands (diagonals) of $A$ have numbers stored in a vector $d$ (so the size of $d$ is the number of bands, 0 corresponds to the main diagonal, positive numbers mean above the main diagonal, negative numbers mean below). Letting $p$ denote the length of the vector $d$ we form a corresponding $n \times p$ matrix, \texttt{Diags}, containing as its columns the corresponding bands (diagonals) of $A$. When columns are longer than the bands they replace (this will be the case except for main diagonal), super (above) diagonals should be put on the lower portion of \texttt{Diags} and sub (below) diagonals on the upper portion of \texttt{Diags}, with remaining entries on the column being set to zero.
$$
\begin{array}{|l|l|l|}
\hline
&\text { This command creates a sparse matrix data type } S, \text { of size } n \times n\\
\texttt{S=spdiags(Diags,d,n,n)}\rightarrow &\text { provided that \texttt{d} is a vector of diagonal indices (say there are } p \text { ), }\\
&\text { and \texttt{Diags} is the corresponding } n \times p \text { matrix whose columns }\\
&\text { are the diagonals of the matrix (arranged as explained above). }\\
\hline
&\text { Converts a sparse matrix data type back to its usual "full" form. }\\
\texttt{full(S)}\rightarrow&\text { This command is rarely used in dealing with sparse matrices as } \\
&\text{it defeats their purpose. }\\
\hline
\end{array}
$$
A simple example will help shed some light on how MATLAB deals with sparse data types. Consider the matrix $
A=\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
4 & 0 & 2 & 0 \\
0 & 5 & 0 & 3 \\
0 & 0 & 6 & 0
\end{array}\right].
$
The following MATLAB commands will store A as a sparse matrix: 

\begin{lstlisting}[frame=none,numbers=none]
>> d=[-l 1] ; Diags=[4 5 6 0; 0 1 2 3]';
>> S=spdiags(Diags,d , 4, 4) 
->=(2,1)		4	   (2,3)	  2
   (1.2) 	  1 	 (4,3)    6
   (3,2) 	  5    (3,4)    3 
\end{lstlisting}
The display shows the storage scheme. Let's compare with the usual form:
\begin{lstlisting}[frame=none,numbers=none]
>> full(S)
-> ans =  0 1 0 0 
          4 0 2 0 
          0 5 0 3 
          0 0 6 0
\end{lstlisting}

The key advantage of sparse matrix storage in MATLAB is that if $A$ is stored as a sparse matrix $S$, then to solve a linear system $A x=b$, MATLAB's left divide operation texttt{x=S $\backslash$ b} takes advantage of sparsity and can greatly increase the size of (sparse) problems we can solve. In fact, at most all of MATLAB's matrix functions are able to operate on sparse matrix data types. This includes MATLAB's iterative solvers texttt{pcg, gmres}, etc. We invite the interested reader to perform some experiments and discover the additional speed and capacity that taking advantage of sparsity can afford. We end with an example of a rematch of MATLAB's left divide against our \texttt{sorsparsediag} program, this time allowing the left divide method to accept a sparse matrix. The results will be quite illuminating.
\\
\\
\textbf{EXAMPLE 7.33:} We examine the large $(10,000 \times 10,000)$ system $A x=b$, where $A$ is given by (63) with $N=100$, and $x=\left(\begin{array}{lllll}1 & 1 & 1 & \cdots & 1\end{array}\right)^{\prime}$. By examining the matrix multiplication we see that
$$
b=A x=\left(\begin{array}{lllllllllllllllllllllll}
2 & 1 & 1 & \cdots & 2 \mid 1 & 0 & 0 & \cdots & 0 & 1 & \cdots & 1 & 0 & 0 & \cdots & 0 & 1 \mid & 2 & 1 & 1 & \cdots & 1 & 2
\end{array}\right)^{\prime} .
$$
We thus have a linear system with which we can easily obtain the exact error of any approximate solution.\\
(a) Solve this system using MATLAB's left divide and by storing $A$ as a sparse matrix. Use \texttt{tic/toc} to track the computation time (on your computer); compute the error as measured by the infinity norm (i.e., as the maximum difference of any component of the computed solution with the exact solution).\\
(b) Solve the system using the \texttt{sorsparsediag} M-file of Exercise for the Reader 7.31. Compute the time and errors as in part (a) and compare.
\\
\\
SOLUTION: Part (a): We begin by entering the parameters (for (63)), creating the needed inputs for \texttt{spdiags}, and then using the latter to store $A$ as a sparse matrix.
\begin{lstlisting}[frame=none,numbers=none]
>> N=200; n=N^2; d=[-N -1 0 1 N];, dia=4*ones(1,n); 
>> seed1=-1*ones(1,N-1); v1=[seed1 0]; 
for i=1:N-1, if i<N-1, v1 = (v1 [seed1 0]];, else, v1 = [vl seed1]; 
>> end, end 
>> b1=[v1 0]; a1=(0 v1]; %below/above 1 unit diagonals 
>> %Next here are the below/above N unit diagonals 
>> bN=[-ones(1,n-N) zeros(1,N)]; 
>> aN=[zeros(1,N) -ones(1,n-N) ]; 
>> %Now we can form the n by 5 Diags matrix. 
>> Diags=[bN; b1; dia; a1; aN]'; 
>> S=spdiags(Diags,d,n,n); %S is the sparsely stored matrix A 
>> %We use a simple iteration to contruct the inhomogeneity 
>> %vector b. 
>> bseedl=ones(1,N);, bseedl([1 N]) = [2 2]; % 2 end pieces 
>> bseed2=bseedl-ones (1,N) ; %N-2 middle pieces 
>> b=bseedl; for k=l:N-2, b=[b bseed2];, end, b=[b bseedl]; 
>> b=b'; 
>> tic, xLD=s\b;, toc 		->Elapsed time is 0.250000 seconds. 
>> x=ones(size(xLD)); 
>> max(x-xLD) 		-> ans =1.0947e-013 (Exact Error)
\end{lstlisting}
Part (b): The syntax and creation of input variables is just as we did in Example $7.31$.
\begin{lstlisting}[frame=none,numbers=none]
>> d=(0 -N N -1 1];, diags=zeros(n,5); 
>> diags(:,l)=dia; diags(:,2:3)=[bN bN']; diags(:,4:5)=[b1' b1' ]; 
>> tic, [xSOR, k, diff]=sorsparsediag(diags, d,b,... 
               2/(l+sin(pi/101))) ; toc 
->Elapsed time is 8.734000 seconds. 
>> max(x-xSOR)          s-> 3.9l02e-0l2 (ExactError)
\end{lstlisting}
Thus, now that the inputted data structures are similar, MATLAB's left divide has transcended our sorsparsediags program both in performance time and in accuracy. The reader is invited to perform further experiments with sparse matrices and MATLAB's iterative solvers.
\\
\rule{485pt}{2pt}
\subsubsection{EXERCISES 7.6:}\noindent

\begin{enumerate}
\item 
For each of the following data for a linear systern $A x=b$, perform the following iterations using the zero vector as the initial vector.\\
(a) Use Jacobi iteration until the error (as measured by the infinity norm of the difference of successive iterates) is less than le-10, if this is possible. In cases where the iteration does not converge, try rearranging the rows of the matrices to attain convergence (through all $n$ ! rearrangements, if necessary). Find the norm of the exact error (use MATLAB's left divide to get the "exact" solutions of these small systems).\\
(b) Repeat part (a) with the Gauss-Seidel iteration.
\settasks{
	counter-format=,
	label-width=4ex
}
\begin{tasks}(2)
\task (i) $
A=\left[\begin{array}{cc}
6 & -1 \\-1 & 6
\end{array}\right], \quad b=\left[\begin{array}{l}
1 \\2
\end{array}\right].$
\task (ii) $
A=\left[\begin{array}{rrr}
6 & -1 & 0 \\-1 & 6 & -1 \\0 & -1 & 6
\end{array}\right], b=\left[\begin{array}{l}
1 \\2 \\1
\end{array}\right].$
\task (iii) $
\text { (iii) } A=\left[\begin{array}{ccc}
-2 & 5 & 4 \\6 & 2 & -3 \\1 & 1 & -1
\end{array}\right], \quad b=\left[\begin{array}{l}
1 \\2 \\3
\end{array}\right].$
\task (iv) $
\text { (iv) } A=\left[\begin{array}{cccc}
2 & 1 & 0 & 0 \\2 & 4 & 1 & 0 \\0 & 4 & 8 & 2 \\0 & 0 & 8 & 16
\end{array}\right], b=\left[\begin{array}{c}
4 \\-2 \\1 \\3
\end{array}\right]. $
\end{tasks}
\item
For each of the following data for a linear system $A x=b$, perform the following iterations using the zero vector as the initial vector. Determine if the Jacobi and Gauss-Seidel iterations converge. In cases of convergence, produce a graph of the errors (as measured by the infinity norm of the difference of successive iterates) versus the number of iterations, that contains both the Jacobi iteration data as well as the Gauss-Seidel data. Let the errors go down to $10^{-10}$.
\settasks{
	counter-format=,
	label-width=4ex
}
\begin{tasks}(2)
\task (i) $
A=\left[\begin{array}{ll}
5 & 0 \\2 & 4
\end{array}\right], \quad b=\left[\begin{array}{c}
-1 \\3
\end{array}\right].$
\task (ii) $
A=\left[\begin{array}{ccc}
10 & 2 & -1 \\2 & 10 & 2 \\-1 & 2 & 10
\end{array}\right], \quad b=\left[\begin{array}{l}
1 \\2 \\3
\end{array}\right].$
\task (iii) $
A=\left[\begin{array}{ccc}
7 & 5 & 4 \\3 & 2 & 1 \\2 & 8 & 21
\end{array}\right], \quad b=\left[\begin{array}{l}
1 \\0 \\5
\end{array}\right].$
\task (iv) $
A=\left[\begin{array}{cccc}
3 & 1 & 0 & 0 \\1 & 9 & 1 & 0 \\0 & 1 & 27 & 1 \\0 & 0 & 1 & 81
\end{array}\right], \quad b=\left[\begin{array}{l}
4 \\3 \\2 \\1
\end{array}\right].$
\end{tasks}
\item
(a) For each of the linear systems specified in Exercise 1, run a set of SOR iterations with initial vector the zero vector by letting the relaxation parameter run form $0.05$ to $1.95$ in increments of $0.5$. Use a tolerance of $1 e-6$, but a maximum of 1000 iterations. Plot the number of iterations versus the relaxation parameter $\omega$.\\
(b) Using MATLAB's eig function, let the relaxation parameter $\omega$ run through the same range $0.05$ to $1.95$ in increments of $0.5$, and compute the maximum absolute value of the eigenvalues of the matrix $I-B^{-1} A$ where the matrix $B$ is as in Table $7.1$ (for the SOR iteration). Create a plot of this maximum versus $\omega$, compare and comment on the relationship with the plot of part (a) and Theorem 7.10.
\item
Repeat both parts (a) and (b) for each of the linear systems A x=b \text of Exercise 2 .
\item
For the linear system specified in Exercise 2 (iv), produce graphs of the exact errors of each component of the solution: $x_{1}, x_{2}, x_{3}, x_{4}$ as a function of the iteration. Use the zero vector as the initial iterate. Measure the errors as the absolute values of the differences with the corresponding components of the exact solution as determined using MATLAB's left divide. Continue with iterations until the errors are all less than $10^{-10}$. Point out any observations.
\item
(a) For which of the linear systems specified in Exercise 1(i)-(iv) will the Jacobi iteration converge for all initial iterates?\\
(b) For which of the linear systems specified in Exercise 1(i)-(iv) will the Gauss-Seidel iteration converge for all initial iterates?
\item
(a) For which of the linear systems specified in Exercise 2 (i)-(iv) will the Jacobi iteration converge for all initial iterates?\\
(b) For which of the linear systems specified in Exercise 2(i)-(iv) will the Gauss-Seidel iteration converge for all initial iterates?
\item
\textit{(An Example Where Gauss-Seidel Jteration Converges, but Jacobi Diverges)} Consider the following linear system:
$$
\left[\begin{array}{lll}
5 & 3 & 4 \\
3 & 6 & 4 \\
4 & 4 & 5
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{l}
12 \\
13 \\
13
\end{array}\right].
$$
(a) Show that if initial iterate $x^{(0)}=\left[\begin{array}{lll}0 & 0 & 0\end{array}\right]^{\prime}$, the Jacobi iteration converges to the exact solution $x=[111]^{\prime}$. Show that the same holds true if we start with $x^{(0)}=\left[\begin{array}{ccc}10 & 8 & -6\end{array}\right]^{\prime}$.\\
(b) Show that if initial iterate $x^{(0)}=\left[\begin{array}{lll}0 & 0 & 0\end{array}\right]^{\prime}$, the Gauss-Seidel iteration will diverge. Show that the same holds true if we start with $x^{(0)}=\left[\begin{array}{lll}10 & 8 & -6\end{array}\right]^{\prime}$.\\
(c) For what sort of general initial iterates $x^{(0)}$ do the phenomena in parts (a) and (b) continue to hold?\\
(d) Show that the coefficient matrix of this system is positive definite. What does the SOR convergence theorem (Theorem 7.12) allow us to conclude?
Suggestion: For all parts (especially part (c)) you should first do some MATLAB experiments, and then aim to establish the assertions mathematically.
\item
\textit{(An Example Where Jacobi Iteration Converges, but Gauss-Seidel Diverges)} Consider the following linear system:
$$
\left[\begin{array}{ccc}
1 & 2 & -2 \\
1 & 1 & 1 \\
2 & 2 & 1
\end{array}\right]\left[\begin{array}{l}
x_{1} \\
x_{2} \\
x_{3}
\end{array}\right]=\left[\begin{array}{l}
1 \\
3 \\
5
\end{array}\right]
$$
(a) Show that if initial iterate $x^{(0)}=\left[\begin{array}{lll}0 & 0 & 0\end{array}\right]^{\prime}$, the Jacobi iteration will converge to the exact solution $x=\left[\begin{array}{lll}1 & 1 & 1\end{array}\right]$ ' in just four iterations. Show that the same holds true if we start with $x^{(0)}=\left[\begin{array}{lll}10 & 8-6\end{array}\right]^{\prime}$.\\
(b) Show that if initial iterate $x^{(0)}=\left[\begin{array}{lll}0 & 0 & 0\end{array}\right]^{\prime}$, the Gauss-Seidel iteration will diverge. Show that the same holds true if we start with $x^{(0)}=\left[\begin{array}{lll}10 & 8-6\end{array}\right]^{\prime}$.\\
(c) For what sort of general initial iterates $x^{(0)}$ do the phenomena in parts (a) and (b) continue to hold?
Suggestion: For all parts (especially part (c)) you should first do some MATLAB experiments, and then aim to establish the assertions mathematically.
Note: This example is due to Collatz [Col-42].
\item
(a) Use the formulas of Lemma $7.15$ to write a function M-file with the following syntax:
$$\texttt{b = sparsediag(diags, inds, x)}$$
The input variables are \texttt{diags}, an $n \times j$ matrix where each column consists of the entries of $A$ on one of its diagonals, and \texttt{inds}, a $1 \times j$ vector of the corresponding set of indices for the diagonals (index zero corresponds to the main diagonal). The last input $x$ is the $n \times 1$ vector to be multiplied by $A$. The output is the corresponding product $b=A x$.\\
(b) Apply this program to check that $x=\left[\begin{array}{lll}1 & 1 & 1\end{array}\right]$ ' solves the linear system of Exercise 9.\\
(c) Apply this program to compute the matrix products of Exercise for the Reader $7.30$ and check the error against the exact solution obtained in the latter.
\item
(a) Modify the program sorsparsediag of Exercise for the Reader $7.31$ to construct an analogous M-file:
$$\texttt{[x,k,diff] = jacbobisparsediag(diags, inds, b, xO, tol, kmax)}$$
for the Jacobi method.\\
(b) Modify the program sorsparsedia g of Exercise for the Reader 7.31 to construct an 
analogous M-file: 
$$\texttt{(x,k,diff]=gaussseidelsparsediag(diags, inds, b, xO, tol, kmax)}$$
for the Gauss-Seidel method.\\
(c) Apply these programs to recover the results of Examples $7.26$ and 7.27.\\
(d) Using the $M$-files of parts (a) and (b), along with \texttt{sorsparsediag}, recreate the MATLAB graphic that is shown in Figure 7.43.
\item
(a) Find a $2 \times 2$ matrix $A$ whose optimal relaxation parameter $\omega$ appears to be greater than $1.5$ (as demonstrated by a MATLAB plot like the one in Figure 7.42) resulting from the solution of some linear system $A x=b$.\\
(b) Repeat part (a), but this time try to make the optimal value of $\omega$ to be less than $0.5$.
\item
Repeat both parts of Exercise 12, this time working with $3 \times 3$  matrices. 
\item
(a) Find a $2 \times 2$ matrix $A$ whose optimal relaxation parameter $\omega$ appears to be greater than $1.5$ (as demonstrated by a MATLAB plot like the one in Figure 7.42) resulting from the solution of some linear system $A x=b$.\\
(b) Repeat part (a), but this time try to make the optimal value of $\omega$ to be less than $0.5$.
\item
\textit{(A Program 10 Estimate the Optimal SOR Parameter $\omega$ )}(a) Write a program that will aim to find the optimal relaxation parameter $\omega$ for the SOR method in the problem of solving a certain linear system $A x=b$ for which it is assumed that the SOR method will converge. (For example, by the SOR convergence theorem, if $A$ is symmetric positive definite, this program is applicable.) The syntax is as follows:
$$\texttt{omega = optimalomega (A,b,tol,iter)}$$
Of the input and output variables, only the last two input variables need explanation. The input variable \texttt{tol} is simply the accuracy goal that we wish to approximate \texttt{omega}. The variable \texttt{iter} denotes the number of iterations to use on each trial run. The default value for\texttt{tol} is le-3 and for \texttt{iter} it is 10. (For very large matrices a larger value may be needed for \texttt{iter}, and likewise for very small matrices a smaller value should be used.) Once this tolerance is met, the program terminates. The program should work as follows: First run through a set of SOR iterations with the values of $\omega$ running from $0.05$ to $1.95$ in increments of $0.05$. For each value of $\omega$ we run through iter iterations. For each of these we keep track of the infinity norm of the difference of the final iterate and the immediately preceding iterate. For each tested value of $\omega=a_{0}$ for which this norm is minimal, we next run the tests on the values of $\omega$ running from $a_{b}-.05$ to $a_{0}+.05$ in increments of $0.005$ (omit the values $\omega=0$ or $\omega=2$ should these occur as endpoints). In the next iteration, we single out those new values of $\omega$ for which the new error estimate is minimized. For each new corresponding value $\omega=\omega_{b}$ for which the norm is minimized, we will next run tests on the set of values from $\omega_{0}-.005$ to $a_{b}+.005$ in increments of $0.0005$. At each iteration, the minimizing values of $\omega=\omega_{0}$ should be unique; if they are not, the program should deliver an error message to this effect, and recommend to try running the program again with a larger value of \texttt{iter}. When the increment size is less than tol, the program terminates and outputs the resulting value of $\omega=\omega_{\mathrm{b}}$.\\
(b) Apply the above program to aim to determine the optimal value of the SOR parameter $\omega$ for the linear system of Example $7.26$ with default tolerances. Does the resulting output change if we change \texttt{iter} to 5 ? To 20 ?\\
(c) Repeat part (b), but now change the default tolerance to le-6.\\
(d) Run the SOR iteration on the linear system using the values of the relaxation parameter computed in parts (a) and (b) and compare the rate of convergences with each other and with that seen in the text when $\omega=0.9$ (Figure 7.41).\\
(e) Is the program in part (a) practical to run on the large matrix such as the $2500 \times 2500$ matrix of Example $7.30$ (perhaps using a small value for \texttt{iter})? If yes, run the program and compare with the result of Proposition 7.14.
\item
\textit{(A Program to Estimate the Optimal SOR Parameter $\omega$ for Sparse Banded Systems)} (a) Write a program that will aim to find the optimal relaxation parameter $\omega$ for the SOR method in the problem of solving a certain linear system $A x=b$ for which it is assumed that the SOR method will converge. The functionality of the program will be similar to that of the preceding exercise, except that now the program should be specially designed to deal with sparsely banded systems, as did the program \texttt{sorsparsediag} of Exercise for the Reader $7.31$ (in fact, the present program should call on this previous program). The syntax is as follows:
$$\texttt{omega = optimalomegasparsediag(diags, inds, b, tol,iter)}$$
The first three input variables are as explained in Exercise for the Reader $7.31$ for the program \texttt{sorsparsediag}. The remaining variables and functionality of the program are as explained in the preceding exercise.\\
(b) Apply the above program to aim to determine the optimal value of the SOR parameter $\omega$ for the linear system of Example $7.26$ with default tolerances. Does the resulting output change if we change \texttt{iter }to 5 ? To 20?\\
(c) With default tolerances, run the program on the linear system of Example $7.30$ and compare with the exact result of Proposition 7.14. You may need to experiment with different values of \texttt{iter} to attain a successful approximation. Run SOR on the system with this computed value for the optimal relaxation parameter, and 308 iterations. Compute the exact error and compare with the results of Example 7.31.\\
(d) Repeat part (c) but now with \texttt{tol} reset to le-6.
\end{enumerate}
NOTE: For tridiagonal matrices that are positive definite, the following formula gives the optimal 
value of the relaxation parameter for the SOR iteration: 
\begin{equation}
\omega=\frac{2}{1+\sqrt{1-\rho(D-L)^{2}}},
\end{equation}
where the matrices $D$ and $L$ are as in (60) $A=D-L-U,^{24}$ and $\rho(D-L)$ denotes the spectral radius of the matrix $D-L$.
\begin{enumerate}[resume]
\item
We consider tridiagonal square $n \times n$ matrices of the following form:
$$
F=\left[\begin{array}{llllll}
2 & a & & & & \\
a & 2 & a & & 0 & \\
& a & 2 & a & & \\
& & \ddots & \ddots & \ddots & \\
& 0 & & \ddots & \ddots & a \\
& & & & a & 2
\end{array}\right]
$$
(a) With $a=-1$ and $n=10$, show that $F$ is positive definite.\\
(b) What does formula (17) give for the optimal SOR parameter for the linear system?\\
(c) Run the SOR iteration with the value of $\omega$ obtained in part (b) for the linear system $F_{x}=b$ where the exact solution is $x=\left[\begin{array}{llllll}1 & 2 & 1 & 2 & \ldots & 1\end{array}\right].$How many iterations are needed to get the exact error to be less than le-10?\\
(d) Create a graph comparing the performance of the SOR of part (c) along with the corresponding Jacobi and Gauss-Seidel iterations.
\item
Repeat all parts of Exercise 17, but change a to -0.5. 
\item
Repeat all parts of Exercise 17 , but change $n$ to 100 . Can you prove that with $a=-1$, the matrix $F$ in Exercise 17 is always positive definite? If you cannot, do some MATLAB experiments (with different values of $n$ ) and conjecture whether you think this is a true statement.
\item
(a) Show that the Jacobi iteration scheme is represented in matrix form (61) by the matrix $B$ indicated in Table 7.1.\\
(b) Repeat part (a) for the Gauss-Seidel iteration.\\
(c) Repeat part (a) for the SOR iteration.
\item
Prove Lemma 7.15. 
\item
(a) Given a nonsingular matrix $A$, find a corresponding matrix $T$ so that the Jacobi iteration can be expressed in the form $x^{(k+1)}=x^{(k)}+\operatorname{Tr}^{(k)}$, where $r^{(k)}=b-A x^{(k)}$ is the residual vector for the $k$ th iterate.\\
(b) Repeat part (a) for the Gauss-Seidel iteration.\\
(c) Can the result of part (b) be generalized for the SOR iteration?
\item
\textit{(Proof of Jacobi Convergence Theorem)} Complete the following outline for a proof of the Jacobi Convergence Theorem (part of Theorem 7.11): As in the text, we let $e^{(k)}=x^{(k)}-x$ denote the error vector of the $k$ th iterate $x^{(k)}$ for the Jacobi method for solving a linear system $A x=b$, where the $n \times n$ matrix $A$ is assumed to be strictly diagonally dominant. For each (row) index $i$, we let $\mu_{i}=\frac{1}{\left|a_{i i}\right|} \sum_{\substack{j=1 \\ j \neq i}}^{n}\left|a_{i j}\right|(1 \leq i \leq n)$. For any vector $v$, we let $\|v\|$ denote its infinity norm: $\|v\|_{\infty}=\max \left(\left|v_{i}\right|\right).$\\
For each iteration index k and component index i, use the triangle inequality to show that
$$
\left|e_{i}^{(k)}\right| \leq \frac{1}{\left|a_{i i}\right|} \sum_{\substack{j=1 \\ j \neq i}}^{n}\left|a_{i j}\right|\left|e_{i}^{(k-1)}\right| \leq \mu_{i}\left\|e^{(k-1)}\right\|,
$$
and conclude that 
$$
\left\|e^{(k)}\right\| \leq\|\mu\|\left\|e^{(k-1)}\right\|,
$$
and, in turn, that the Jacobi iteration converges. 

\newpage

This page intentionally left blank


\newpage


\end{enumerate}



	
\clearpage
\end{document} 
